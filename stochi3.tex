\documentclass{cheat-sheet}

\pdfinfo{
  /Title (Zusammenfassung Stochastik 3)
  /Author (Tim Baumann)
}

\usepackage{bbm} % Für 1 mit Doppelstrich (Indikatorfunktion)
\usepackage{mathtools} % psmallmatrix environment
\usepackage{nicefrac}

% Kleinere Klammern
\delimiterfactor=701

% TODO: Include-File für Stochastik

\newcommand{\Alg}{\mathfrak{A}} % (Mengen-)Algebra
%\newcommand{\Ring}{\mathfrak{R}} % (Mengen-)Ring
%\newcommand{\LebAlg}{\mathfrak{L}} % Lebesgue-Borel-Mengen
\renewcommand{\P}{\mathbb{P}} % Wahrscheinlichkeitsmaß
\newcommand{\E}{\mathbb{E}} % Erwartungswert
\newcommand{\Bor}{\mathfrak{B}} % Borel
%\newcommand{\Leb}{\mathcal{L}} % Lebesgue
\newcommand{\ind}{\mathbbm{1}} % Indikatorfunktion
\newcommand{\Cont}{\mathcal{C}} % Menge der stetigen/diff'baren Funktionen
\newcommand{\scp}[2]{\left( #1 \!\mid\! #2 \right)} % Skalarprodukt
%\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\eqqd}{\stackrel{d}{=}} % Gleichheit in Verteilung (equality in distribution)
\newcommand{\iid}{i.\,i.\,d.} % identisch unabhängig verteilt
\newcommand{\Uniform}{\mathcal{R}} % Gleichverteilung
\DeclareMathOperator{\Cum}{Cum} % Kumulante
\DeclareMathOperator{\rk}{rk} % Rang einer Matrix

\DeclareMathOperator{\var}{Var} % Varianz
\DeclareMathOperator{\cov}{Cov} % Kovarianz
\DeclareMathOperator{\cor}{Cor} % Korrelation

% Verteilungen
\newcommand{\Normal}{\mathcal{N}} % Gaußsche Normalverteilung
\DeclareMathOperator{\Exp}{Exp} % Exponentialverteilung
\newcommand{\MN}{\mathcal{M}} % Multinomialverteilung

\begin{document}

\maketitle{Zusammenfassung Stochastik 3}

% Vorlesung vom 12.10.2015

\section{Hypothesentests mittels Stichprobenfktn}

% 1. Wiederholung

% 1.1 Grundbegriffe der Testtheorie

\begin{modell}
  Gegeben sei ein parametrisches Modell, \dh eine Zufallsgröße $X$, deren Verteilungsfunktion $P_X \in \Set{P_\vartheta}{\vartheta \in \Theta \subset \R^n}$ von einem Parameter $\vartheta$ abhängt.
\end{modell}

\begin{prob}
  Anhand einer \emph{Stichprobe} $x_1, \ldots, x_n \in \R^1$ von $X$ (\dh{} $x_1, \ldots, x_n$ sind Realisierung von iid ZGen $X_1, \ldots, X_n \sim P_X$) ist zu entscheiden, ob die sogenannte \emph{Nullhypothese} $H_0 : \vartheta \in \Theta_0 \subset \Theta$ oder eine \emph{Gegenhypothese} $H_1 : \vartheta \in \Theta_1 = \Theta \setminus \Theta_0$ angenommen oder abgelehnt werden soll.
\end{prob}

\begin{defn}
  Der \emph{Stichprobenraum} ist $(\R^n, \Bor(\R^n), P_\vartheta \times \ldots \times P_\vartheta)$
\end{defn}

\begin{terminologie}
  Die Hypothese $H_i$ heißt \emph{einfach}, falls $\abs{\Theta_i} = 1$, andernfalls \emph{zusammengesetzt}.
\end{terminologie}

\begin{defn}
  Ein (nichtrandomisierter) \emph{Test} für $H_0$ gegen $H_1$ ist eine Entscheidungsregel über die Annahme von $H_0$ basierend auf einer Stichprobe, die durch eine messbare Abbildung $\varphi : \R^n \to \{ 0, 1 \}$ augedrückt wird und zwar durch
  \[ \varphi(x_1, \ldots, x_n) = \begin{cases}
    0 & \text{bei Annahme von $H_0$,} \\
    1 & \text{bei Ablehung von $H_0$.}
  \end{cases} \]
\end{defn}

\begin{defn}
  Der \emph{Ablehungsbereich} oder \emph{kritische Bereich} von $\varphi$ ist
  \[ K_n \coloneqq \Set{(x_1, \ldots, x_n) \in \R^n}{\varphi(x_1, \ldots, x_n) = 1}. \]
\end{defn}

\begin{bem}
  Es gilt $\varphi = \ind_{K_n}$.
\end{bem}

\begin{defn}
  Ein \emph{Fehler 1. Art} ist eine Ablehnung der Nullhypothese $H_0$, obwohl $H_0$ richtig ist;
  ein \emph{Fehler 2. Art} ist eine Annahme von $H_0$, obwohl $H_0$ falsch ist.
\end{defn}

% Ausgelassen: Stichprobenraum ist [R^n, B(R^n), P_\vartheta \times \ldots \times P_\vartheta]

\begin{defn}
  Die \emph{Güte- oder Machtfunktion} des Tests $\varphi$ ist
  \begin{align*}
    m_\varphi : \Theta \to \cinterval{0}{1},
    m_\varphi(\vartheta) & \coloneqq
    \E_\vartheta \varphi(X_1, \ldots, X_n) \\
    & = \P_\vartheta ((X_1, \ldots, X_n) \in K_n) \\
    & = (P_\vartheta \times \ldots \times P_\vartheta)(K_n)
  \end{align*}
  Die Gegenwsk. $(1 {-} m_\varphi(\vartheta))$ heißt \emph{Operationscharakteristik} von $\varphi$.
\end{defn}

\begin{bem}
  Es gilt \enspace
  $\begin{array}[t]{r c l l}
    \P_\vartheta(\text{Fehler 1. Art}) &=& m_\varphi(\vartheta) & \enspace\text{für $\vartheta \in \Theta_0$,} \\
    \P_\vartheta(\text{Fehler 2. Art}) &=& 1 - m_\varphi(\vartheta) & \enspace\text{für $\vartheta \in \Theta_1$.}
  \end{array}$
\end{bem}

% Ausgelassen: Graph einer "fast idealen Kurve"

\begin{defn}
  Ein Test $\varphi : \R^n \to \{ 0, 1 \}$ mit
  \[ \sup_{\vartheta \in \Theta_0} m_\varphi(\vartheta) \leq \alpha \]
  heißt \emph{$\alpha$-Test} o. \emph{Signifikanztest} zum \emph{Signifikanzniveau} $\alpha \in \ointerval{0}{1}$. \\[2pt]
  Ein $\alpha$-Test $\varphi$ heißt \emph{unverfälscht} (erwartungstreu, unbiased), falls
  \[ \inf_{\vartheta \in \Theta_1} m_\varphi(\vartheta) \geq \alpha. \]
\end{defn}

% Konstruktion nichtrandomisierter Tests mittels Stichprobenfunktionen (= Statistiken) im Falle einfacher Nullhypothesen

\begin{situation}
  Sei nun eine Stichprobenfunktion oder \emph{Teststatistik} $T : \R^n \to \R^1$ gegeben.
  Wir wollen einen Test der einfachen Nullhypothese $H_0 : \vartheta \in \Theta_0 = \{ \vartheta_0 \}$ entwickeln.
\end{situation}

\begin{defn}
  $K_n^T \subset \R^1$ heißt \emph{kritischer Bereich der Teststatistik}, falls
  \[ K_n = T^{-1}(K_n^T). \]
\end{defn}

\begin{bem}
  Es gilt
  \begin{alignat*}{4}
    m_\varphi(\vartheta_0) &= \P_{\vartheta_0} \left( (X_1, \ldots, X_n) \in K_n \right)
    &&= \\
    &= \P_{\vartheta_0} \left( (T(X_1), \ldots, T(X_n)) \in K_n^T \right)
    &&= \Int{K_n^T}{}{f_T(x)}{x} \leq \alpha,
  \end{alignat*}
  wobei $f_T$ die Dichte von $T(X_1, \ldots, X_n)$ unter $H_0$ ist.
\end{bem}

\begin{bsp}
  Sei $X \sim \Normal(\mu, \sigma^2)$, $\sigma$ bekannt und $\alpha \in \ointerval{0}{1}$ vorgegeben. \\
  Zum Test von $H_0 : \mu = \mu_0$ vs. $H_1 : \mu \neq \mu_0$ wählen wir als Statistik
  \[
    T(X_1, \ldots, X_n) \coloneqq \tfrac{\sqrt{n}}{\sigma} \left( \overline{X}_n - \mu_0 \right) \enspace
    \text{mit }
    \overline{X}_n \coloneqq \tfrac{1}{n} \left( X_1 + \ldots + X_n \right).
  \]
  Unter Annahme von $H_0$ gilt $T(X_1, \ldots, X_n) \sim \Normal(0,1)$. \\
  Der Ablehnungsbereich der Statistik ist
  \[
    K_n^T = \Set{t \in \R^1}{\abs{t} > z_{1 - \alpha/2}}
    \quad \text{mit} \quad
    z_{1 - \alpha/2} \coloneqq \Phi^{-1}(1 - \alpha/2).
  \]
  Für $\alpha = 0,5$ gilt beispielsweise $z_{1 - \alpha/2} \approx 1,96$.
\end{bsp}

% Vorlesung vom 15.10.2015

\begin{bem}
  Es gilt
  \begin{align*}
    t \in (K_n^T)^c
    &\iff \abs{t} \leq z_{1-\alpha/2}
    \iff \abs{\overline{X}_n - \mu_0} \leq \tfrac{\sigma}{\sqrt{n}} z_{1-\alpha/2} \\
    &\iff \mu_0 \in \cinterval{\overline{X}_n - \tfrac{\sigma}{\sqrt{n}} z_{1-\alpha/2}}{\overline{X}_n + \tfrac{\sigma}{\sqrt{n}} z_{1-\alpha/2}}.
  \end{align*}
  Letzteres Intervall wird \emph{Konfidenzintervall} für $\mu_0$ zum Konfidenz- niveau $1-\alpha$ genannt.
\end{bem}

\begin{bsp}
  Sei wieder $X \sim \Normal(\mu, \sigma^2)$, $\sigma^2$ aber diesmal unbekannt. \\
  Zum Testen von $H_0 : \mu = \mu_0$ vs. $H_1 : \mu \neq \mu_0$ verwenden wir
  \[
    \hat{T}(X_1, \ldots, X_n) = \tfrac{\sqrt{n}}{S_n} \left( \overline{X}_n - \mu_0 \right), \quad
    S_n^2 \coloneqq \tfrac{1}{n-1} \sum_{i=1}^n \left( X_i - \overline{X}_n \right)^2.
  \]
  Dabei ist $S_n$ die \emph{(korrigierte) Stichprobenvarianz}.
  Man kann zeigen, dass $\hat{T}(X_1, \ldots, X_n) \sim t_{n-1}$ unter $H_0$.
  Dabei ist $t_m$ die \emph{Student'sche $t$-Verteilung} mit $m$ Freiheitsgraden. \\
  Der Ablehnungsbereich ist
  \[ K_n^T = \Set{t \in \R^1}{\abs{t} > t_{n-1,1-\alpha/2}}. \]
\end{bsp}

\begin{bem}
  $S_n^2$ und $\overline{X}_n$ sind unabhängig für $n \geq 2$. % und umgekehrt?
\end{bem}

\begin{diskussion}
  \begin{itemize}
    \item Je kleiner $\alpha$ ist, desto "`nullhypothesenfreundlicher"' ist der Test.
    Häufig verwendet wird $\alpha \in \{ 10\%, 5\%, 1\%, 0,5\% \}$.
    \item Einseitige Tests: Die Gegenhypothese zu $H_0 \!:\! \mu \!=\! \mu_0$ ist $H_1 \!:\! \mu \!>\! \mu_0$.
    Die Nullhypothese wird nur abgelehnt, falls zu große Stichproben- mittelwerte $\overline{x}_n$ vorliegen. Es ist dann $K_n^T = \ointerval{z_{1-\alpha}}{\infty}$.
  \end{itemize}
\end{diskussion}

% 1.2. Prüfverteilung bei normalverteilten Grundgesamtheit

\begin{defn}
  Es seien $X_1, \ldots, X_n \sim \Normal(0, 1)$.
  Dann heißt die Summe $X_1^2 + \ldots + X_n^2 \sim \chi_n^2$ \emph{Chi-Quadrat-verteilt} mit $n$ Freiheitsgraden.
\end{defn}

\begin{defn}
  Falls $X \sim \Normal(0,1)$ und $Y_n \sim \chi_n^2$ unabhängig sind, so heißt
  \[ \tfrac{X}{\sqrt{\tfrac{Y_n}{n}}} \sim t_n \]
  \emph{$t$-verteilt} mit $n$-Freiheitsgraden.
\end{defn}

\begin{lem}
  $\tfrac{n-1}{\sigma^2} S_n^2 \sim \chi_{n-1}^2$
\end{lem}

\begin{kor}
  $\hat{T}$ aus dem zweiten obigen Bsp ist tatsächlich $t$-verteilt.
\end{kor}

\begin{defn}
  Seien $Y_{n_i} \sim \chi_{n_i}^2$, $i = 1, 2$ zwei unabhängige ZGen.
  Dann heißt %der Quotient
  \[ \tfrac{Y_{n_1} / n_1}{Y_{n_2} / n_2} \sim F_{n_1, n_2} \]
  \emph{F-verteilt} (wie Fisher) mit $(n_1, n_2)$ Freiheitsgraden.
  % auch: Suedecor-verteilt
\end{defn}

% Vorlesung vom 19.10.2015

\begin{bsp}
  Sei $X \sim \Normal(\mu, \sigma^2)$ mit $\mu$ unbekannt.
  Wir testen $H_0 : \sigma = \sigma_0$ vs. $H_1 : \sigma \neq \sigma_0$ mit
  \[ T \coloneqq \tfrac{n-1}{\sigma_0^2} S_n^2 \]
  Unter Annahme von $H_0$ gilt $T ~ \chi_{n-1}^2$.
  Falls $\mu$ bekannt ist, muss man
  \[
    \widetilde{T} \coloneqq \tfrac{n}{\sigma_0^2} \widetilde{S}_n^2, \quad
    \widetilde{S}_n^2 \coloneqq \tfrac{1}{n} \sum_{i=1}^n (X_i - \mu)^2.
  \]
  als Statistik wählen. Unter Annahme von $H_0$ ist $\widetilde{T} \sim \chi_n^2$.
\end{bsp}

\begin{bsp}
  Seien Stichproben $X_1^{(1)}, \ldots, X_{n_1}^{(1)} \sim \Normal(\mu_1, \sigma_1^2)$ und $X_1^{(2)}, \ldots, X_{n_2}^{(2)} \sim \Normal(\mu_2, \sigma_2^2)$ gegeben.
  Wir wollen $H_0 : \sigma_1 = \sigma_2$ gegen $H_1 : \sigma_1 \neq \sigma_2$ testen.
  Dazu verwenden wir
  \[
    T = \frac{S_{X^{(1)}}^2}{S_{X^{(2)}}^2}, \quad
    S_{X^{(j)}}^2 \coloneqq \tfrac{1}{n-1} \sum_{i=1}^{n_j} \left( X_i^{(j)} - \overline{X}^{(j)}_n \right)^n.
  \]
  Falls $H_0$ gilt, so ist $T \sim F_{n_1-1,n_2-1}$.
\end{bsp}

\begin{bsp}
  Situation wie im letzten Beispiel mit $\sigma_1 = \sigma_2$.
  Wir testen $H_0 : \mu_1 = \mu_2$ vs. $H_1 : \mu_1 \neq \mu_2$ mit
  \[
    T = \sqrt{\frac{n_1 \cdot n_2}{n_1 + n_2}} \cdot \frac{\overline{X}_{n_1}^{(1)} - \overline{X}_{n_2}^{(2)}}{S_{n_1,n_2}}, \quad
    S_{n_1,n_2}^2 = \frac{(n_1{-}1) S_{X^{(1)}}^2 + (n_2{-}1) S_{X^{(2)}}^2}{n_1 + n_2 - 2}
  \]
  Unter $H_0$ gilt $T \sim t_{n_1 + n_2 - 2}$.
\end{bsp}

\begin{bsp}
  Seien $\begin{psmallmatrix} X_1 \\ Y_1 \end{psmallmatrix}, \ldots, \begin{psmallmatrix} X_n \\ Y_n \end{psmallmatrix} \sim \Normal\left(
    \begin{psmallmatrix}
      \mu_1\vphantom{\sigma_1^2} \\
      \mu_2\vphantom{\sigma_2^2}
    \end{psmallmatrix},
    \begin{psmallmatrix}
      \sigma_1^2 & \sigma_1 \sigma_2 \rho \\
      \sigma_1 \sigma_2 \rho & \sigma_2^2
    \end{psmallmatrix} \right)$. \\[2pt]
  Wir testen $H_0 : \rho = 0$ vs. $H_1 : \rho \neq 0$ mit
  \[
    T \coloneqq \frac{\sqrt{n-2} \cdot \hat{\rho}_n}{\sqrt{1 - \hat{\rho}_n^2}}, \quad
    \hat{\rho}_n \coloneqq \frac{\tfrac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X}_n) (Y_i - \overline{Y}_n)}{S_{X,n} \cdot S_{Y,n}}.
    % Nenner ausgeschrieben: \sqrt{\tfrac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X}_n)^2 \tfrac{1}{n-1} \sum_{i=1}^n (Y_i - \overline{Y}_n)^2}
  \]
  Falls $H_0$ richtig ist, so gilt $T \sim t_{n-2}$. \\[2pt]
  Um $H_0 : \rho = \rho_0 \in \ointerval{0}{1}$ vs. $H_1 : \rho \neq \rho_0$ zu testen, kann man
  \[ T = \tfrac{\sqrt{n-3}}{2} \left( \log \tfrac{1 + \hat{\rho}_n}{1 - \hat{\rho}_n} - \log \tfrac{1+\rho_0}{1-\rho_0} \right) \]
  verwenden. Für $n$ groß gilt $T \sim \Normal(0, 1)$ unter $H_0$.
\end{bsp}

% 1.4. Lemma von Slutzky und varianzstabilisiernde Transformationen

\begin{lem}[\emph{Slutzky}]
  Seien $(X_n)$, $(Y_n)$ Folgen von ZGn über $(\Omega, \Alg, \P)$ mit $X_n \xra[n \to \infty]{\P} c = \text{const}$ (\dh{} $\fa{\epsilon > 0} \P(\abs{X_n - c} > \epsilon) \to 0$) und $Y_n \xra[n \to \infty]{d} Y$ (\dh{} $\P(Y_n \leq y) \to \P(Y \leq y)$ für alle Stetigkeitspunkte $y$ der VF $y \mapsto \P(Y \leq y)$). Dann gilt:
  \[
    X_n + Y_n \xra{d} c + Y, \quad
    X_n \cdot Y_n \xra{d} c \cdot Y, \quad
    Y_n / X_n \xra{d} Y / c \enspace \text{(falls $c \neq 0$)}
  \]
  und allgemeiner $f(X_n, Y_n) \xra[n \to \infty]{d} f(c, Y)$ für jede Fkt $f \in \Cont(\R^2, \R)$.
\end{lem}

\begin{bem}
  Unabhängigkeit von $(X_n)$ und $(Y_n)$ wird nicht vorausgesetzt!
\end{bem}

% Vorlesung vom 22.10.2015

% Varianzstabilisierende Transformationen

\begin{situation}
  Sei $T_n = T(X_1, \ldots, X_n)$ eine Statistik.
  Falls der ZGWS für $T_n$ die Form
  \[ \sqrt{n} (T_n - \vartheta) \xra[n \to \infty]{d} \Normal(0, g(\vartheta)) \]
  besitzt, so benötigen wir für Hypothesentests eine Möglichkeit, die Abhängigkeit der Varianz von Parameter $\vartheta$ zu beseitigen.
  Man sagt, man führt eine \emph{varianzstabilisierende Transformation} durch. \\
  Wir suchen dazu eine stetig diff'bare Funktion $f : \Theta \to \R^1$, sodass
  \[ \sqrt{n} (f(T_n) - f(\vartheta)) \xra[n \to \infty]{d} \Normal(0, 1). \]
  Man zeigt, dass dafür gelten muss:
  \[
    f'(\vartheta) = \tfrac{1}{\sqrt{g(\vartheta)}}, \quad \text{also} \quad
    f(\theta) = \myint{}{} \tfrac{\d \vartheta}{\sqrt{g(\vartheta)}}.
  \]
\end{situation}

\begin{bspe}
  \begin{itemize}
    \item Sei $X \sim \Exp(\mu)$, $\hat{\mu}_n \coloneqq \tfrac{1}{\overline{X}_n}$.
    Dann gilt
    \begin{align*}
      & \sqrt{n} (\overline{X}_n - \tfrac{1}{\mu}) \xra{d} \Normal(0, g(\tfrac{1}{\mu}))
      \quad \text{mit} \quad
      g(\vartheta) \coloneqq \vartheta^2. \\
      \leadsto \enspace & \text{Mit } f(\theta) \coloneqq \myint{}{} \tfrac{\d \vartheta}{\sqrt{g(\vartheta)}} = \myint{}{} \tfrac{\d \vartheta}{\vartheta} = \log \theta \\
      & \text{gilt } \sqrt{n} (\log(\overline{X}_n - \log(\tfrac{1}{\mu}))) \xra[n \to \infty]{d} \Normal(0, 1).
    \end{align*}
    \item Wir wollen eine unbek. Wahrscheinlichkeit~$p$ schätzen, etwa durch Wurf einer Münze.
    Der ZGWS von de-Moirre-Laplace besagt
    \[ \sqrt{n} ( \hat{p}_n - p) \xra[n \to \infty]{d} \Normal(0, p (1-p)), \]
    wobei~$\hat{p}_n$ die relative Häufigkeit ist.
    Zur Stabilisierung der Varianz verwenden wir nun
    \[ f(\theta) \coloneqq \myint{}{} \tfrac{\d p}{\sqrt{p (1-p)}} = 2 \arcsin(\sqrt{\theta}). \]
  \end{itemize}
\end{bspe}

% 1.5. Mehrdimensionale Normalverteilung

\begin{defn}
  Die $k$-dim (Gaußsche) Normalverteilung $\Normal_k(m, C)$ mit EW~$m \in \R^k$ und einer nichtnegativ-definiten, symmetrischen Kovarianzmatrix~$C \in \R^{k \times k}$ ist gegeben durch die Dichte
  \[ f_{\Normal_k(m, C)}(x) \coloneqq \left( (2\pi)^{k/2} \sqrt{\det(C)} \right)^{-1} \exp \left( - \tfrac{1}{2} (x-m) C^{-1} (x-m)^T \right). \]
\end{defn}

\begin{bem}
  Bei $k=2$ schreibt man oft
  \[
    C = \begin{psmallmatrix}
      \sigma_1^2 & \sigma_1 \sigma_2 \rho \\
      \sigma_1 \sigma_2 \rho & \sigma_2^2
    \end{psmallmatrix}
    \quad \text{mit} \quad
    \rho \coloneqq \cor(X_1, X_2).
  \]
\end{bem}

\begin{defn}
  Die \emph{charakteristische Fkt} eines ZV $X = (X_1, \ldots, X_k)^T$ ist
  \[
    \varphi : \R^k \to \R, \enspace
    t \mapsto \E e^{i \scp{t}{X}} = \Int{\R^k}{}{e^{i (t_1 x_1 + \ldots + t_k x_k)}}{F_X(x_1, \ldots, x_k)}.
  \]
\end{defn}

\begin{bem}
  Die charakteristische Funktion von $\Normal_k(m, C)$ ist
  \[ \varphi_{\Normal_k(m, C)}(t) = \exp \left( i \sum_{i=1}^k t_i m_i - \tfrac{1}{2} \sum_{i,j=1}^k t_i c_{ij} t_j \right). \]
\end{bem}

\begin{satz}
  Für $A \in R^{k \times l}$ gilt $\Normal_k(m, C) \cdot A = \Normal_l(m \cdot A, A^T C A)$.
\end{satz}

% Vorlesung vom 26.10.2015

% 2. Anpassungstests und weitere nichtparametrische Tests
\section{Chi-Quadrat-Anpassungstest}

% 2.1. Chi-Quadrat-Anpassungstest

\begin{aufgabe}
  Prüfe, ob eine vorliegende Stichprobe $x_1, \ldots, x_n$ aus einer bestimmten (stetig oder diskret verteilten) Grundgesamtheit gezogen wurde. Wir testen also $H_0 : F = F_0$ vs. $H_1 : F \neq F_0$.
\end{aufgabe}

% Ausgelassen: Bemerkung über Einordnung in parametrische Tests via. $\Theta \coloneqq \{ \text{Verteilungsfunktionen auf $\R$} \}}$ und $\Theta_0 \coloneqq \{ F_0 \}$.

\begin{verf}
  Wir teilen zunächst $\R$ in Klassen ein,
  \begin{align*}
    & \R = \bigcup_{i=1}^{s+1} I_j
    \quad \text{mit} \quad
    I_j \coloneqq \ocinterval{y_{j-1}}{y_j},
    \quad
    \text{wobei} \\
    & - \infty = y_0 < y_1 < \ldots < y_s < y_{s+1} = + \infty.
  \end{align*}
  Wir setzen
  \begin{align*}
    & h_{n_j} \coloneqq \abs{\Set{k \in \{ 1, \ldots, n \}}{X_k \in I_j}} \tag{absolute Klassenhäufigkeit} \\
    & p_j^{(0)} \coloneqq \P(X \in I_j) = F_0(y_j) - F_0(y_{j-1}) \tag{Klassenwktn unter $H_0$}
  \end{align*}
  Die Klassenhäufigkeiten sind nun multinomialverteilt:
  \[ \P(h_{n_1} \!=\! n_1, \nldots, h_{n_{s+1}} \!=\! n_{s+1}) = \binom{n}{n_1, \nldots, n_{s+1}} (p_1^{(0)})^{n_1} \cdots (p_{s+1}^{(0)})^{n_{s+1}}. \]
  Als (näherungsweises) Maß für die Abweichung einer empirischen Verteilung von $F_0$ bei gegebener Klasseneinteilung dient
  \[ T_{n,s+1} \coloneqq \sum_{j=1}^{s+1} \frac{(h_{n_j} - n p_j^{(0)})^2}{n p_j^{(0)}}. \]
\end{verf}

\begin{satz}
  $T_{n,s+1} \xra[n \to \infty]{d} \chi_s^2$
\end{satz}

\begin{faustregel}
  Für $n p_j^{(0)} \geq 5$, $j = 1, \ldots, s+1$ ist $T_{n,s+1}$ mit guter Näherung $\chi_s^2$-verteilt.
\end{faustregel}

% Vorlesung vom 29.10.2015

\begin{entscheidungsregel}[$\chi^2$-Anpassungstest]
  Die Nullhypothese $H_0 : F = F_0$ wird genau dann verworfen, wenn $T_{n,s+1} > \chi^2_{s,1-\alpha}$.
\end{entscheidungsregel}

\begin{bemn}
  \begin{itemize}
    \item $T_{n,s+1}$ misst eigentlich nicht die Abweichung von der VF~$F_0$, sondern von der Multinomialverteilung $\MN(n, p^{(0)})$.
    \item Der $\chi^2$-Anpassungstest gilt als hypothesenfreundlich.
    \item Es ist üblich, zunächst die Parameter $\vartheta = (\vartheta_1, \ldots, \vartheta_r)$ der VF~$F_0$ durch MLE zu schätzen, also durch
    \begin{align*}
      & \hat{\vartheta}_n \coloneqq \argmax L(h_{n_1}, \ldots, h_{n_{s+1}}; \vartheta), \quad \text{wobei} \\
      & L(h_{n_1}, \ldots, h_{n_{s+1}}; \vartheta) \coloneqq \prod_{j=1}^{s+1} \left( p_j^{(0)} \right)^{h_{n_j}}.
    \end{align*}
    % Natürliche Bedingungen: Rao-Cramér-Regularität
    Es kann (unter "`natürlichen"' Bedingungen) gezeigt werden, dass
    \[ T_{n,s+1}(\hat{\vartheta}_n) = \sum_{j=1}^{s+1} \frac{(h_{n_j} - n p_j^{(0)}(\hat{\vartheta}_n))^2}{n p_j^{(0)}(\hat{\vartheta}_n)} \xra[n \to \infty]{d} \chi_{s-r}^2, \]
    wobei $r$ die Anzahl der geschätzten Parameter ist. %$\hat{\vartheta}_n = (\hat{\vartheta}_{n,1}, \ldots, \hat{\vartheta}_{n,r})$.
    \item Manchmal wird die Parameter-Schätzung auch direkt aus der SP $x_1, \ldots, x_n$ ermittelt (\zB{} $\tilde{\mu}_n \coloneqq \tfrac{1}{n} (x_1 + \ldots + x_n)$ für den MW einer Normalverteilung).
    In manchen Fällen kann dann auf die Reduktion der Freiheitsgrade von $s$ auf $s-r$ verzichtet werden.
  \end{itemize}
\end{bemn}

% Klassisches Beispiel: Hufschlagtote in der preußischen Armee

% 2.2. Chi-Quadrat-Unabhängigkeitstest, Kontingenztafeln

\begin{ziel}
  Überprüfen, ob die Komponenten $X$ und $Y$ eines zweidim. Zufallsvektors $(X, Y)^T$ unabhängig sind.
\end{ziel}

\begin{verf}
  Seien $I_1, \ldots, I_k \subset \R^{n_1}$ und $J_1, \ldots, J_l \subset \R^{n_2}$ jeweils Familien paarweise disjunkter Mengen mit $\P(X \in I_1 \cup \ldots \cup I_k) = 1$ bzw. $\P(Y \in J_1 \cup \ldots \cup J_l) = 1$.
  Wir setzen
  \begin{align*}
    & p_{ij} \coloneqq \P((X, Y) \in I_i \times J_j) = \P(\{ X \in I_i \} \cap \{ X_j \in J_j \}), \\
    & p_{i\bullet} \coloneqq \sum_{j=1}^l p_{ij} = \P(X \in I_i), \quad
    p_{\bullet j} \coloneqq \sum_{i=1}^k p_{ij} = \P(Y \in J_j).
  \end{align*}
  Wir wollen nun die Nullhypothese $H_0 : \fa{(i, j)} p_{ij} = p_{i \bullet} \cdot p_{\bullet j}$ gegen $H_1 : \ex{(i,j)} p_{ij} \neq p_{i \bullet} \cdot p_{\bullet j}$ testen.
  Wir zählen dazu die Häufigkeiten einer Stichprobe $(X_1, Y_1), \ldots, (X_n, Y_n)$:
  \begin{align*}
    & h_{ij}^{(n)} \coloneqq \abs{\Set{m \in \{ 1, \ldots, n \}}{(X_m, Y_m) \in I_i \times J_j}}, \\
    & h_{i \bullet} \coloneqq \sum_{j=1}^l h_{ij}, \quad
    h_{\bullet j} \coloneqq \sum_{i=1}^k h_{ij}.
  \end{align*}
  Diese Häufigkeiten werden in einer \emph{Kontingenztafel} dargestellt:
  \begin{center}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{c | c c c c | c}
      & $1$ & $2$ & $\cdots$ & $l$ & \\ \hline
      $1$ & $h_{11}^{(n)}$ & $h_{12}^{(n)}$ & $\cdots$ & $h_{1l}^{(n)}$ & $h_{1 \bullet}^{(n)}$ \\
      $2$ & $h_{21}^{(n)}$ & $h_{22}^{(n)}$ & $\cdots$ & $h_{2l}^{(n)}$ & $h_{2 \bullet}^{(n)}$ \\
      \vdots & \vdots & \vdots & & \vdots & $\vdots$ \\
      $k$ & $h_{k1}^{(n)}$ & $h_{k2}^{(n)}$ & $\cdots$ & $h_{kl}^{(n)}$ & $h_{k \bullet}^{(n)}$ \\ \hline
      & $h_{\bullet 1}^{(n)}$ & $h_{\bullet 2}^{(n)}$ & $\cdots$ & $h_{\bullet l}^{(n)}$ & n
    \end{tabular}
  \end{center}
  Wir können den Test nun wie folgt als Spezialfall des $\chi^2$-Anpas- sungstests verstehen: Die Nullhypothese ist, dass die Verteilung von $(X, Y)$ das Produkt der Verteilungen von $X$ und $Y$ ist.
  Dabei schätzen wir zunächst die Verteilungen von $X$ und $Y$ mit
  \begin{align*}
    & L(h_{1 \bullet}^{(n)}, \ldots, h_{k \bullet}^{(n)}, h_{\bullet 1}^{(n)}, \ldots, h_{\bullet l}^{(n)}; p_{1 \bullet}, \ldots p_{k-1, \bullet}, p_{\bullet 1}, \ldots, p_{\bullet, l-1}) \\
    \coloneqq & \prod_{i=1}^{k} (p_{i \bullet})^{h_{i \bullet}^{(n)}} \cdot \prod_{j=1}^l (p_{\bullet j})^{h_{\bullet j}^{(n)}}.
  \end{align*}
  Diese Funktion wird maximal bei $\hat{p}_{i \bullet} = \nicefrac{h_{i \bullet}^{(n)}}{n}$ und $\hat{p}_{\bullet j}^{(n)} = \nicefrac{h_{\bullet j}^{(n)}}{n}$. \\
  % Das sind insgesamt $k+l-2$ zu schätzende Parameter.
  % Vorlesung vom 2.11.2015
  Als Test-Statistik verwenden wir
  \begin{align*}
    \hat{T}_{k,l}^{(n)} \coloneqq &
    \sum_{i=1}^k \sum_{j=1}^l \frac{(h_{ij}^{(n)} - n \hat{p}_{i \bullet} \hat{p}_{\bullet j})^2}{n \cdot \hat{p}_{i \bullet} \hat{p}_{\bullet j}} =
    n \sum_{i=1}^k \sum_{j=1}^l \frac{\left( h_{ij}^{(n)} - \nicefrac{h_{i \bullet}^{(n)} \cdot h_{\bullet j}^{(n)}}{n} \right)^2}{h_{i \bullet}^{(n)} \cdot h_{\bullet j}^{(n)}} \\
    & \xra[n \to \infty]{d} \chi^2_{kl - 1 - (k{-}1) - (l{-}1)} = \chi^2_{(k-1)(l-1)}
  \end{align*}
  Testregel: Die Nullhypothese wird genau dann abgelehnt, falls
  \[ \hat{T}_{k,l}^{(n)} > \chi^2_{(k-1)(l-1),1-\alpha}. \]
\end{verf}

\begin{bemn}
  \begin{itemize}
    \item Zum Testen eines höherdim. ZV $(X_1, \ldots, X_r)$ auf Unabhängigkeit aller Komponenten untersuchen wir die Ereignisse
    \[
      (X_1, \ldots, X_r) \in I_{i_1}^{(1)} \times \ldots \times I_{i_r}^{(r)} \quad
      \text{für $(i_1, \ldots, i_r) \in \bigtimes_{j=1}^r \{ 1, \ldots, k_j \}$}
    \]
    für eine passende Intervalleinteilung.
    Wir verwenden dann
    \begin{align*}
      \hat{T}_{k_1, \ldots, k_r}^{(n)} \coloneqq
      & n^{r-1} \sum_{i_1=1}^{k_1} \cdots \sum_{i_r=1}^{k_r} \frac{\left(h_{i_1 \cdots i_r}^{(n)} - n^{-r + 1} \prod_{s=1}^r h_{\bullet \cdots i_j \cdots \bullet}^{(n)}\right)^2}{\prod_{s=1}^r h_{\bullet \cdots i_j \cdots \bullet}} \\
      & \xra[n \to \infty]{d} \chi^2_{k_1 \cdots k_s - k_1 - \ldots - k_r + r - 1}
    \end{align*}
    \item Im Spezialfall $k \!=\! l \!=\! 2$ (Vierfeldertafel) hat die Statistik die Form
    \[
      \hat{T}_{2,2}^{(n)} =
      n \cdot \frac{\left(h_{11}^{(n)} \cdot h_{22}^{(n)} - h_{12}^{(n)} \cdot h_{21}^{(n)}\right)^2}{h_{\bullet 1}^{(n)} \cdot h_{\bullet 2}^{(n)} \cdot h_{1 \bullet}^{(n)} \cdot h_{2 \bullet}^{(n)}}
      \xra[n \to \infty]{d} \chi^2_n = \Normal^2(0, 1)
    \]
    und wir lehnen $H_0$ genau dann ab, wenn $\hat{T}_{2,2}^{(n)} > \chi^2_{1,1-\alpha} = z^2_{1 - \nicefrac{\alpha}{2}}$.
  \end{itemize}
\end{bemn}

% 2.3. Kolmogorow-Smirnow-Test
\section{Ein- und Zwei-SP-Tests für VFen}

\begin{situation}
  Sei $X_1, \ldots, X_n \sim F$ eine math. SP.
  Wir sortieren die dabei gezogenen Werte aufsteigend: $X_{1:n} \leq X_{2:n} \leq \ldots \leq X_{n:n}$.
  Dann heißt $\hat{F}_n(x) \coloneqq \nicefrac{1}{n} \sum_{i=1}^n \ind_{\ocinterval{-\infty}{x}}(X_{i:n})$ \emph{empirische VF}.
\end{situation}

\begin{satz}[Gliwenko, Hauptsatz der math. Statistik]
  \[ \sup_{x \in \R^1} \abs{\hat{F}_n(x) - F(x)} \xra[n \to \infty]{\text{$\P$-f.\,s.}} 0 \]
\end{satz}

\begin{lem}
  Sei $F$ stetig.
  Dann ist die Verteilung von $\sup_x \abs{\hat{F}_n(x) - F(x)}$ nicht von der VF $F$ abhängig.
  Genauer:
  \[ \sup_x \abs{\hat{F}_n(x) - F(x)} \eqqd \sup_{0 \leq y \leq 1} \abs{\hat{G}_n(y) - G(y)}, \]
  wobei $\hat{G}_n(y) \coloneqq \nicefrac{1}{n} \sum_{i=1}^n \ind_{\cinterval{0}{y}}(U_i)$ für $U_1, \ldots, U_n \sim \Uniform \cinterval{0}{1}$ \iid{}
\end{lem}

% Vorlesung vom 10.11.2015

\begin{kor}
  Sei $F$ stetig, $n \geq 1$.
  Dann ist die VF
  \[ K_n(z) \coloneqq \P(\sqrt{n} \cdot \sup_{x \in \R} \abs{\hat{F}_n(x) - F(x)} \leq z) \]
  unabhängig von $F$.
\end{kor}

\begin{satz}
  Falls $F$ stetig ist, gilt
  \[ K_n(z) \xra[n \to \infty]{} K(z) \coloneqq \sum_{k = - \infty}^\infty (-1)^k \exp(-2 k^2 z^2). \]
\end{satz}

\begin{defn}
  Dabei ist $K$ die VF der \emph{Kolmogorow-Verteilung}.
\end{defn}

\begin{bem}
  Man zeigt dazu, dass die Folge $X_n : y \mapsto \sqrt{n} \cdot (\hat{G}_n(x) - x)$ gegen die \emph{Brownsche Brücke} $\dot{B}$ konvergiert.
  Für diese gilt
  \[ \sup_{0 \leq x \leq 1} \abs{\dot{B}(x)} \sim K. \]
\end{bem}

\begin{entscheidungsregel}[\emph{Kolmogorow(-Smirnow)-1SP-Test}] \mbox{}\\
  Wir testen $H_0 : F = F_0$ gegen $H_1 : F \neq F_1$.
  Dabei muss $F_0$ eine stetige VF sein.
  Wir verwenden dazu
  \[ T_n \coloneqq \sqrt{n} \cdot \sup_{x \in \R} \abs{\hat{F}_n(x) - F(x)}. \]
  Wir lehnen $H_0$ genau dann ab, wenn $T_n > K_{1-\alpha}$.
\end{entscheidungsregel}

\begin{bemn}
  \begin{itemize}
    \item Für kleine $n \in \N$ sollte man $K_{n,1-\alpha}$ verwenden.
    \item Für große $z$ ist $K(z) \approx 1 - 2 \exp(-2 z^2)$, also $K_{1-\alpha} \approx \sqrt{- \nicefrac{1}{2} \cdot \log(\nicefrac{\alpha}{2})}$ für $\alpha$ klein.
    \item Das Supremum in $T_n$ liegt bei einer Sprungstelle von $\hat{F}_n$.
  \end{itemize}
\end{bemn}

\begin{entscheidungsregel}
  Um $H_0 : F = F_0$ gegen $H_1 : F > F_0$ mit %der Statistik
  \[ T_n^{+} \coloneqq \sqrt{n} \cdot \sup_{x \in \R} (\hat{F}_n(x) - F(x)). \]
  Es gilt
  \[ K_n^{+}(z) \coloneqq \P(T_n^{+} \leq z) \xra[n \to \infty]{} K^{+}(z) \coloneqq 1 - \exp(-2 \max(0, z)^2) \]
  Wir lehnen $H_0$ ab, falls $T_n^{+} > K^+_{1-\alpha}$.
\end{entscheidungsregel}

\begin{acht}
  Der Kolmogorow-Test kann nicht verwendet werden, wenn die Parameter von $F_0$ aus der Stichprobe geschätzt werden.
\end{acht}

% Bemerkung: Es gibt keine Entsprechung für mehrdimensionale ZVen

% Vorlesung vom 12.11.2015

% Analoges Vorgehen beim Cramér-von-Mises-Test:

\begin{defn}
  $\omega_n^2(g) = n \Int{\R^1}{}{g(F(x)) \left( \hat{F}_n(x) - F(x) \right)^2}{F(x)}$ \\
  heißt gewichtete \emph{Cramér-von-Mises-Statistik} oder $\omega^2$-Statistik.
  Dabei ist $g : \cinterval{0}{1} \to \cinterval{0}{\infty}$ eine Gewichtsfktn.
  Häufig verwendet wird
  $g(x) \coloneqq 1$
  und die \emph{Anderson-Darling-Statistik} $g(x) \coloneqq \tfrac{1}{x (1-x)}$.
\end{defn}

\begin{satz}
  Sei $F$ stetig.
  Dann ist
  \[
    \omega_n^2(g) \eqqd n \Int{0}{1}{g(u) \left( \hat{G}_n(u) - u \right)^2}{u}
    \xra[n \to \infty]{d} \Int{0}{1}{g(u) (\dot{B}(u))^2}{u} =: \omega^2(g).
  \]
\end{satz}

\begin{entscheidungsregel}[\emph{CvM-Test}]
  Wir testen $H_0 : F = F_0$ vs. $H_1 : F \neq F_0$ anhand der CvM-Statistik.
  Wir lehnen $H_0$ genau dann ab, wenn $\omega_n^2(g) > \omega_{1-\alpha}^2(g)$.
\end{entscheidungsregel}

\begin{bem}
  Der rechte Wert ist tabelliert für wichtige Funktionen $g$.
\end{bem}

% KS-(2-Stichproben)-Test

\begin{situation}
  Gegeben seien zwei unabhängige SPn $X_1, \ldots, X_n \sim F$ \iid{} und $X_1^*, \ldots, X_m^* \sim F^*$ \iid{}, wobei $F$ und $F^*$ stetig sind. \\
  Wir wollen testen, ob $H_0 : F = F^*$ oder $H_1 : F \neq F^*$ gilt, indem wir die empirischen VFen $\hat{F}_n$ und $\hat{F}_m^*$ vergleichen.
  Dazu verwenden wir
  \[ T_{m,n} \coloneqq \sqrt{\tfrac{m \cdot n}{m + n}} \sup_{x \in \R^1} \abs{\hat{F}_n(x) - \hat{F}_n^*(x)} \]
\end{situation}

\begin{satz}
  Falls $F = F^*$ stetig ist, so gilt
  \[ T_{m,n} \eqqd \sqrt{\tfrac{m \cdot n}{m + n}} \sup_{0 \leq u \leq 1} \abs{\tfrac{1}{n} \sum_{i=1}^n \ind_{\cinterval{0}{u}} (U_i) - \tfrac{1}{m} \sum_{j=1}^m \ind_{\cinterval{0}{u}} (U_j^*)}, \]
  wobei $X_i \eqqd F^{-}(U_i)$, $i = 1, \ldots, n$, \enspace
  $X_j^* \eqqd F^{*,-}(U_j^*)$, $j = 1, \ldots, m$ und
  %$F^{-}$ die Quantilfunktion ist:
  \[
    F^{-}(t) \coloneqq \begin{cases}
      \min \Set{x \in \R^1}{F(x) \geq t} & 0 < t \leq 1, \\
      \lim_{t \downarrow 0} F^{-}(t) & t = 0.
    \end{cases}
    \tag{Quantilfunktion}
  \]
  % (nichtfallend, linksstetig)
\end{satz}

\iffalse
\begin{bem}
  Asymptotik von $T_{n,m}$ für $m, n \to \infty$
  \[
    X_{m,n}(u) \coloneqq \sqrt{\tfrac{m \cdot n}{m + n}} \left( \hat{G}_n(u) - \hat{G}^*_m(u) \right), \quad
    0 \leq u \leq 1.
  \]
  \[
    \E X_{m,n} = \sqrt{\tfrac{m \cdot n}{m + n}} \left( \E \ind_{\cinterval{0}{u}}(U_1) - \E \ind_{\cinterval{0}{u}}(U^*_1) \right) = 0
  \]
  \[
    \var(X_{m,n}) = \tfrac{m \cdot n}{m + n} \left( \E (\hat{G}_n(u))^2 + \E (\hat{G}_m(u))^2 - 2 \E \hat{G}^*_n(u) \cdot \E \hat{G}^*_m(u) \right)
  \]
  % (...)

  Genauso wie oben ergibt sich
  \[
    (X_{m,n}(u_1), \ldots, X_{m,n}(u_k)) \xra[n \to \infty]{d} \Normal_k(0, \Sigma)
    \quad \text{mit} \quad
    \Sigma_{ij} = u_i \wedge u_j - u_i \cdot u_j.
  \]

  Daraus folgt die schwache Konvergenz
  \[ X_{m,n}(\blank) \xra[n \to \infty]{d} \dot{B}(\blank) \]
  im Skorodoch-Raum $\mathcal{D} \cinterval{0}{1}$.
\end{bem}
\fi

\begin{lem}
  $T_{m,n} \xra[n \to \infty]{d} \sup_{0 \leq u \leq 1} \abs{\dot{B}(u)} \sim K$
\end{lem}

\begin{entscheidungsregel}[\emph{Kolmogorow(-Smirnow)-2SP-Test}] \mbox{}\\
  $H_0 : F = F^*$ wird genau dann abgelehnt, falls $T_{m,n} > K_{1-\alpha}$.
\end{entscheidungsregel}

% Vorlesung vom 16.11.2015

% §2.4. 2-Stichprobentest von Wilcoxon-Mann-Whitney (U-Test)

\begin{situation}[\emph{2-SP-Test von Wilcoxon-Mann-Whitney}, U-Test]
  Gegeben seien zwei unabhängige SPn $X_1, \ldots, X_n \sim F$ und $X_1^{*}, \ldots, X_m^{*} \sim F^{*}$, wobei $F$ und $F^*$ stetig sind.
  Ziel: Prüfen von $H_0 : F = F^{*}$ vs. $H_1 : F \neq F^{*}$.
  Konstruktion einer Rangstatistik für konkrete SPn $x_1, \ldots, x_n$ und $x_1^{*}, \ldots, x_m^{*}$.
  \begin{enumerate}
    \item Ordnen: $x_{1:n} < \ldots < x_{n:n}$ und $x_{1:m}^{*} < \ldots < x_{m:m}^{*}$
    \item $\nu_1, \ldots, \nu_m \in \{ 1, \ldots, m+n \}$ seien die Ränge der Werte $x_{i:m}^{*}$ innerhalb der Gesamt-SP:
    \begin{align*}
      x_{1:n} & < \ldots < x_{\nu_1 - 1:n} < x_1^{*} < x_{\nu_1:n} < \ldots < x_{\nu_2 - 2:n} < x_{2:m}^{*} < x_{\nu_2 - 1:n} \\
      & < \ldots < x_{\nu_m-m:n} < x_{m:m}^{*} < x_{\nu_m - m + 1 : n} < \ldots < x_{n:n}
    \end{align*}
  \end{enumerate}
  Heuristik: $H_0$ wird angenommen, falls sich die $x$- und $x^{*}$-Werte "`gut durchmischen"', \dh{} die Anzahl der $x$-Werte, die vor bzw. nach den $x^{*}$-Werten liegen, darf nicht zu groß werden.
  Testgröße:
  \begin{align*}
    W_{m,n} & \coloneqq \! \sum_{i=1}^n \sum_{j=1}^m \ind_{\{ X_i < X_j^{*} \}} \!=\! \abs{\Set{(i, j)}{ X_i \!<\! X_j^{*}}} \!=\! \sum_{j=1}^m \abs{\Set{i}{X_i \!<\! X_{j:m}^{*}}} \\
    & = \sum_{j=1}^m (\nu_j - j) = \nu_1 + \ldots + \nu_m - \tfrac{m}{2} (m+1)
  \end{align*}
  Unter $H_0$ gilt \quad
  $\E W_{m,n} = \tfrac{m \cdot n}{2}$, \quad
  $\var W_{m,n} = \tfrac{m \cdot n}{12} (m + n + 1)$.
\end{situation}

\begin{lem}
  Unter $H_0 : F = F^*$ stetig gilt
  \begin{align*}
    g_{m,n}(z) & \coloneqq \sum_{k=0}^{n \cdot m} \P(W_{m,n} = k) \cdot z^k = \frac{z^{-m (m+1) / 2}}{\binom{m + n}{m}} \enspace\qquad \sum_{\mathclap{1 \leq \nu_1 < \ldots < \nu_m \leq m+n}} \enspace z^{\nu_1 + \ldots + \nu_m} \\
    & = \tfrac{1}{\binom{m + n}{m}} \prod_{k=1}^m \frac{1 - z^{n+k}}{1 - z^k}
  \end{align*}
\end{lem}

\begin{entscheidungsregel}
  Ablehnung von $H_0$, falls $w_{m,n} \leq c_{\nicefrac{\alpha}{2}}$ oder $w_{m,n} \geq m \cdot n - c_{\nicefrac{\alpha}{2}}$, wobei
  \[
    c_{\nicefrac{\alpha}{2}} = \min \Set{k \geq 0}{\P(W_{m,n} \leq k) = \P(W_{m,n} \geq m \cdot n - k) \geq \nicefrac{\alpha}{2}}.
  \]
  % ($c_{\nicefrac{\alpha}{2}} - \tfrac{m \cdot n}{2}$ ist Quantil der Ordnung $\tfrac{\alpha}{2}$ der ZG $W_{m,n} - \tfrac{m \cdot n}{2}$).
  Annahme von $H_0$ genau dann, wenn $\abs{w_{m,n} - \tfrac{m \cdot n}{2}} < \tfrac{m \cdot n}{2} - c_{\nicefrac{\alpha}{2}}$
\end{entscheidungsregel}

\begin{satz}
  Unter $H_0 : F = F^{*}$ stetig gilt
  \[ T_{m,n} \coloneqq \frac{W_{m,n} - \tfrac{m \cdot n}{2}}{\sqrt{\tfrac{m \cdot n}{2} (m + n + 1)}} \xra[m, n \to \infty]{d} \Normal(0, 1) \]
\end{satz}

% §2.5 Kruskal-Wallis-Test

\begin{samepage}

\begin{bem}[\emph{Kruskal-Wallis-Test}]
  Gegeben seien $k$ Messreihen $X_{i,1}, \ldots, X_{i,n_i} \sim F_i$, $i = 1, \ldots, k$ unabhängige SPn, $F_i$ stetig. \\
  Ziel: Testen von $H_0 : F_1 = \ldots = F_k$.
  Vorgehen:
  \begin{enumerate}
    \item Ordnen der Beobachtungen der Größe nach
    \item $\nu_{i,1} < \ldots < \nu_{i,n_i}$ Platznummern der $n_i$ Beobachtungen der $i$-ten Messreihe in der Gesamt-SP
    \item $\overline{\nu}_i \coloneqq \tfrac{1}{n_i} (\nu_{i,1} + \ldots + \nu_{i,n_i})$, $\overline{\nu} \coloneqq \tfrac{1}{n} \sum_{i=1}^k n_i \overline{\nu}_i$ mit $n \coloneqq n_1 + \ldots + n_k$.
  \end{enumerate}
  Heuristik: $H_0$ ist richtig, falls $\overline{\nu}_i \approx \overline{\nu}$ für alle $i$.
  Testgröße:
  \[ \tfrac{12}{n (n+1)} \sum_{i=1}^k n_i (\overline{\nu}_i - \tfrac{n+1}{2})^2 \xra[n_i \to \infty]{d} \chi^2_{k-1} \]
\end{bem}

% Faustregel: $\min_{1 \leq i \leq k} n_i \geq 5$, $k \geq 4$

% §3. U-Statistiken
\section{Theorie der U-Statistiken}

\end{samepage}

% §3.1. Hoeffdings Projektionsmethode und ZGWS

\begin{situation}
  Sei $n \geq m$, $X_1, \ldots, X_n \sim F$ \iid{}, $h : \R^m \to \R^1$ Borel-messbar und symmetrisch, \dh{}
  \[
    h(x_1, \ldots, x_n) = h(x_{\sigma(1)}, \ldots, x_{\sigma(n)}) \quad
    \forall \, \sigma \in S_n.
  \]
  Gelte $\E \abs{h(X_1, \ldots, X_m)} < \infty$.
\end{situation}

\begin{defn}
  Die \emph{U-Statistik der Ordnung $m$} mit Kernfunktion $h$ ist
  \[ U_n^{(m)} \coloneqq \tfrac{1}{\binom{n}{m}} \sum_{1 \leq i_1 < \ldots < i_m \leq n} h(X_{i_1}, \ldots, X_{i_m}). \]
\end{defn}

\begin{bem}
  Offenbar: $\E U_n^{(m)} = \E h(X_1, \ldots, X_m)$.
\end{bem}

\begin{bsp}
  Für $m=2$ gilt $\sigma^2 = \var(X_1) = \tfrac{1}{2} \E (X_1 - X_2)^2$.
  Davon inspiriert setzen wir $h(x_1, x_2) \coloneqq \tfrac{1}{2}(x_1 - x_2)^2$.
  Damit haben wir
  \[
    U_n^{(2)} = \tfrac{2}{n (n-1)} \sum_{1 \leq i < j \leq n} \tfrac{1}{2} (X_i - X_j)^2
    %= \tfrac{1}{n (n-1)} ( (n-1) \sum_{i=1}^n X_i^2 - (X_1 + \ldots X_n)^2 + \sum_{i=1}^n X_i^2 ) - X_n
    %= \tfrac{1}{n} (X_1 + \ldots + X_n)
    %= \tfrac{1}{n-1} ( \sum_{i=1}^n X_i^2 - n (\overline{X}_n)^2 )
    = \tfrac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X}_n)^2 = S_n^2
  \]
\end{bsp}

\begin{ziel}
  Wir würden gerne den ZGWS auf $U_n^{(m)}$ anwenden.
  Problem dabei: Die Summanden in der Def. von $U_n^{(m)}$ sind nicht unabhängig.
  Wir approximieren deshalb $U_n^{(m)}$ mit einer Summe von \iid{} ZGn.
\end{ziel}

\begin{lem}
  Sei \enspace
  $\tilde{U}_n^{(m)} = \theta + \sum_{i=1}^n (\underbrace{\E(U_n^{(m)} | X_i)}_{\text{\iid{}}} - \theta)$ \enspace
  mit $\theta \coloneqq \E U_n^{(m)}$ und
  \begin{align*}
    g(x) & = \E[h(X_1, \ldots, X_n) | X_1 = x ] = \E h(x, X_2, \ldots, X_m) \\
    & = \Int{}{}{\cdots \Int{}{}{h(x, x_2, \ldots, x_n)}{F(x_2)} \cdots}{F(x_n)}.
  \end{align*}
  Falls $\E h^2(X_1, \ldots, X_m) < \infty$, so gilt
  \begin{enumerate}[label=(\arabic*), itemindent=6pt]
    \item $\var(U_n^{(m)} - \tilde{U}_n^{(m)}) = \var(U_n^{(m)}) - \var(\tilde{U}_n^{(m)})$
    \item $\E (U_n^{(m)} | X_i = x) = \theta + \tfrac{m}{n} (g(x) - \theta)$
  \end{enumerate}
\end{lem}

\begin{lem}
  \begin{enumerate}[label=(\arabic*), itemindent=6pt]
    \setcounter{enumi}{1}
    \item $\var(\tilde{U}^{(m)}_n) = \tfrac{m^2}{n} \cdot \var(g(X_1)) = \tfrac{m^2}{2} ( \E g^2(X_1) - \theta^2 )$
    \item Falls $\E \abs{h(X_1, \ldots, X_m)} < \infty$, so gilt
    \begin{align*}
      \var(U_n^{(m)}) & = \tfrac{1}{\binom{n}{m}} \sum_{k=1}^m \binom{m}{k} \binom{n - m}{m - k} \cdot \zeta_k \quad \text{mit} \\
      h_k(x_1, \ldots, x_k) & \coloneqq \E(h(x_1, \ldots, x_k, X_{k+1}, \ldots, X_m) \\
      \zeta_k & \coloneqq \var(h_k(X_1, \ldots, X_k)) \\
      & =
      \arraycolsep=0.2pt
      \begin{array}[t]{l l}
        \E[ & h(X_1, \nldots, X_k, X_{k+1}, \nldots, X_m) \cdot \\
        & h(X_1, \nldots, X_k, X_{m+1}, \nldots, X_{2m-k})] - \theta^2
      \end{array}
    \end{align*}
  \end{enumerate}
\end{lem}

\begin{kor}
  Aus (1), (3) und (4) folgt für $m=2$:
  \[
    \var(U_n - \tilde{U}_n)
    = \var(U_n) - \var(\tilde{U}_n)
    %& = \tfrac{1}{n (n-1)} \left( \E (\overline{h}(X_1, X_2)^2) - 2 (n-2) \E \overline{h}(X_1, X_2) \overline{h}(X_2, X_3) \right) - \tfrac{4}{n} \var(g(X_1)) \\
    = \ldots
    = - \tfrac{4}{n (n-1)} \var(g(X_1))
  \]
  Für $m \geq 2$ gilt $\var(U_n^{(m)} - \tilde{U}_n^{(m)}) \leq \tfrac{c(m)}{n^2} \var(h(X_1, \ldots, X_m))$.
\end{kor}

% Vorlesung vom 23.11.2015

% §3.2. ZGWS für (nichtentartete) U-Statistiken

\begin{satz}[\emph{Hoeffding}]
  Sei $U_n^{(m)}$ eine U-Statistik mit Kern $h : \R^m \to \R$, sodass $\E h^2(X_1, \ldots, X_m) < \infty$ und $\sigma_g^2 \coloneqq \var(g(X_1)) > 0$.
  Dann gilt
  %Für eine U-Statistik $U_n^{(m)}$ mit Kern $h : \R^m \to \R$, sodass $\E h^2(X_1, \ldots, X_m) < \infty$ und $\sigma_g^2 \coloneqq \var(G(X_1)) > 0$, gilt
  \[ \sqrt{n} (U_n^{(m)} - \theta) \xra[n \to \infty]{d} \Normal(0, \sigma_g^2). \]
\end{satz}

\begin{bemn}
  \begin{itemize}
    \item Der Fall $\var(g(X_1)) = 0$ (entarteter Fall) zieht eine kompliziertere Asymptotik nach sich. % (Übungsaufgabe dazu)
    \item $\E g^2(X_1) < \infty$ ist schwächer als $\E h^2(X_1, \ldots, X_m) < \infty$.
    \item Aus $E \abs{h(X_1, \ldots, X_m)}^{1+q} < \infty$ für $0 < q \leq 1$ folgt %die Abschätzung
    \[
      \E \abs{\sqrt{n} (U_n^{(m)} - \tilde{U}_n^{(m)})}^{1+q} \leq \tfrac{c(q, m)}{n^{2q}} \E \abs{h(X_1, \ldots, X_m)}^{1+q}.
    \]
    Mit einer Abschneidetechnik zeigt man, dass $\E g^2(X^1) < \infty$ und $\E \abs{h(X_1, \ldots, X_m)}^{\frac{4}{3}} < \infty$ schon für $\P(\sqrt{n} \abs{U_n^{(m)} - \tilde{U}_n^{(m)}} < \epsilon) \to 0$ für alle $\epsilon > 0$ ausreichen und damit für den Satz von Hoeffding.
    \item U-Statistiken erweisen sich (unter gewissen Bedingungen) als suffiziente Schätzer mit minimaler Varianz.
  \end{itemize}
\end{bemn}

\begin{bsp}
  Wir betrachten die U-Statistik $S_n^2 = \binom{n}{2}^{-1} \sum_{i < j} \tfrac{1}{2} (X_i - X_j)^2$. \\
  Dann ist $g(x) = \tfrac{1}{2} (x - \E X_1)^2 + \tfrac{1}{2} \sigma^2$ mit $\sigma^2 \coloneqq \var(X_1)$.
  Es gilt
  \[ \sqrt{n} (S_n^2 - \sigma^2) \xra[n \to \infty]{d} \Normal(0, 4 \sigma_g^2) \]
  mit $\sigma_g^2 = \E g^2(X_2) - (\E g(X_2))^2 = \tfrac{1}{4} \mu_4 - \tfrac{1}{4} \sigma^4$, $\mu_4 \coloneqq \E (X_1 - \E X_2)^4$.
  
  Spezialfall: Ist $X_i \sim \Normal(\mu, \sigma^2)$, so gilt $\mu_4 = 3 \sigma^4$. \\
  Dann gilt $\sqrt{n} (S_n^2 - \sigma^2) \xra[n \to \infty]{d} \Normal(0, 2 \sigma^4)$.
  Es folgt
  \[ \frac{ \sqrt{n} (S_n^2 - \sigma^2) }{\sqrt{2 (S_n^2)^2}} = \sqrt{\nicefrac{n}{2}} \left( 1 - \frac{\sigma^2}{S_n^2} \right) \xra[n \to \infty]{d} \Normal(0, 1). \]
  Alternativ erhält man durch Anwenden einer varianzstab. Trafo:
  \[
    \sqrt{\nicefrac{n}{2}} (\log S_n^2 - \log \sigma^2) \xra[n \to \infty]{d} \Normal(0, 1).
  \]
\end{bsp}


\begin{defn}
  Die \emph{Kumulante} oder \emph{Semi-Invariante} $m$-ter Ordnung ist
  \[ \Cum_m(X) = \tfrac{1}{m! 2^m} \tfrac{\partial^m}{\partial t^m}|_{t=0} \log \E e^{it X}. \]
\end{defn}

\begin{bem}
  Falls $X_1$, \ldots, $X_n$ unabhängig sind, so gilt
  \[ \Cum_m(X_1 + \ldots + X_n) = \Cum_m(X_1) + \ldots + \Cum_m(X_n). \]
  Für $m=3$ gilt $\Cum_3(X) = \E X^3 - 3 \E X \cdot \E X^2 + 2 (\E X)^3$.
\end{bem}

\begin{bsp}
  Schätzung der Kumulante $m$-ter Ord. mit der SP $X_1, \ldots, X_n$:
  \begin{align*}
    (\widehat{\Cum_3(X)})_n & \coloneqq \tfrac{1}{n (n-1) (n-2)} (n^2 \hat{M}_3^{(n)} - 3 n \hat{M}_1^{(n)} \hat{M}_2^{(n)} - 2 (\hat{M}_1^{(n)})^3) \\
    & = \tfrac{1}{\binom{n}{3}} \sum_{1 \leq i < j < j \leq n} h(X_i, X_j, X_k) \\
    \text{mit} \enspace
    h(x, y, z) & \coloneqq \begin{array}[t]{l}
      - \tfrac{1}{2} (xy^2 + x^2 y + x z^2 + x^2 z + y z^2 + x^2 z + y z^2 + y^2 z) \\
      + \tfrac{1}{3} (x^3 + y^3 + z^3) + 2 xyz
    \end{array} \\
    \enspace \text{wobei} \enspace
    \hat{M}_j^{(n)} & \coloneqq \tfrac{1}{n} \sum_{i=1}^n X_i^j
  \end{align*}
\end{bsp}

\begin{defn}
  Eine VF $F$ heißt \emph{symmetrisch} bzgl. $\vartheta_0 \in \R^1$, falls
  \[
    F(\vartheta_0 - x) = 1 - F(\vartheta_0 + x) \quad
    \forall \, x \in \R^1.
  \]
\end{defn}

\begin{bsp}[\emph{Wilcoxon-1-SP-Test} auf Symmetrie]
  Sei $X_1, \ldots, X_n \sim F$ eine math. SP mit $F$ stetig.
  Angenommen, $F$ ist symmetrisch bzgl. $\vartheta_0 \in \R^1$.
  Dann sind $Z_i = X_i - \vartheta_0$ symmetrisch bzgl. $0$. \\
  Seien $\nu_1^{+}, \ldots, \nu_n^{+}$ die Ränge der ZGn $\abs{Z_1}, \ldots, \abs{Z_n}$.
  Setze
  \[ T_n^{+} = \sum_{i=1}^n \ind_{\{ Z_i > 0 \}} \nu_i^{+}. \]
  Unter $H_0 : \text{$F$ ist symmetrisch bzgl. $\vartheta_0$}$ gilt
  \[
    \E T_n^{+} = \tfrac{1}{2} \sum_{i=1}^n \E \nu_i^{+} = \tfrac{n (n+1)}{4}, \quad
    \var(T_n^{+}) = \tfrac{n}{24} (n + 1) (2n + 1).
  \]
\end{bsp}

\begin{bsp}
  Alternativ können wir die U-Statistik
  \[
    U_n = \tfrac{1}{\binom{n}{2}} \sum_{1 \leq i < j \leq n} \ind_{\{ Z_i + Z_j > 0 \}}.
  \]
  zum Test auf Symmetrie betrachten.
  Unter $H_0$ gilt
  \begin{align*}
    \E \ind_{\{ Z_i + Z_j > 0 \}}
    = \P(Z_1 > - Z_2)
    & = \IInt{(1 - F(-z))}{F(z)} \\
    & = \IInt{F(z)}{F(z)}
    = \tfrac{1}{2}.
  \end{align*}
  Aus dem ZGWS für U-Statistiken folgt
  \[
    \sqrt{n} (U_n - \tfrac{1}{2}) \xra[n \to \infty]{d} \Normal(0, \tfrac{1}{3}).
  \]
  Testregel: Wir lehnen $H_0$ genau dann ab, wenn \enspace
  $\abs{U_n - \tfrac{1}{2}} > \tfrac{z_{1 - \nicefrac{\alpha}{2}}}{\sqrt{3n}}$.
\end{bsp}

% Vorlesung vom 26.11.2015

% Ausgelassen: Beispiel: U-Statistiken in der stochastischen Geometrie

\begin{defn}
  Sei $h : \R^{m_1} \times \R^{m_2} \to \R^1$ Borel-messbar, symmetrisch in den ersten $m_1$ und den letzten $m_2$ Argumenten.
  Seien $X_1, \ldots, X_{n_1} \sim F$ und $X_1^*, \ldots, X_{n_2}^* \sim F^*$ zwei unabh. math. SPn.
  Dann heißt
  \[
    U_{n_1, n_2}^{(m_1, m_2)} \coloneqq \left( \binom{n_1}{m_1} \binom{n_2}{m_2} \right)^{-1} \qquad
    \sum_{\mathclap{\substack{1 \leq i_1 < \ldots < i_{m_1} \leq n_1 \\ 1 \leq j_1 < \ldots < j_{m_2} \leq n_2}}} \enspace
    h(X_{i_1}, \nldots, X_{i_{m_1}}, X_{j_1}^*, \nldots, X_{j_{m_2}}^*)
  \]
  (verallg.) \emph{U-Statistik} der Ordnung $(m_1, m_2)$ mit Kernfunktion $h$.
\end{defn}

\begin{nota}
  Sei $m_1 = m_2 = 1$.
  Wir setzen
  \begin{align*}
    \theta & \coloneqq \E h(X_1, X_1^*) = \E U_{n_1, n_2}^{(1, 1)} \\
    g_1(x) & \coloneqq \E( h(X_1, X_1^*) \mid X_1 = x), \quad
    \sigma_1^2 \coloneqq \var g(X_1), \\
    g_2(y) & \coloneqq \E( h(X_1, X_1^*) \mid X_1^* = y), \quad
    \sigma_2^2 \coloneqq \var g(X_1^*), \\
    \tilde{U}_{n_1, n_2}^{(1,1)} & \coloneqq \tfrac{1}{n_1} \sum_{i=1}^{n_1} g_1(X_i) + \tfrac{1}{n_2} \sum_{j=1}^{n_2} g_2(X_j^*) - \theta
  \end{align*}
\end{nota}

\begin{lem}
  Es seien $\E h^2(X_1, X_1^*) < \infty$ und $\sigma_1^2, \sigma_2^2 \in \ointerval{0}{\infty}$.
  Dann gilt
  \[
    \sqrt{\frac{n_1 n_2}{n_2 \sigma_1^2 + n_1 \sigma_2^2}} \cdot
    (U_{n_1, n_2} - \theta) \xra[n_1, n_2 \to \infty]{d} \Normal(0, 1).
  \]
\end{lem}

\begin{bsp}
  Die Wilcoxon-2-SP-Statistik ist eine U-Statistik mit
  \[ h(x, y) \coloneqq \abs{\Set{\heartsuit}{x < y}}. \]
\end{bsp}

% §4. Das allgemeine lineare Modell
% (Modell I  der Regressions- und Varianzanalyse)
\section{Das allgemeine lineare Modell}

\begin{modell}[allgemein]
  Für Zufallsgrößen $X$ und $Y$ gilt $Y = g(X) + \epsilon$ mit einer Funktion $g$, wobei $\E \epsilon = 0$ und $\sigma^2 \coloneqq \var (Y - g(X)) = \E \epsilon^2$.
  %Bei einfacher linearer Regression nimmt man an, dass $g(x) = a + bx$.
\end{modell}

\begin{modell}[\emph{Lineare Regression}]
  $Y = X \beta + \epsilon$, wobei
  \[
    \arraycolsep=1.4pt
    \begin{array}{r c l l}
      Y &=& (Y_1, \ldots, Y_n)^T \quad & \text{\emph{Beobachtungsvektor},} \\
      X &=& (x_{ij}) \in \R^{n \times p} & \text{\emph{Einstellgrößen-}, Versuchsplanmatrix,} \\
      \beta &=& (\beta_1, \ldots, \beta_p)^T & \text{(unbek.) \emph{Parametervektor}, Regressionskoeff.,} \\
      \epsilon &=& (\epsilon_1, \ldots, \epsilon_n)^T & \text{(nicht beobachtbarer) \emph{Fehlervektor} heißt.}
    \end{array}
  \]
  Wir nehmen an, dass $\cov(Y_i, Y_j) = \cov(\epsilon_i, \epsilon_j) = \sigma^2 \delta_{ij}$ und $n > p$. \\
  Dabei heißt $\sigma$ \emph{Modellstreuung}.
\end{modell}

% Bem: Zunächst keine Verteilungsvoraussetzungen

\begin{bem}
  Falls $Y$ eine bekannte Kovarianzmatrix $K \in \R^{n \times n}$ besitzt, so können wir $X^* \coloneqq K^{\nicefrac{-1}{2}} X$, $Y^* \coloneqq K^{\nicefrac{-1}{2}} Y$, $\epsilon^* \coloneqq K^{\nicefrac{-1}{2}} \epsilon$ setzen und erhalten $Y^* = X^* \beta + \epsilon^*$ und $\cov(Y*) = I_n$.
\end{bem}

% Aufgaben:
% * Schätzung von $\beta$ mit der Methode der kleinsten Quadrate (Problem dabei: $\rk X < p$)
% * Schätzung der linearen Funktion $c^T \beta = \sum_{j=1}^p c_j \beta_j$
% * Schätzung der Modellstreuung
% * Test- und Konfidenzintervalle

% §4.2 Die MkQ-Schätzung

\begin{problem}
  Gegeben seien $[Y, X \beta, \sigma^2 I_n]$. \\
  Gesucht sind Schätzungen $\hat{\beta}(y) = (\hat{\beta}_1(y), \ldots, \hat{\beta}_p(y))^T$ für $\beta$.
\end{problem}

\begin{defn}
  Eine Schätzfunktion $\hat{\beta}(y)$ heißt \emph{MkQ-Schätzung} (Methode der kleinsten Quadrate) für $\beta$, falls
  \[ S(y, \hat{\beta}) = \min_{\beta \in \R^p} S(y, \beta) \]
  wobei $S(y, \beta) \coloneqq \norm{y - X \beta}^2 = \sum_{i=1}^n (y_i - \sum_{j=1}^n x_{ij} \beta_j)^2$.
\end{defn}

\begin{bem}
  $S(y, \beta)$ besitzt lokale Minima, da
  \[
    \tfrac{\partial}{\partial \beta} S(y, \beta) = - 2 X^T y + 2 X^T X \beta, \quad
    \tfrac{\partial^2}{\partial \beta^2} S(y, \beta) = 2 X^T X.
  \]
  Für die Minima gelten die Normalengleichungen
  \[
    X^T X \beta = X^T Y \iff \sum_{j=1}^p \xi_{ij} \beta_j = \sum_{j=1}^n x_{ji} y_j \enspace
    \text{mit } (\xi_{ij}) = X^T X.
    \tag{N}
  \]
\end{bem}

\begin{satz}
  (N) ist stets lösbar und jede Lsg ist eine MkQ-Schätzung.
  Falls $\rk X = p$, so ist $\hat{\beta}$ eind. bestimmt durch $\hat{\beta} = (X^T X)^{-1} X^T y$.
\end{satz}

\begin{bsp}[\emph{Einfache lineare Regression}] \mbox{} \\
  Annahme: \enspace
  $Y_i = \beta_1 + \beta_2 x_i + \epsilon_i$, \enspace
  $i = 1, \ldots, n$.
  Dann ist
  \[
    X = \begin{pmatrix}
      1 & x_1 \\
      \vdots & \vdots \\
      1 & x_n
    \end{pmatrix} \quad
    \begin{array}{l}
      \hat{X^T X} = n \sum x_i^2 - \left( \sum x_i \right)^2 = n \cdot \sum (x_i - \overline{x_n})^2 > 0 \\[4pt]
      \hat{\beta} = \det(X X^T)^{-1} \begin{pmatrix}
        \sum x_i^2 & - \sum x_i \\
        - \sum x_i & n
      \end{pmatrix} \begin{pmatrix}
        \sum Y_i \\
        \sum x_i Y_i
      \end{pmatrix}
    \end{array}
  \]
\end{bsp}

% §4.1 Definition und Aufgabenstellung

\iffalse

% §5. Dichteschätzung und Regressionskurvenschätzer
\section{Dichte- und Regressionskurvenschätzer}

% §5.1 Dichteschätzung (allgemein)

\begin{nota}
  Sei $\mathcal{P}$ die Menge aller bezüglich des Lebesgue-Maß $\lambda_1$ absolut stetigen Wahrscheinlichkeitsmaße auf $\R^1$ und
  \[ \mathcal{F}_c \coloneqq \Set{f \in \Cont(\R^1)}{f = \nicefrac{\d P}{\d \lambda_1} \text{ für ein } P \in \mathcal{P}} \]
  die Menge der stetigen W-Dichtefunktionen.
\end{nota}

\begin{ziel}
  Finden einer Dichteschätzung $\hat{f}_n(X, \blank) : \R^1 \to \R^1$, wobei $X = (X_1, \ldots, X_n)$ eine math. SP ist, in Form einer messbaren Abb. $\hat{f}_n(\blank, \blank) : \R^n \times \R^1 \to \R^1$.
\end{ziel}

\begin{nota}
  $\hat{f}_n(t) \coloneqq \hat{f}_n(X_1, \ldots, X_n)$
\end{nota}

\begin{lem}
  Es gibt keinen Dichteschätzer $\hat{f}_n(\blank)$ mit
  \[
    \E_f \hat{f}_n(t) = f(t) \quad
    \text{für $\lambda_1$-fast alle $t$ für alle $f \in \mathcal{F}_c$.}
  \]
\end{lem}

\begin{defn}
  Sei $x_0 \in \R$, $h > 0$.
  Setze $I_j \coloneqq \cointerval{x_0 + jh}{x_0 + (j+1) h}$ für $j \in \Z$.
  Das \emph{Histogramm} ist der (naive) Dichte-Schätzer
  \[
    \hat{f}_n(t) \coloneqq \tfrac{1}{nh} \sum_{i=1}^n \ind_{I_j}(X_i), \quad
    \text{wobei $z \in \Z$ so ist, dass $t \in I_j$}.
  \]
\end{defn}

\begin{bem}
  Nach dem Gesetz der großen Zahlen gilt
  \[
    \hat{f}_n(t) \xra[n \to \infty]{\text{$\P_f$-f.s.}}
    h^{-1} \Int{I_j}{}{f(x)}{x} \quad
    \text{für $t \in I_j$.}
  \]
\end{bem}

\begin{defn}
  Sei $\hat{f}_n(\blank)$ ein Dichteschätzer und $f \in \mathcal{F}_c$.
  Dann heißt
  \[
    \Delta_n \coloneqq \E_f \Int{\R}{}{(\hat{f}_n(t) - f(t))^2}{t}
  \]
  \emph{MISE} (mean integrated squared error) bzgl. $f$.
\end{defn}

\begin{satz}[Freedman, Diaconis]
  Sei $f \in L^2(\R^1)$ und $f$ absolut stetig, \dh{} fast überall differenzierbar mit $\gamma \coloneqq \Int{\R^1}{}{(f'(t))^2}{t} > 0$.
  Setze
  \[
    \alpha \coloneqq \sqrt[3]{6} \cdot \gamma^{- \nicefrac{1}{3}}, \quad
    \beta \coloneqq \frac{3}{2 \sqrt[3]{6}} \gamma^{\nicefrac{1}{3}}.
  \]
  Dann gilt:
  \[
    \min_{h = h_n > 0} \Delta_n^2 = \tfrac{\beta}{n^{\nicefrac{2}{3}}} + o \left( \tfrac{1}{n^{\nicefrac{2}{3}}} \right) \quad
    \text{für }
    h_n = \tfrac{\alpha}{n^{\nicefrac{1}{3}}} + o \left( \tfrac{1}{n^{\nicefrac{1}{3}}} \right).
  \]
  % TODO: Was ist die Aussage davon???
\end{satz}

\begin{defn}
  Sei $K \in L^1(\R)$ eine Fktn mit $\IInt{K(t)}{t} = 1$ und $(h_n)_{n \in \N}$ eine Folge in $\ointerval{0}{\infty}$ mit $h_n \to 0$.
  Dann heißt
  \[
    \hat{f}_n(t) = \hat{f}_n(X_1, \ldots, X_n; t) \coloneqq \tfrac{1}{n \cdot h_n} \sum_{i=1}^n K \left( \tfrac{X_i - t}{h_n} \right)
  \]
  \emph{Kerndichteschätzer} für $f$ mit \emph{Kernfunktion} $K$.
  % auch: Parzen-Rosenblatt-Schätzung
\end{defn}

\begin{bspe}
  Mit der \emph{empirischen Dichte} $K(x) \coloneqq \tfrac{1}{2} \ind_{\ocinterval{-1}{1}}(x)$ gilt
  \[ \hat{f}_n(t) = \tfrac{1}{2 h_n} \left( \hat{F}_n(t + h_n) - \hat{F}_n(t - h_n) \right), \]
  mit dem \emph{Gauß-Kern} $K(x) \coloneqq \tfrac{1}{\sqrt{2 \pi}}\exp( \tfrac{- x^2}{2})$ gilt $\hat{f}_n(\blank) \in \Cont^\infty(\R)$.
\end{bspe}

\begin{lem}
  \begin{itemize}
    \item $\E_f \hat{f}_n(t) = \IInt{K(x) f(t + h_n x)}{x}$
    \item Falls $K \!\in\! L^2(\R)$: \enspace
    $\var_f (\hat{f}_n(t)) = 
    \begin{array}[t]{l}
      \tfrac{1}{n \cdot h_n} \IInt{K^2(x) \cdot f(t + h_n x)}{x} \\
      - \tfrac{1}{n} \cdot \left( \IInt{K(x) \cdot f(t + h_n x)}{x} \right)^2
    \end{array}$
  \end{itemize}
\end{lem}

\begin{satz}
  Sei $f$ eine beschränkte W-Dichtefktn, $\fa{x \in \R} f(x) \leq M$, mit Stetigkeitsstellen $C \subseteq \R$.
  Sei $K \in L^2(\R)$. \\
  Angenommen, $n \cdot h_n \xra[n \to \infty]{} \infty$.
  Dann gilt
  \begin{itemize}
    \item $\E_f \hat{f}_n(t) \xra[n \to \infty]{} f(t) \quad \forall \, t \in C$,
    \item $n \cdot h_n \cdot \var_f (\hat{f}_n(t)) \xra[n \to \infty]{} f(t) + \IInt{K^2(x)}{x} \quad \forall \, t \in C$
    \item $\sup_{t \in \R} \left( \hat{f}_n(t) - f(t) \right)^2 \xra[n \to \infty]{} 0$, falls $f$ glm. stetig auf $\R$.
  \end{itemize}
\end{satz}

\fi

% Vorlesung vom 3.12.2015

\begin{bsp}[\emph{Multiple lineare Regression}]
  \[ Y_i = \beta_0 + \beta_1 X_1^{(i)} + \ldots + \beta_m X_m^{(i)} + \epsilon_i \]
  % $\beta = (\beta_1, \ldots, \beta_m)^T$
  % Regressionskoeffizienten $X = ...$
\end{bsp}


\begin{bsp}[\emph{Quasilineare (multiple) Regression}]
  \[ Y_i = \beta_0^{(i)} + \beta_1 f_1(X_1^{(i)}) + \ldots + \beta_m f_m(X_m^{(i)}) + \epsilon_i \]
  mit (nichtlinearen) Funktionen $f_1$, \ldots, $f_m$
\end{bsp}


\begin{defn}
  Eine Matrix $A^{-} \in \R^{n \times m}$ heißt \emph{g-Inverse} (g = generalized) von $A \in \R^{m \times n}$, wenn für jedes $y \in \R^m$, für welches $Ax = y$ lösbar ist, auch $x = A^{-} y$ eine Lösung ist.
\end{defn}

\begin{satz}
  $A^{-}$ ist eine g-Inverse von $A$ $\iff$ $A A^{-} A = A$
\end{satz}

\begin{bem}
  \begin{itemize}
    \item Falls $n = m$ und $A^{-1}$ existiert, so ist $A^{-} = A^{-1}$ eindeutig.
    \item $A^{-}$ ist im Allgemeinen nicht eindeutig.
    Man erhält Eindeutigkeit durch Zusatzforderungen:
  \end{itemize}
\end{bem}

\begin{defn}
  Eine \emph{Moore-Penrose-Inverse} $A^{+}$ ist eine g-Inverse, welche folgende Bedingungen erfüllt:
  \[
    A^{+} A A^{+} = A^{+}, \quad
    (A A^{+})^T = A A^{+}, \quad
    (A^{+} A)^T = A^{+} A.
  \]
\end{defn}

% Allgemeine Lösung von (N)

\begin{satz}
  Die allgemeine Lösung von (N) lautet mit $S \coloneqq X^T X$:
  \[
    \beta = S^{-} X^T y + (S^{-} S - I_p) z, \quad
    \text{wobei } z \in \R^p.
  \]
  % Andererseits kann jede Lösung von (N) in dieser Form angegeben werden.
  Für die spezielle Lsg $z = 0$ gilt
  $\hat{\beta} = S^{-} X^T Y$
  für die MkQ-Schätzung für $\beta$.
  Es gilt
  \[
    \E \hat{\beta} = S^{-} S \beta
    \quad \text{und} \quad
    \cov(\hat{\beta}) = \sigma^2 S^{-} S S^{-}.
  \]
\end{satz}

\begin{bem}
  Bei Nichteindeutigkeit der Lsg von (N) gilt i.\,A. $S^{-} S \neq I_p$. \\
  Falls $\rk X = \rk S = p$, so gilt $\E \hat{\beta} = \beta$ und $\cov \hat{\beta} = \sigma^2 S^{-1}$
\end{bem}

% Der Begriff der schätzbaren Funktion (estimable function)

\begin{defn}
  Eine Linearkombination $l(\beta) = c^T \beta$ mit $c \in \R^p$, $\beta \in \R^p$ heißt bzgl. des linearen Modells $[Y, X \beta, \sigma^2 I_n]$ \emph{schätzbare Funktion}, falls ein $a \in \R^n$ mit $c = X^T a$ existiert.
\end{defn}

% Die Bedeutung schätzbarer Funktionen ergibt sich aus

\begin{satz}
  Es sind äquivalent:
  \begin{itemize}
    \item $l(\beta) = c^T \beta$ ist eine schätzbare Funktion
    \item $\hat{l} \coloneqq l(\hat{\beta}) \coloneqq c^T \hat{\beta}$ (wobei $\hat{\beta}$ MkQ-Schätzung) ist eine lin. Funktion von~$Y$ und eine erwartungstreue Schätzung für $l(\beta)$ %(\dh{} $\E l(\hat{\beta}) = l(\beta)$)
    \item $c \in \im(X^T) = \im(X^T X)$
    \item $l(\hat{\beta}) = c^T \hat{\beta}$ ist konstant für alle $\hat{\beta}$, die Lösung von (N) sind, \dh{} $X^T X \hat{\beta} = X^T y$.
    \item Es existiert ein $a \in \R^n$ mit $\E (a^T Y) = c^T \beta$.
  \end{itemize}
\end{satz}

% Vorlesung vom 7.12.2015

\begin{satz}[\emph{Gauß-Markov}]
  In einem lin. Modell $[Y, X \beta, \sigma^2 I_n]$ ex. für jede schätzbare (lin.) Funktion $l(\beta) = c^T \beta$ eine eindeutig bestimmte, in $Y$ lin. erwartungstreue Schätzung $\hat{l} = a_*^T Y$ (für genau ein $a_* \in \im(X) \subseteq \R^n$) und diese hat die Gestalt $\hat{l} = l(\hat{\beta}) = c^T \hat{\beta}$, wobei $\hat{\beta}$ eine MkQ-Schätzung ist.
  Außerdem besitzt $\hat{l}$ minimale Varianz in der Klasse aller linearen erwartungstreuen Schätzungen $\hat{l} = a^T Y$.
\end{satz}

\begin{konstr}
  $a_* = X (X^T X)^{-} c$
\end{konstr}

\begin{defn}
  Der Schätzer heißt Best Linear Unbiased Estimation (\emph{BLUE}).
\end{defn}

% §4.3 Schätzung der Modellstreuung $\sigma^2$

\begin{bem}
  Es gilt
  %Darstellung von $S(Y, \hat{\beta})$:
  \begin{align*}
    S(Y, \hat{\beta}) & = \min_{\beta \in \R^p} \norm{Y - X \beta}^2 = \norm{Y - X \hat{\beta}}^2 = (Y - X \hat{\beta})^T (Y - X \hat{\beta}) = \\
    & = Y^T Y - \underbrace{Y^T X \hat{\beta}}_{\mathclap{= (X \hat{\beta})^T X \hat{\beta}}} - (X \hat{\beta})^T Y + \underbrace{(X \hat{\beta})^T X \hat{\beta}}_{\mathclap{= \hat{\beta}^T X^T X \hat{\beta} = \beta^T X^T Y}} = \norm{Y}^2 - \norm{X \hat{\beta}}^2.
  \end{align*}
\end{bem}

\begin{defn}
  $(Y - X \hat{\beta})$ \enspace
  heißt \emph{Restvektor} oder \emph{Residuum}.
\end{defn}

\begin{lem}
  Falls $\hat{\beta}$ MkQ-Schätzung, so gilt
  \begin{itemize}
    \item $\E (Y - X \hat\beta) = 0$
    \item $c^T \beta$ ist eine schätzbare Funktion und $\E (c^T \hat{\beta} (Y - X \hat\beta)) = 0$
    \item $\cov(Y - X \hat\beta) = \cov(Y) - \cov(X \hat\beta)$.
  \end{itemize}
\end{lem}

Orthogonale Transformation von $[Y, X \beta, \sigma^2 I_n]$

$X = (\tilde{x}_1 \cdots \tilde{x}_p)$ mit $\tilde{x}_i \in \R^p$, $\rk X = r \leq p$.

Es existiert eine orthonormale Basis $o_1, \ldots, o_r$ von $\im(X)$ und $\sigma_{r+1}, \ldots, \sigma_n \in \im(X)^\perp$.

$Y = \sum_{i=1}^n o_i Z_i$, $Z = (Z_1 \cdots Z_n)^T$, \dh{} $Y = O Z$, wobei $O (o_1 \cdots o_n)$, $O_1 = (a_1 \cdots a_r)^T$, $O_2 = (a_{r+1} \cdots a_n)^T$

$Z = O^{-1} Z = O^T Y$

$\E Z = O^T \E Y = O^T X \beta = \begin{psmallmatrix}
  O_1^T \\ O_2^T
\end{psmallmatrix} X \beta$

$\E Z_i = o_i^T X \beta = \begin{cases}
  o_i^T X \beta & \text{für $i = 1, \ldots, r$,} \\
  0 & \text{für $i = r+1, \ldots, n$.}
\end{cases}$

$\E Z = \begin{psmallmatrix}
  O_1^T X \beta \\ 0
\end{psmallmatrix}$, $\cov(Z) = \cov(O^T Y) = O^T \cov(Y) O = \sigma^2 I$

Transformation: $[Y, X \beta, \sigma^2 I_n] \rightsquigarrow [Z, O^T X \beta, \sigma^2 I_n]$ mit $Z = O^T Y$

\begin{satz}
  In einem linearen Modell $[Y, X \beta, \sigma^2 I_n]$ mit $\rk X = r \leq p$ und einer MkQ-Schätzung $\hat{\beta}$ ist
  \[ \sigma^2 = \tfrac{1}{n-r} S(Y, \hat{\beta}) = \tfrac{1}{n-r} \norm{Y - X \hat\beta}^2 = \tfrac{1}{n-r} \sum_{i=1}^n (Y_i - \sum_{j=1}^p x_{ij} \hat\beta_r)^2 \]
  eine erwartungstreue Schätzung für $\sigma^2$.
\end{satz}

% Vorlesung vom 10.12.2015

% §4.4 Das normalverteilte lineare Modell $[Y, \Normal_n(X \beta, \sigma^2 I_n)]$

\begin{satz}
  Für ein normalverteiltes lineares Modell $[Y, \Normal_n(X \beta, \sigma^2 I_n)]$ mit $\rk X = r \leq p$ gilt:
  \begin{itemize}
    \item Die ML-Schätzung für $\beta \in \R^p$ stimmt mit der MkQ-Schätzung $\hat\beta$ überein und es gilt $\hat\beta \sim \Normal_p(\E \hat\beta, \cov(\hat{\beta}))$.
    \item Die ML-Schätzung für $\sigma^2$ lautet $\hat\sigma_n^2 = \tfrac{S(Y, \hat{\beta})}{n} = \tfrac{n-r}{n} \hat\sigma^2$ mit $\E \hat\sigma_n^2 = \tfrac{n-r}{n} \sigma^2 \xra[n \to \infty]{} \sigma^2$ (asympt. erw.-treu) und $\tfrac{S(Y, \hat\beta)}{\sigma^2} \sim \chi_{n-r}^2$
    \item Für einen Vektor $l^T(\beta) = (l_1(\beta), \ldots, l_q(\beta))$ von $q \leq r$ linear unabhängigen schätzbaren Funktionen $l_i(\beta) = c_i^T \beta$, $c_i \in \R^p$
    gilt $\hat{l} \coloneqq l(\hat{\beta}) \sim \Normal_q(l(\beta), \sigma^2 A_* A_*^T)$ mit $\rk A_* = q$, wobei
    \begin{align*}
      A_* = (a_{*,1}, \ldots, a_{*,q})^T \quad
      \text{mit } a_{*,i} \in L(X) \enspace & \text{optimal gemäß} \\
      & \text{Gauß-Markov-Theorem}
      % $c_i^T \hat{\beta} = a_{*,i}^T Y$ für $i = 1, \ldots, q$
    \end{align*}
    \item Die Schätzungen $\hat{l} = l(\hat\beta)$ und $\hat\sigma^2$ (bzw. $\hat\sigma_n^2$) sind unabhängig.
  \end{itemize}
\end{satz}

\begin{kor}
  Für $\rk X = p$ gilt $\hat{\beta} \sim \Normal_p(\beta, \sigma^2 (X^T X)^{-1})$ und $\hat\beta$ und $\hat\sigma^2$ sind unabhängig.
  (Grund: $\beta_i = e_i^T \beta$ sind schätzbare Funktionen)
\end{kor}

\begin{test}[$\sigma^2$-Streuungstest im Modell $(Y, \Normal_n(X \beta, \sigma^2 I_n))$]
  % TODO: Ersetze runde Klammern durch eckige (Problem: LaTeX-Kompilierfehler)
  $H_0 : \sigma^2 = \sigma_0^2$, $\alpha > 0$ vorgegeben, $\rk X = r \leq p$
  $T \coloneqq \tfrac{\norm{Y - X \hat{\beta}}^2}{\sigma_0^2} \sim \chi_{n-r}^2$ (unter $H_0$)
  Kritischer Berech für $T$: $K^* = \cinterval{0, \chi_{n-r, \nicefrac{\alpha}{2}^2}} \cup \cointerval{\chi_{n-r, 1 - \nicefrac{\alpha}{2}}}{\infty}$
\end{test}

Konfidenzschätzung für den Vektor der schätzbaren Funktionen
\[
  l(\beta) = (l_1(\beta), \ldots, l_q(\beta))^T, \enspace
  1 \leq q \leq r \leq p < n, \enspace
  l_1(\beta), \ldots, l_q(\beta) \text{ lin. unabh.}
\]

\[
  \P((l(\hat\beta) - l(\beta))^T (A_* A_*^T)^{-1} (l(\hat\beta) - l(\beta)) \leq \tfrac{q}{n-r} \norm{Y - X \hat\beta}^2 \cdot F_{q, n - r, 1 - \alpha}) = 1 - \alpha.
\]
% wobei $F_{n_1, n_2}$ die F-Verteilung mit $(n_1, n_2)$ Freiheitsgraden ist.

% Vorlesung vom 14.12.2015

% §4.5 Anwendung auf das Modell I der Varianzanalyse (ANOVA)

% 4.5.1 Einfache Klassifikation, Einwegklassifikation

Vergleich von Erwartungswerten von $p$ Stufen (Populationen), $\Normal(\mu, \sigma^2)$-verteilter unabhängiger Beobachtungen ($i = 1, \ldots, p$)

Versuchsplan:

\begin{center}
  \renewcommand{\arraystretch}{1.5}
  \begin{tabular}{c | c c c c}
    & $1$ & $2$ & $\cdots$ & $n_i$ \\ \hline
    $1$ & $y_{11}$ & $y_{12}$ & $\cdots$ & $y_{1,n_1}$ \\
    $2$ & $y_{21}$ & $y_{22}$ & $\cdots$ & $y_{2,n_2}$ \\
    \vdots & \vdots & \vdots & & \vdots \\
    $p$ & $y_{p1}$ & $y_{p2}$ & $\cdots$ & $y_{p,n_p}$ \\
  \end{tabular}
\end{center}

Mathematisches Modell: $Y_{ik} = \mu_i + \epsilon_{ik}$, $n = 1, \ldots, n_i$, $i = 1, \ldots, p$, $\beta = (\mu_1, \ldots, \mu_p)^T$, $\epsilon_{ik} \sim \Normal(0, \sigma^2)$ \iid{}

% ausgelassen: Matrixgleichung

Wichtig: Prüfen, ob tatsächlich die Varianz von $\epsilon_{ik}$ gleich sind (Bartlett-Test).

Normalengleichung: $X^T X \beta = X^T Y$

$X^T X = \begin{pmatrix}
  n_1 & 0 & & 0 \\
  0 & n_2 & 0 & 0 \\
  & 0 & \ddots & 0 & 0 \\
  0 & & 0 & n_p
\end{pmatrix}$

$X^T Y = \begin{pmatrix}
  Y_{1 \bullet} = \sum_{k=1}^{n_1} Y_{1k} \\
  \vdots \\
  Y_{p \bullet} = \sum_{k=1}^{n_p} Y_{pk}
\end{pmatrix}$

$\hat{\mu}_i = \overline{Y}_{i \bullet} = \tfrac{1}{n_i} \sum_{k=1}^{n_i} Y_{ik}$ für $i = 1, \ldots, p$

Schätzung der Modellstreuung:

%$\hat{\beta} =$

$\hat{\sigma}^2 = \tfrac{1}{n-p} \norm{Y - X \hat{\beta}}^2 = \tfrac{1}{n-p} \sum_{i=1}^p \sum_{k=1}^{n_i} (Y_{ik} - \overline{Y}_{i \bullet})^2$

Dann ist $(n-p) \frac{\hat{\sigma}^2}{\sigma^2} \sim \chi_{n-p}^2$

Test auf Gleichheit der Erwartungswerte:

$H_0 : \mu_1 = \mu_2 = \ldots = \mu_p$

Testgröße: $T = \tfrac{n-p}{p-1} \tfrac{S_1^2 - S_0^2}{S_0^2} \sim F_{p-1, n-p}$

Ablehnung von $H_0$, falls $T \geq T_{p-1,n-p,1-\alpha}$ % wobei $\alpha > 0$ die Irrtumswahrscheinlichkeit

$C = \begin{pmatrix}
  1 & -1 & 0 \\
  0 & 1 & -1 & 0 \\
  && \ddots & \ddots \\
  && 0 & 1 & -1
\end{pmatrix}$

$S_0^2 \coloneqq \norm{Y - X \hat{\beta}}^2 = (n-p) \hat{\sigma}^2 = \sum_{i=1}^p \sum_{k=1}^{n_i} (Y_{ik} - \overline{Y}_{i \bullet})^2$

$S_1^2 = \min_{\beta : C \beta = 0} \norm{Y - X \beta}^2$

Berechnung von $S_1^2$ mittels Lagrange-Multiplikatoren ergibt:
$S_1^2 = \sum_{i=1}^p \sum_{k=1}^{n_i} (Y_{ik} - \overline{Y}_{i \bullet})^2$
% $F(\beta, \lambda) = (Y - X \beta)^T (Y - X \beta) + (C \beta)^T \lambda$ mit $\beta^T = (\mu_1, \ldots, \mu_p)$, $\lambda^T = (\lambda_1, \ldots, \lambda_{p-1})$

$S_1^2 - S_0^2 = \ldots = \sum_{i=1}^p n_i (\overline{Y}_{i \bullet} - \overline{Y}_{\bullet \bullet})^2$

\begin{sprechweise}
  Übliche Bezeichnungen:
  \begin{align*}
    S_1^2 & = \text{SQG} = \text{Summe der Quadrate der Abweichungen in der Gesamtheit} \\
    S_1^2 - S_0^2 = \text{SQA} = \text{Summe der Quadrate der Abweichungen zwischen den Stufen des Faktors $A$} \\
    S_0^2 = \text{SQR} = \text{Summe der Quadrate der Abweichungen innerhalb der Stufen des Faktors $A$ (Restquadratesumme)}
  \end{align*}
\end{sprechweise}

Bartlett-Test: Prüfen der Hypothese, dass $p$ unabhänige ZGn $X_i \sim \Normal(\mu_i, \sigma^2)$, $i = 1, \ldots, p$ dieselbe Streuung $\sigma^2$ besitzen. $H_0 : \sigma_1^2 = \ldots = \sigma_p^2 = \sigma^2$
Gegeben seien $p$ unabhängige Stichproben $X_{i 1}, \ldots, X_{i n_i}$, $i = 1, \ldots, p$, $n = n_1 + \ldots + n_p$.

Testgröße: $T_{n_1, \ldots, n_p} = \tfrac{1}{D} \left( (n-p) \log S^2 - \sum_{i=1}^p (n_i - 1) \log S_i^2 \right)$ mit $D \coloneqq 1 + \tfrac{1}{3 (- 1)} \left( \sum_{i=1}^p \tfrac{1}{n_i - 1} - \tfrac{1}{n-p} \right)$, $S_i^2 = \tfrac{1}{n_i - 1} \sum_{k=1}^{n_i} (X_{ik} - \overline{X}_{i \bullet})^2$, $S^2 = \tfrac{1}{n-p} \sum_{i=1}^p S_i^2$

Für $\min(n_1, \ldots, n_p) \to \infty$ gilt $T_{n_1, \ldots, n_p} \xra{d} \chi_{p-1}^2$.

Faustregel: $\min(n_1, \ldots, n_p) \geq 5$

Ablehnung von $H_0$, falls $T_{n_1, \ldots, n_p} > \chi_{p-1, 1-\alpha}^2$.

\iffalse
Beispiel: Wirkung vonschmerzstillenden Medikamenten
Verabreichung von 2 Medikamenten und einem Placebo.
Gemessen wird die Zeitdauer nach Einnahme, in der sich der Patient schmerzfrei fühlt

5 Patienten mit Placebo \\ 
4 Patienten mit Droge A \\
6 Patienten mit Droge B

\begin{tabular}
  Placebo & 2,2 & 0,3 & 1,1 & 2,0 & 3,4 && $\sim \Normal(\mu_1, \sigma^2)$ \\
  Droge A & 2,8 & 1,4 & 1,7 & 4,3 &&& $\sim \Normal(\mu_2, \sigma^2)$ \\
  Droge B & 1,1 & 4,2 & 3,8 & 2,6 & 0,5 & 4,3 & $\sim \Normal(\mu_2, \sigma^2)$
\end{tabular}

$H_0 : \mu_1 = \mu_2 = \mu_3$

% Rechnen...

$T = 0,652$, damit Annahme von $H_0$
\fi

% Vorlesung vom 17.12.2015

Zweifache Varianzanalyse (Zweiwegklassifikation, Kreuzwegkassifikation)

Wirkung eines Faktors $A$ in $p$ Stufen und Wirkung eines Faktors $B$ in $q$ Stufen mit $s$ Wiederholungen in jdeder Stufe von Faktor $A$ und $B$.

\begin{bsp}
  Faktor A: Düngemittel
  Faktor B: Bodenart
  $Y_{ijk}$: Ernteertrag in Stufe $i$ von Faktor A, in Stufe $j$ von Faktor B in $k$-ter Wiederholung.
  Es geht um den Vergleich von Mittelwerten bei eventueller Wechselwirkung zwischen den Stufen der Faktoren.
\end{bsp}

\begin{modell}
  $Y_{ijk} = \mu_0 + \alpha_i + \beta_j + \gamma_{ij} + \epsilon_{ijk}$
  $\alpha_i$: mittlerer Effekt in Stufe $i$ von Faktor A
  $\beta_j$: mittlerer Effekt in Stufe $j$ von Faktor B
  $\gamma_{ij}$: mittlerer Effekt aus Wechselwikrung von Stufe $i$ und Stufe $j$
  Voraussetzung: Alle Beobachtungen sind normalverteilt und unabhängig mit $\E \epsilon_{ijk} = 0$, $\E \epsilon_{ijk}^2 = \sigma^2$
  (eventuell Prüfung mit Bartlett-Test)
\end{modell}

% ausgelassen: Versuchsplan

Prüfen der folgenden Hypothese:
$H_A : \alpha_1 = \ldots = \alpha_p = 0$,
$H_B : \beta_1 = \ldots = \beta_q = 0$,
$H_{AB} : \gamma_{11} = \ldots = \gamma_{pq} = 0$

Schreibweise als lineares Modell:

$Y = X \beta$ mit $\dim Y = p \dots q \cdot s$, $X \in \R^{pqs \times (1+p+q+pq)}$.

$(p+1) \cdot (q+1)$ Parameter, $\beta = (\mu_0, \alpha_1, \ldots, \alpha_p, \beta_1, \ldots, \beta_q, \gamma_{11}, \ldots, \gamma_{pq})$

$\rk X = pq - 1 < \min \{ pqs, (p+1) \cdot (q+1) \}$

Reparametrisierung ist notwendig, \dh{} es werden Gleichungen zwischen den Parametern hinzugefügt, die die eindeutige Lösbarkeit von (N) garantieren:

\[
  \alpha_\bullet = 0, \quad
  \beta_\bullet = 0, \quad
  \gamma_{1 \bullet} = \ldots = \gamma_{p \bullet} = 0, \quad
  \gamma_{\bullet 1} = \ldots = \gamma_{\bullet q} = 0.
\]

Wegen $pq - 1 + (2 + p + q) = (p+1) \cdot (q+1)$ kann die Eindeutigkeit der MkQ-Schätzung gesichtert werden.
Diese Bedingungen bedeuten keine Einschränkung der Allgemeinheit der Darstellung $Y_{ijk} = \mu_0 + \alpha_i + \beta_j + \gamma_{ij} + \epsilon_{ijk}$ denn:

% TODO: Summenkonvention erklären

$\mu_0^* = \mu_0 + \overline{\alpha}_\bullet + \overline{\beta}_\bullet + \overline{\gamma}_\bullet$,
$\alpha_i^* = \alpha_i - \overline{\alpha}_\bullet + \overline{\gamma}_{i \bullet} - \overline{\gamma}_{\bullet \bullet}$,
$\beta_j^* = \beta_j - \overline{\beta}_\bullet + \overline{\gamma}_{\bullet j} - \overline{\gamma}_{\bullet \bullet}$ und
$\gamma_{ij}^* = \gamma_{ij} - \overline{\gamma_{i \bullet}} - \overline{\gamma_{\bullet j}} + \overline{\gamma}_{\bullet \bullet}$
für $i = 1, \ldots, p$, $j = 1, \ldots, q$.

Es ergeben sich die Gleichungen

$Y_{ijk} = \mu_0^* + \alpha_i^* + \beta_j^* + \gamma_{ij}^* + \epsilon_{ijk}$ für $i = 1, \ldots, p$, $j = 1, \ldots, q$, $k = 1, \ldots, s$

Bezeichnungen für Mittelwert:

$\hat{\mu}_0 = \overline{Y}_{\bullet \bullet \bullet}$, $\hat{\alpha}_i = \overline{Y}_{i \bullet \bullet} - \overline{Y}_{\bullet \bullet \bullet}$, $\hat{\beta}_j = \overline{Y}_{\bullet j \bullet} - \overline{Y}_{\bullet \bullet \bullet}$

$\hat{\gamma}_{ij} = \overline{Y}_{ij \bullet} - \overline{Y}_{i \bullet \bullet} - \overline{Y}_{\bullet j \bullet} + \overline{Y}_{\bullet \bullet \bullet}$

Zum Prüfen der Hypothese $H_A$, $H_B$, $H_{AB}$ verwendet man folgende Testgrößen:

\begin{align*}
  S_{pqs}^2 & \coloneqq \sum_{i=1}^p \sum_{j=1}^q \sum_{k=1}^s (Y_{ijk} - \overline{Y}_{i j \bullet})^2, \\
  F_A & \coloneqq \frac{pq \cdot (s-1)}{p-1} \frac{qs \sum_{i=1}^p (\overline{Y}_{i \bullet \bullet} - \overline{Y}_{\bullet \bullet \bullet})^2}{S_{pqs}^2}
  \sim F_{p-1,pq(s-1)} \\
  F_B & \coloneqq \frac{pq \cdot (s-1)}{q-1} \frac{ps \sum_{i=1}^q (\overline{Y}_{\bullet j \bullet} - \overline{Y}_{\bullet \bullet \bullet})^2}{S_{pqs}^2}
  \sim F_{q-1,(pq(s-1))} \\
  F_{AB} & \coloneqq \frac{pq (s-1)}{(p-1)(q-1)} \frac{s \sum_{i=1}^p \sum_{j=1}^q (\overline{Y}_{i j \bullet} - \overline{Y}_{i \bullet \bullet} - \overline{Y}_{\bullet j \bullet} + \overline{Y}_{\bullet \bullet \bullet})^2}{S_{pqs}^2} \sim F_{(p+1)(q+1), pq(s-1)}
\end{align*}

Entscheidungsregel: % $\alpha > 0$ sei ein gegebenes Signifikanzniveau
Die Hypothesen $H_A$, $H_B$ bzw. $H_{AB}$ werden abgelehnt, falls
\[
  F_A > F_{p-1,pq(s-1),1-\alpha}, \quad
  F_B > F_{q-1,pq(s-1),1-\alpha}
  \enspace \text{bzw.} \enspace
  F_{AB} > F_{(p-1)(q-1),pq(s-1),1-\alpha}.
\]

\begin{bem}
  Die Anzahl der Wiederholungen kann auch in den einzelnen Stufen variieren.
  % für Details siehe: V. Nollau, Statistische Analysen, Leipzig, 1978, H. Ahrens, J. Läuter: Mehrdimensionale Varianzanalyse, Berlin, 1981
\end{bem}

\end{document}
