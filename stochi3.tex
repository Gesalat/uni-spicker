\documentclass{cheat-sheet}

\pdfinfo{
  /Title (Zusammenfassung Stochastik 3)
  /Author (Tim Baumann)
}

\usepackage{bbm} % Für 1 mit Doppelstrich (Indikatorfunktion)
\usepackage{mathtools} % psmallmatrix environment
\usepackage{nicefrac}

% Kleinere Klammern
\delimiterfactor=701

% TODO: Include-File für Stochastik

\newcommand{\Alg}{\mathfrak{A}} % (Mengen-)Algebra
%\newcommand{\Ring}{\mathfrak{R}} % (Mengen-)Ring
%\newcommand{\LebAlg}{\mathfrak{L}} % Lebesgue-Borel-Mengen
\renewcommand{\P}{\mathbb{P}} % Wahrscheinlichkeitsmaß
\newcommand{\E}{\mathbb{E}} % Erwartungswert
\newcommand{\Bor}{\mathfrak{B}} % Borel
%\newcommand{\Leb}{\mathcal{L}} % Lebesgue
\newcommand{\ind}{\mathbbm{1}} % Indikatorfunktion
\newcommand{\Cont}{\mathcal{C}} % Menge der stetigen/diff'baren Funktionen
\newcommand{\scp}[2]{\langle #1, #2 \rangle} % Skalarprodukt
%\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\eqqd}{\stackrel{d}{=}} % Gleichheit in Verteilung (equality in distribution)
\newcommand{\iid}{i.\,i.\,d.} % identisch unabhängig verteilt
\newcommand{\Uniform}{\mathcal{R}} % Gleichverteilung
\DeclareMathOperator{\Cum}{Cum} % Kumulante
\DeclareMathOperator{\rk}{rk} % Rang einer Matrix
\DeclareMathOperator{\MSE}{MSE} % mean squared error
\DeclareMathOperator{\MASE}{MASE} % mean averaged squared error
\DeclareMathOperator{\MISE}{MISE} % mean integrated squared error
\DeclareMathOperator{\Graph}{Graph} % Funktionsgraph
\DeclareMathOperator{\conv}{conv} % konvexe Hülle

\DeclareMathOperator{\var}{Var} % Varianz
\DeclareMathOperator{\cov}{Cov} % Kovarianz
\DeclareMathOperator{\cor}{Cor} % Korrelation

% Hervorhebung der Nullhypothese und der Gegenhypothese
%\definecolor{TestColor}{rgb}{0.1,0.5,0.4}
\definecolor{TestColor}{rgb}{0.7,0.2,0.0}
\newcommand{\testh}[1]{\textcolor{TestColor}{\textbf{#1}}}

% Verteilungen
\newcommand{\Normal}{\mathcal{N}} % Gaußsche Normalverteilung
\DeclareMathOperator{\Exp}{Exp} % Exponentialverteilung
\newcommand{\MN}{\mathcal{M}} % Multinomialverteilung

\begin{document}

\raggedcolumns % stretche Inhalt nicht über die gesamte Spaltenhöhe

\maketitle{Zusammenfassung Stochastik 3}

% Vorlesung vom 12.10.2015

\section{Hypothesentests mittels Stichprobenfktn}

% §1. Wiederholung

% §1.1 Grundbegriffe der Testtheorie

\begin{modell}
  Gegeben sei ein parametrisches Modell, \dh eine Zufallsgröße $X$, deren Verteilungsfunktion $P_X \in \Set{P_\vartheta}{\vartheta \in \Theta \subset \R^n}$ von einem Parameter $\vartheta$ abhängt.
\end{modell}

\begin{prob}
  Anhand einer \emph{Stichprobe} $x_1, \ldots, x_n \in \R^1$ von $X$ (\dh{} $x_1, \ldots, x_n$ sind Realisierung von iid ZGen $X_1, \ldots, X_n \sim P_X$) ist zu entscheiden, ob die sogenannte \emph{Nullhypothese} $H_0 : \vartheta \in \Theta_0 \subset \Theta$ oder eine \emph{Gegenhypothese} $H_1 : \vartheta \in \Theta_1 = \Theta \setminus \Theta_0$ angenommen oder abgelehnt werden soll.
\end{prob}

\begin{defn}
  Der \emph{Stichprobenraum} ist $(\R^n, \Bor(\R^n), P_\vartheta \times \ldots \times P_\vartheta)$.
\end{defn}

\begin{terminologie}
  Die Hypothese $H_i$ heißt \emph{einfach}, falls $\abs{\Theta_i} = 1$, andernfalls \emph{zusammengesetzt}.
\end{terminologie}

\begin{defn}
  Ein (nichtrandomisierter) \emph{Test} für $H_0$ gegen $H_1$ ist eine Entscheidungsregel über die Annahme von $H_0$ basierend auf einer Stichprobe, die durch eine messbare Abbildung $\varphi : \R^n \to \{ 0, 1 \}$ augedrückt wird und zwar durch
  \[ \varphi(x_1, \ldots, x_n) = \begin{cases}
    0 & \text{bei Annahme von $H_0$,} \\
    1 & \text{bei Ablehnung von $H_0$.}
  \end{cases} \]
\end{defn}

\begin{defn}
  Der \emph{Ablehnungsbereich} oder \emph{kritische Bereich} von $\varphi$ ist
  \[ K_n \coloneqq \Set{(x_1, \ldots, x_n) \in \R^n}{\varphi(x_1, \ldots, x_n) = 1}. \]
\end{defn}

\begin{bem}
  Es gilt $\varphi = \ind_{K_n}$.
\end{bem}

\begin{defn}
  \begin{minipage}[t]{0.88 \linewidth}
    \emph{Fehler 1. Art}: \enspace Ablehnung von $H_0$, obwohl $H_0$ richtig ist \\
    \emph{Fehler 2. Art}: \enspace \makebox[0pt][l]{Annahme}\phantom{Ablehnung} von $H_0$, obwohl $H_0$ falsch ist
  \end{minipage}
\end{defn}

% Ausgelassen: Stichprobenraum ist [R^n, B(R^n), P_\vartheta \times \ldots \times P_\vartheta]

\begin{defn}
  Die \emph{Güte- oder Machtfunktion} des Tests $\varphi$ ist
  \begin{align*}
    m_\varphi : \Theta \to \cinterval{0}{1}, \enspace
    m_\varphi(\vartheta) & \coloneqq
    \E_\vartheta \varphi(X_1, \ldots, X_n) \\
    & = \P_\vartheta ((X_1, \ldots, X_n) \in K_n) \\
    & = (P_\vartheta \times \ldots \times P_\vartheta)(K_n)
  \end{align*}
  Die Gegenwsk. $(1 {-} m_\varphi(\vartheta))$ heißt \emph{Operationscharakteristik} von $\varphi$.
\end{defn}

\begin{bem}
  Es gilt \enspace
  $\begin{array}[t]{r c l l}
    \P_\vartheta(\text{Fehler 1. Art}) &=& m_\varphi(\vartheta) & \enspace\text{für $\vartheta \in \Theta_0$,} \\
    \P_\vartheta(\text{Fehler 2. Art}) &=& 1 - m_\varphi(\vartheta) & \enspace\text{für $\vartheta \in \Theta_1$.}
  \end{array}$
\end{bem}

% Ausgelassen: Graph einer "fast idealen Kurve"

\begin{defn}
  Ein Test $\varphi : \R^n \to \{ 0, 1 \}$ mit
  \[ \sup_{\vartheta \in \Theta_0} m_\varphi(\vartheta) \leq \alpha \]
  heißt \emph{$\alpha$-Test} o. \emph{Signifikanztest} zum \emph{Signifikanzniveau} $\alpha \in \ointerval{0}{1}$. \\[2pt]
  Ein $\alpha$-Test $\varphi$ heißt \emph{unverfälscht} (erwartungstreu, unbiased), falls
  \[ \inf_{\vartheta \in \Theta_1} m_\varphi(\vartheta) \geq \alpha. \]
\end{defn}

% Konstruktion nichtrandomisierter Tests mittels Stichprobenfunktionen (= Statistiken) im Falle einfacher Nullhypothesen

\begin{situation}
  Sei nun eine Stichprobenfunktion oder \emph{Teststatistik} $T : \R^n \to \R^1$ gegeben.
  Wir wollen einen Test der einfachen Nullhypothese $H_0 : \vartheta \in \Theta_0 = \{ \vartheta_0 \}$ entwickeln.
\end{situation}

\begin{defn}
  $K_n^T \subset \R^1$ heißt \emph{kritischer Bereich der Teststatistik}, falls
  \[ K_n = T^{-1}(K_n^T). \]
\end{defn}

\begin{bem}
  Es gilt
  \begin{alignat*}{4}
    m_\varphi(\vartheta_0) &= \P_{\vartheta_0} \left( (X_1, \ldots, X_n) \in K_n \right)
    &&= \\
    &= \P_{\vartheta_0} \left( T(X_1, \ldots, X_n) \in K_n^T \right)
    &&= \Int{K_n^T}{}{f_T(x)}{x},
  \end{alignat*}
  wobei $f_T$ die Dichte von $T(X_1, \ldots, X_n)$ unter $H_0$ ist.
\end{bem}

\begin{test}
  Sei $X \sim \Normal(\mu, \sigma^2)$, $\sigma$ bekannt und $\alpha \in \ointerval{0}{1}$ vorgegeben. \\
  Zum Test von \testh{$H_0 : \mu = \mu_0$ vs. $H_1 : \mu \neq \mu_0$} wählen wir als Statistik
  \[
    T(X_1, \ldots, X_n) \coloneqq \tfrac{\sqrt{n}}{\sigma} \left( \overline{X}_n - \mu_0 \right) \enspace
    \text{mit }
    \overline{X}_n \coloneqq \tfrac{1}{n} \left( X_1 + \ldots + X_n \right).
  \]
  Unter Annahme von $H_0$ gilt $T(X_1, \ldots, X_n) \sim \Normal(0,1)$. \\
  Der Ablehnungsbereich der Statistik ist
  \[
    K_n^T = \Set{t \in \R^1}{\abs{t} > z_{1 - \nicefrac{\alpha}{2}}}
    \quad \text{mit} \quad
    z_{1 - \nicefrac{\alpha}{2}} \coloneqq \Phi^{-1}(1 - \nicefrac{\alpha}{2}).
  \]
  Für $\alpha = 0,05$ gilt beispielsweise $z_{1 - \nicefrac{\alpha}{2}} \approx 1,96$.
\end{test}

% Vorlesung vom 15.10.2015

\begin{bem}
  Es gilt
  \begin{align*}
    t \in (K_n^T)^c
    &\iff \abs{t} \leq z_{1-\nicefrac{\alpha}{2}}
    \iff \abs{\overline{X}_n - \mu_0} \leq \tfrac{\sigma}{\sqrt{n}} z_{1-\nicefrac{\alpha}{2}} \\
    &\iff \mu_0 \in \cinterval{\overline{X}_n - \tfrac{\sigma}{\sqrt{n}} z_{1-\nicefrac{\alpha}{2}}}{\overline{X}_n + \tfrac{\sigma}{\sqrt{n}} z_{1-\nicefrac{\alpha}{2}}}.
  \end{align*}
\end{bem}

\begin{defn}
  Dieses Intervall heißt \emph{Konfidenzintervall} für $\mu_0$ zum Konfidenzniveau $1-\alpha$.
\end{defn}

\begin{test}
  Sei wieder $X \sim \Normal(\mu, \sigma^2)$, $\sigma^2$ aber diesmal unbekannt. \\
  Zum Testen von \testh{$H_0 : \mu = \mu_0$ vs. $H_1 : \mu \neq \mu_0$} verwenden wir
  \[
    \hat{T}(X_1, \ldots, X_n) = \tfrac{\sqrt{n}}{S_n} \left( \overline{X}_n - \mu_0 \right), \quad
    S_n^2 \coloneqq \tfrac{1}{n-1} \sum_{i=1}^n \left( X_i - \overline{X}_n \right)^2.
  \]
  Dabei ist $S_n$ die \emph{(korrigierte) Stichprobenvarianz}.
  Man kann zeigen, dass $\hat{T}(X_1, \ldots, X_n) \sim t_{n-1}$ unter $H_0$.
  Dabei ist $t_m$ die \emph{Student'sche $t$-Verteilung} mit $m$ \textit{Freiheitsgraden} (siehe unten). \\
  Der Ablehnungsbereich ist
  \[ K_n^T = \Set{t \in \R^1}{\abs{t} > t_{n-1,1-\nicefrac{\alpha}{2}}}. \]
\end{test}

\begin{bem}
  $S_n^2$ und $\overline{X}_n$ sind unabhängig für $n \geq 2$. % und umgekehrt?
\end{bem}

\begin{diskussion}
  \begin{itemize}
    \item Je kleiner $\alpha$ ist, desto "`nullhypothesenfreundlicher"' ist der Test.
    Häufig verwendet wird $\alpha \in \{ 10\%, 5\%, 1\%, 0,5\% \}$.
    \item Einseitige Tests: Die Gegenhypothese zu $H_0 \!:\! \mu \!=\! \mu_0$ ist $H_1 \!:\! \mu \!>\! \mu_0$.
    Die Nullhypothese wird nur abgelehnt, falls zu große Stichproben- mittelwerte $\overline{x}_n$ vorliegen. Es ist dann $K_n^T = \ointerval{z_{1-\alpha}}{\infty}$.
  \end{itemize}
\end{diskussion}

% 1.2. Prüfverteilung bei normalverteilten Grundgesamtheit
\subsection{Prüfverteilung bei normalvert. Grundgesamtheit}

\begin{defn}
  Es seien $X_1, \ldots, X_n \sim \Normal(0, 1)$.
  Dann heißt die Summe $X_1^2 + \ldots + X_n^2 \sim \chi_n^2$ \emph{Chi-Quadrat-verteilt} mit $n$ \textit{Freiheitsgraden}.
\end{defn}

\begin{defn}
  Falls $X \sim \Normal(0,1)$ und $Y_n \sim \chi_n^2$ unabhängig sind, so heißt
  \[
    \tfrac{X}{\sqrt{\nicefrac{Y_n}{n}}} \sim t_n
    \qquad \text{\emph{$t$-verteilt} mit $n$-Freiheitsgraden.}
  \]
\end{defn}

\begin{lem}
  $\tfrac{n-1}{\sigma^2} S_n^2 \sim \chi_{n-1}^2$
\end{lem}

\begin{kor}
  $\hat{T}$ aus dem zweiten obigen Bsp ist tatsächlich $t$-verteilt.
\end{kor}

\begin{defn}
  Seien $Y_{n_i} \sim \chi_{n_i}^2$, $i = 1, 2$ zwei unabhängige ZGen.
  Dann heißt %der Quotient
  \[
    \tfrac{Y_{n_1} / n_1}{Y_{n_2} / n_2} \sim F_{n_1, n_2} \quad
    \text{\emph{F-verteilt} (\textit{Fisher}) mit $(n_1, n_2)$ Freiheitsgraden.}
  \]
  % auch: Suedecor-verteilt
\end{defn}

% Vorlesung vom 19.10.2015

\begin{test}
  Sei $X \sim \Normal(\mu, \sigma^2)$ mit $\mu$ unbekannt. \\
  Wir testen \testh{$H_0 : \sigma = \sigma_0$ vs. $H_1 : \sigma \neq \sigma_0$} mit
  $T \coloneqq \nicefrac{(n-1)}{\sigma_0^2} S_n^2$. \\
  Unter Annahme von $H_0$ gilt $T \sim \chi_{n-1}^2$.
  Falls $\mu$ bekannt ist, muss
  \[
    \widetilde{T} \coloneqq \tfrac{n}{\sigma_0^2} \widetilde{S}_n^2, \quad
    \widetilde{S}_n^2 \coloneqq \tfrac{1}{n} \sum_{i=1}^n (X_i - \mu)^2.
  \]
  als Statistik gewählt werden.
  Unter Annahme von $H_0$ ist $\widetilde{T} \sim \chi_n^2$.
\end{test}

\begin{test}
  Seien Stichproben $X_1^{(i)}, \nldots, X_{n_i}^{(i)} \sim \Normal(\mu_i, \sigma_i^2)$, $i = 1, 2$ gegeben.
  Wir testen \testh{$H_0 : \sigma_1 = \sigma_2$ vs. $H_1 : \sigma_1 \neq \sigma_2$}.
  Dazu verwenden wir
  \[
    T = \frac{S_{X^{(1)}}^2}{S_{X^{(2)}}^2}, \quad
    S_{X^{(j)}}^2 \coloneqq \tfrac{1}{n-1} \sum_{i=1}^{n_j} \left( X_i^{(j)} - \overline{X}^{(j)}_n \right)^2.
  \]
  Falls $H_0$ gilt, so ist $T \sim F_{n_1-1,n_2-1}$.
\end{test}

\begin{test}
  Situation wie im letzten Test mit $\sigma_1 = \sigma_2$. \\
  Wir testen \testh{$H_0 : \mu_1 = \mu_2$ vs. $H_1 : \mu_1 \neq \mu_2$} mit
  \[
    T = \sqrt{\tfrac{n_1 \cdot n_2}{n_1 + n_2}} \cdot \frac{\overline{X}_{n_1}^{(1)} - \overline{X}_{n_2}^{(2)}}{S_{n_1,n_2}}, \quad
    S_{n_1,n_2}^2 = \frac{(n_1{-}1) S_{X^{(1)}}^2 + (n_2{-}1) S_{X^{(2)}}^2}{n_1 + n_2 - 2}
  \]
  Unter $H_0$ gilt $T \sim t_{n_1 + n_2 - 2}$.
\end{test}

\begin{test}
  Seien $\begin{psmallmatrix} X_1 \\ Y_1 \end{psmallmatrix}, \ldots, \begin{psmallmatrix} X_n \\ Y_n \end{psmallmatrix} \sim \Normal\left(
    \begin{psmallmatrix}
      \mu_1\vphantom{\sigma_1^2} \\
      \mu_2\vphantom{\sigma_2^2}
    \end{psmallmatrix},
    \begin{psmallmatrix}
      \sigma_1^2 & \sigma_1 \sigma_2 \rho \\
      \sigma_1 \sigma_2 \rho & \sigma_2^2
    \end{psmallmatrix} \right)$. \\[2pt]
  Wir testen \testh{$H_0 : \rho = 0$ vs. $H_1 : \rho \neq 0$} mit
  \[
    T \coloneqq \frac{\sqrt{n-2} \cdot \hat{\rho}_n}{\sqrt{1 - \hat{\rho}_n^2}}, \quad
    \hat{\rho}_n \coloneqq \frac{\tfrac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X}_n) (Y_i - \overline{Y}_n)}{S_{X,n} \cdot S_{Y,n}}.
    % Nenner ausgeschrieben: \sqrt{\tfrac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X}_n)^2 \tfrac{1}{n-1} \sum_{i=1}^n (Y_i - \overline{Y}_n)^2}
  \]
  Falls $H_0$ richtig ist, so gilt $T \sim t_{n-2}$. \\[2pt]
\end{test}

\begin{bem}
  Um \testh{$H_0 : \rho = \rho_0 \in \ointerval{-1}{1}$ vs. $H_1 : \rho \neq \rho_0$} zu testen, kann man
  \[ T = \tfrac{\sqrt{n-3}}{2} \left( \log \tfrac{1 + \hat{\rho}_n}{1 - \hat{\rho}_n} - \log \tfrac{1+\rho_0}{1-\rho_0} \right) \]
  verwenden.
  Für $n$ groß gilt approx. $T \sim \Normal(0, 1)$ unter $H_0$.
\end{bem}

% §1.4. Lemma von Slutzky und varianzstabilisiernde Transformationen
\subsection{Lemma von Slutzky und varianzstab. Trafos}

\begin{lem}[\emph{Slutzky}]
  Seien $(X_n)$, $(Y_n)$ Folgen von ZGn über $(\Omega, \Alg, \P)$ mit $X_n \xra[n \to \infty]{\P} c = \text{const}$ (\dh{} $\fa{\epsilon > 0} \P(\abs{X_n - c} > \epsilon) \to 0$) und $Y_n \xra[n \to \infty]{d} Y$ (\dh{} $\P(Y_n \leq y) \to \P(Y \leq y)$ für alle Stetigkeitspunkte $y$ der VF $y \mapsto \P(Y \leq y)$). Dann gilt:
  \[
    X_n + Y_n \xra{d} c + Y, \quad
    X_n \cdot Y_n \xra{d} c \cdot Y, \quad
    Y_n / X_n \xra{d} Y / c \enspace \text{(falls $c \neq 0$)}
  \]
  und allgemeiner $f(X_n, Y_n) \xra[n \to \infty]{d} f(c, Y)$ für jede Fkt $f \in \Cont(\R^2, \R)$.
\end{lem}

\begin{bem}
  Unabhängigkeit von $(X_n)$ und $(Y_n)$ wird nicht vorausgesetzt!
\end{bem}

% Vorlesung vom 22.10.2015

% Varianzstabilisierende Transformationen

\begin{situation}
  Sei $T_n = T(X_1, \ldots, X_n)$ eine Statistik.
  Falls der ZGWS für $T_n$ die Form
  \[ \sqrt{n} (T_n - \vartheta) \xra[n \to \infty]{d} \Normal(0, g(\vartheta)) \]
  besitzt, so benötigen wir für Hypothesentests eine Möglichkeit, die Abhängigkeit der Varianz vom Parameter $\vartheta$ zu beseitigen.
  Man sagt, man führt eine \emph{varianzstabilisierende Transformation} durch. \\
  Wir suchen dazu eine stetig diff'bare Funktion $f : \Theta \to \R^1$, sodass
  \[ \sqrt{n} (f(T_n) - f(\vartheta)) \xra[n \to \infty]{d} \Normal(0, 1). \]
  Man zeigt mit dem MWS und Slutzky, dass dafür gelten muss:
  \[
    f'(\vartheta) = \tfrac{1}{\sqrt{g(\vartheta)}}, \quad \text{also} \quad
    f(\theta) = \myint{}{} \tfrac{\d \vartheta}{\sqrt{g(\vartheta)}}.
  \]
\end{situation}

\begin{bspe}
  \begin{itemize}
    \item Sei $X \sim \Exp(\mu)$ (also $\E X = \mu^{-1}$).
    Dann gilt
    \begin{align*}
      & \sqrt{n} (\overline{X}_n - \tfrac{1}{\mu}) \xra[n \to \infty]{d} \Normal(0, g(\tfrac{1}{\mu}))
      \quad \text{mit} \quad
      g(\vartheta) \coloneqq \vartheta^2. \\
      \leadsto \enspace & \text{Mit } f(\theta) \coloneqq \myint{}{} \tfrac{\d \vartheta}{\sqrt{g(\vartheta)}} = \myint{}{} \tfrac{\d \vartheta}{\vartheta} = \log \theta \\
      & \text{gilt } \sqrt{n} (\log(\overline{X}_n - \log(\tfrac{1}{\mu}))) \xra[n \to \infty]{d} \Normal(0, 1).
    \end{align*}
    \item Wir wollen eine unbek. Wahrscheinlichkeit~$p$ schätzen, etwa durch Wurf einer Münze.
    Der ZGWS von de-Moirre-Laplace besagt
    \[ \sqrt{n} ( \hat{p}_n - p) \xra[n \to \infty]{d} \Normal(0, p (1-p)), \]
    wobei~$\hat{p}_n$ die relative Häufigkeit ist.
    Zur Stabilisierung der Varianz verwenden wir nun
    \[ f(\theta) \coloneqq \myint{0}{\theta} \tfrac{\d p}{\sqrt{p (1-p)}} = 2 \arcsin(\sqrt{\theta}). \]
  \end{itemize}
\end{bspe}

% Vorlesung vom 26.10.2015

% 2. Anpassungstests und weitere nichtparametrische Tests
\section{Chi-Quadrat-Anpassungstest}

% 2.1. Chi-Quadrat-Anpassungstest

\begin{aufgabe}
  Prüfe, ob eine vorliegende Stichprobe $x_1, \ldots, x_n$ aus einer bestimmten (stetig oder diskret verteilten) Grundgesamtheit gezogen wurde. Wir testen also \testh{$H_0 : F = F_0$ vs. $H_1 : F \neq F_0$}.
\end{aufgabe}

% Ausgelassen: Bemerkung über Einordnung in parametrische Tests via. $\Theta \coloneqq \{ \text{Verteilungsfunktionen auf $\R$} \}}$ und $\Theta_0 \coloneqq \{ F_0 \}$.

\begin{verf}
  Wir teilen zunächst $\R$ in Klassen ein,
  \begin{align*}
    & \R = \bigcup_{i=1}^{s+1} I_j
    \quad \text{mit} \quad
    I_j \coloneqq \ocinterval{y_{j-1}}{y_j},
    \quad
    \text{wobei} \\
    & - \infty = y_0 < y_1 < \ldots < y_s < y_{s+1} = + \infty.
  \end{align*}
  Wir setzen
  \begin{align*}
    & h_{n_j} \coloneqq \abs{\Set{k \in \{ 1, \ldots, n \}}{X_k \in I_j}} \tag{absolute Klassenhäufigkeit} \\
    & p_j^{(0)} \coloneqq \P(X \in I_j) = F_0(y_j) - F_0(y_{j-1}) \tag{Klassenwktn unter $H_0$}
  \end{align*}
  Die Klassenhäufigkeiten sind multinomialverteilt unter $H_0$:
  \[ \P(h_{n_1} \!=\! n_1, \nldots, h_{n_{s+1}} \!=\! n_{s+1}) = \binom{n}{n_1, \nldots, n_{s+1}} (p_1^{(0)})^{n_1} \cdots (p_{s+1}^{(0)})^{n_{s+1}}. \]
  Als (näherungsweises) Maß für die Abweichung einer empirischen Verteilung von $F_0$ bei gegebener Klasseneinteilung dient
  \[ T_{n,s+1} \coloneqq \sum_{j=1}^{s+1} \frac{(h_{n_j} - n p_j^{(0)})^2}{n p_j^{(0)}}. \]
\end{verf}

\begin{satz}
  $T_{n,s+1} \xra[n \to \infty]{d} \chi_s^2$
\end{satz}

\begin{faustregel}
  Für $n p_j^{(0)} \geq 5$, $j = 1, \ldots, s+1$ ist $T_{n,s+1}$ mit guter Näherung $\chi_s^2$-verteilt.
\end{faustregel}

% Vorlesung vom 29.10.2015

\begin{entscheidungsregel}[\emph{$\chi^2$-Anpassungstest}]
  Die Nullhypothese $H_0 : F = F_0$ wird genau dann verworfen, wenn $T_{n,s+1} > \chi^2_{s,1-\alpha}$.
\end{entscheidungsregel}

\begin{bemn}
  \begin{itemize}
    \item $T_{n,s+1}$ misst eigentlich nicht die Abweichung von der VF~$F_0$, sondern von der Multinomialverteilung $\MN(n, p^{(0)})$.
    \item Der $\chi^2$-Anpassungstest gilt als hypothesenfreundlich.
    \item Es ist üblich, zunächst die Parameter $\vartheta = (\vartheta_1, \ldots, \vartheta_r)$ der VF~$F_0$ durch MLE zu schätzen, also durch
    \begin{align*}
      & \hat{\vartheta}_n \coloneqq \argmax L(h_{n_1}, \ldots, h_{n_{s+1}}; \vartheta), \quad \text{wobei} \\
      & L(h_{n_1}, \ldots, h_{n_{s+1}}; \vartheta) \coloneqq \prod_{j=1}^{s+1} \left( p_j^{(0)} \right)^{h_{n_j}}.
    \end{align*}
    % Natürliche Bedingungen: Rao-Cramér-Regularität
    Es kann (unter "`natürlichen"' Bedingungen) gezeigt werden, dass
    \[ T_{n,s+1}(\hat{\vartheta}_n) = \sum_{j=1}^{s+1} \frac{(h_{n_j} - n p_j^{(0)}(\hat{\vartheta}_n))^2}{n p_j^{(0)}(\hat{\vartheta}_n)} \xra[n \to \infty]{d} \chi_{s-r}^2, \]
    wobei $r$ die Anzahl der geschätzten Parameter ist. %$\hat{\vartheta}_n = (\hat{\vartheta}_{n,1}, \ldots, \hat{\vartheta}_{n,r})$.
    \item Manchmal wird die Parameter-Schätzung auch direkt aus der SP $x_1, \ldots, x_n$ ermittelt (\zB{} $\tilde{\mu}_n \coloneqq \tfrac{1}{n} (x_1 + \ldots + x_n)$ für den MW einer Normalverteilung).
    In manchen Fällen kann dann auf die Reduktion der Freiheitsgrade von $s$ auf $s-r$ verzichtet werden.
  \end{itemize}
\end{bemn}

% Klassisches Beispiel: Hufschlagtote in der preußischen Armee

% 2.2. Chi-Quadrat-Unabhängigkeitstest, Kontingenztafeln
\section{Chi-Quadrat-Unabhängigkeitstest}

\begin{ziel}
  Überprüfen, ob die Komponenten $X \in \R^{n_1}$ und $Y \in \R^{n_2}$ eines zweidim. Zufallsvektors $(X, Y)^T$ \testh{unabhängig} sind.
\end{ziel}

\begin{verf}
  Seien $I_1, \ldots, I_k \subset \R^{n_1}$ und $J_1, \ldots, J_l \subset \R^{n_2}$ jeweils Familien paarweise disjunkter Mengen mit $\P(X \in I_1 \cup \ldots \cup I_k) = 1$ bzw. $\P(Y \in J_1 \cup \ldots \cup J_l) = 1$.
  Wir setzen
  \begin{align*}
    & p_{ij} \coloneqq \P((X, Y) \in I_i \times J_j) = \P(\{ X \in I_i \} \cap \{ X_j \in J_j \}), \\
    & p_{i\bullet} \coloneqq \sum_{j=1}^l p_{ij} = \P(X \in I_i), \quad
    p_{\bullet j} \coloneqq \sum_{i=1}^k p_{ij} = \P(Y \in J_j).
  \end{align*}
  Wir wollen nun die Nullhypothese $H_0 : \fa{(i, j)} p_{ij} = p_{i \bullet} \cdot p_{\bullet j}$ gegen $H_1 : \ex{(i,j)} p_{ij} \neq p_{i \bullet} \cdot p_{\bullet j}$ testen.
  Wir zählen dazu die Häufigkeiten einer Stichprobe $(X_1, Y_1), \ldots, (X_n, Y_n)$:
  \begin{align*}
    & h_{ij}^{(n)} \coloneqq \abs{\Set{m \in \{ 1, \ldots, n \}}{(X_m, Y_m) \in I_i \times J_j}}, \\
    & h_{i \bullet} \coloneqq \sum_{j=1}^l h_{ij}, \quad
    h_{\bullet j} \coloneqq \sum_{i=1}^k h_{ij}.
  \end{align*}
  Diese Häufigkeiten werden in einer \emph{Kontingenztafel} dargestellt:
  \begin{center}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{c | c c c c | c}
      & $1$ & $2$ & $\cdots$ & $l$ & \\ \hline
      $1$ & $h_{11}^{(n)}$ & $h_{12}^{(n)}$ & $\cdots$ & $h_{1l}^{(n)}$ & $h_{1 \bullet}^{(n)}$ \\
      $2$ & $h_{21}^{(n)}$ & $h_{22}^{(n)}$ & $\cdots$ & $h_{2l}^{(n)}$ & $h_{2 \bullet}^{(n)}$ \\
      \vdots & \vdots & \vdots & & \vdots & $\vdots$ \\
      $k$ & $h_{k1}^{(n)}$ & $h_{k2}^{(n)}$ & $\cdots$ & $h_{kl}^{(n)}$ & $h_{k \bullet}^{(n)}$ \\ \hline
      & $h_{\bullet 1}^{(n)}$ & $h_{\bullet 2}^{(n)}$ & $\cdots$ & $h_{\bullet l}^{(n)}$ & n
    \end{tabular}
  \end{center}
  Wir können den Test nun wie folgt als Spezialfall des $\chi^2$-Anpas- sungstests verstehen: Die Nullhypothese ist, dass die Verteilung von $(X, Y)$ das Produkt der Verteilungen von $X$ und $Y$ ist.
  Dabei schätzen wir zunächst die Verteilungen von $X$ und $Y$ mit
  \begin{align*}
    & L(h_{1 \bullet}^{(n)}, \ldots, h_{k \bullet}^{(n)}, h_{\bullet 1}^{(n)}, \ldots, h_{\bullet l}^{(n)}; p_{1 \bullet}, \ldots p_{k-1, \bullet}, p_{\bullet 1}, \ldots, p_{\bullet, l-1}) \\
    \coloneqq & \prod_{i=1}^{k} (p_{i \bullet})^{h_{i \bullet}^{(n)}} \cdot \prod_{j=1}^l (p_{\bullet j})^{h_{\bullet j}^{(n)}}.
  \end{align*}
  Diese Funktion wird maximal bei $\hat{p}_{i \bullet} = \nicefrac{h_{i \bullet}^{(n)}}{n}$ und $\hat{p}_{\bullet j}^{(n)} = \nicefrac{h_{\bullet j}^{(n)}}{n}$. \\
  % Das sind insgesamt $k+l-2$ zu schätzende Parameter.
  % Vorlesung vom 2.11.2015
  Als Test-Statistik verwenden wir
  \begin{align*}
    \hat{T}_{k,l}^{(n)} \coloneqq &
    \sum_{i=1}^k \sum_{j=1}^l \frac{(h_{ij}^{(n)} - n \hat{p}_{i \bullet} \hat{p}_{\bullet j})^2}{n \hat{p}_{i \bullet} \hat{p}_{\bullet j}} =
    n \sum_{i=1}^k \sum_{j=1}^l \frac{\left( h_{ij}^{(n)} - \nicefrac{h_{i \bullet}^{(n)} \cdot h_{\bullet j}^{(n)}}{n} \right)^2}{h_{i \bullet}^{(n)} \cdot h_{\bullet j}^{(n)}} \\
    & \xra[n \to \infty]{d} \chi^2_{kl - 1 - (k{-}1) - (l{-}1)} = \chi^2_{(k-1)(l-1)}
  \end{align*}
\end{verf}

\begin{entscheidungsregel}
  $H_0$ wird genau dann abgelehnt, falls
  \[ \hat{T}_{k,l}^{(n)} > \chi^2_{(k-1)(l-1),1-\alpha}. \]
\end{entscheidungsregel}

\begin{bemn}
  \begin{itemize}
    \item Zum Testen eines höherdim. ZV $(X_1, \ldots, X_r)$ auf Unabhängigkeit aller Komponenten untersuchen wir die Ereignisse
    \[
      (X_1, \ldots, X_r) \in I_{i_1}^{(1)} \times \ldots \times I_{i_r}^{(r)} \quad
      \text{für $(i_1, \ldots, i_r) \in \bigtimes_{j=1}^r \{ 1, \ldots, k_j \}$}
    \]
    für eine passende Intervalleinteilung.
    Wir verwenden dann
    \begin{align*}
      \hat{T}_{k_1, \ldots, k_r}^{(n)} \coloneqq
      & n^{r-1} \sum_{i_1=1}^{k_1} \cdots \sum_{i_r=1}^{k_r} \frac{\left(h_{i_1 \cdots i_r}^{(n)} - n^{-r + 1} \prod_{j=1}^r h_{\bullet \cdots i_j \cdots \bullet}^{(n)}\right)^2}{\prod_{j=1}^r h_{\bullet \cdots i_j \cdots \bullet}} \\
      & \xra[n \to \infty]{d} \chi^2_{k_1 \cdots k_s - k_1 - \ldots - k_r + r - 1}
    \end{align*}
    \item Im Spezialfall $k \!=\! l \!=\! 2$ (Vierfeldertafel) hat die Statistik die Form
    \[
      \hat{T}_{2,2}^{(n)} =
      n \cdot \frac{\left(h_{11}^{(n)} \cdot h_{22}^{(n)} - h_{12}^{(n)} \cdot h_{21}^{(n)}\right)^2}{h_{\bullet 1}^{(n)} \cdot h_{\bullet 2}^{(n)} \cdot h_{1 \bullet}^{(n)} \cdot h_{2 \bullet}^{(n)}}
      \xra[n \to \infty]{d} \chi^2_1 = \Normal^2(0, 1)
    \]
    und wir lehnen $H_0$ genau dann ab, wenn $\hat{T}_{2,2}^{(n)} > \chi^2_{1,1-\alpha} = z^2_{1 - \nicefrac{\alpha}{2}}$.
  \end{itemize}
\end{bemn}

% 2.3. Kolmogorow-Smirnow-Test
\section{Kolmogorow-Smirnow-1SP-Test}

\begin{situation}
  Sei $X_1, \ldots, X_n \sim F$ eine math. SP.
  Wir sortieren die dabei gezogenen Werte aufsteigend: $X_{1:n} \leq X_{2:n} \leq \ldots \leq X_{n:n}$.
  Dann heißt $\hat{F}_n(x) \coloneqq \tfrac{1}{n} \sum_{i=1}^n \ind_{\ocinterval{-\infty}{x}}(X_{i:n})$ \emph{empirische VF}.
\end{situation}

\begin{satz}[\emph{Gliwenko-Cantelli}, Hauptsatz der math. Statistik]
  \[ \sup_{x \in \R^1} \abs{\hat{F}_n(x) - F(x)} \xra[n \to \infty]{\text{$\P$-f.\,s.}} 0 \]
\end{satz}

\begin{lem}
  Sei $F$ stetig.
  Dann ist die Verteilung von $\sup_x \abs{\hat{F}_n(x) - F(x)}$ nicht von der Verteilungsfunktion $F$ abhängig.
  Genauer:
  \[ \sup_x \abs{\hat{F}_n(x) - F(x)} \enspace\eqqd\enspace \sup_{0 \leq y \leq 1} \abs{\hat{G}_n(y) - G(y)}, \]
  wobei $G$ die Verteilungsfunktion von $\Uniform \cinterval{0}{1}$ ist (also $G(y) = y$) und $\hat{G}_n(y) \coloneqq \tfrac{1}{n} \sum_{i=1}^n \ind_{\cinterval{0}{y}}(U_i)$ für $U_1, \ldots, U_n \sim \Uniform \cinterval{0}{1}$ \iid{}
\end{lem}

% Vorlesung vom 10.11.2015

\begin{kor}
  Sei $F$ stetig, $n \geq 1$.
  Dann ist die Verteilungsfunktion
  \[ K_n(z) \coloneqq \P(\sqrt{n} \cdot \sup_{x \in \R} \abs{\hat{F}_n(x) - F(x)} \leq z) \]
  unabhängig von $F$.
\end{kor}

\begin{satz}
  Falls $F$ stetig ist, so gilt für alle $z \in \R^1$:
  \[ K_n(z) \xra[n \to \infty]{} K(z) \coloneqq \sum_{k = - \infty}^\infty (-1)^k \exp(-2 k^2 z^2). \]
\end{satz}

\begin{defn}
  Dabei ist $K$ die VF der \emph{Kolmogorow-Verteilung}.
\end{defn}

\begin{bem}
  Man zeigt dazu, dass die Folge $X_n : y \mapsto \sqrt{n} \cdot (\hat{G}_n(x) - x)$ gegen die \emph{Brownsche Brücke} $\dot{B}$ konvergiert.
  Für diese gilt
  \[ \sup_{0 \leq x \leq 1} \abs{\dot{B}(x)} \sim K. \]
\end{bem}

\begin{entscheidungsregel}[\emph{Kolmogorow-(Smirnow-)1SP-Test}] \mbox{}\\
  Wir testen \testh{$H_0 : F = F_0$ gegen $H_1 : F \neq F_0$}.
  Dabei muss $F_0$ eine stetige VF sein.
  Wir verwenden dazu
  \[ T_n \coloneqq \sqrt{n} \cdot \sup_{x \in \R} \abs{\hat{F}_n(x) - F_0(x)}. \]
  Wir lehnen $H_0$ genau dann ab, wenn $T_n > K_{1-\alpha}$.
\end{entscheidungsregel}

\begin{bemn}
  \begin{itemize}
    \item Für kleine $n \in \N$ sollte man $K_{n,1-\alpha}$ verwenden.
    \item Für große $z$ ist $K(z) \approx 1 - 2 \exp(-2 z^2)$, also $K_{1-\alpha} \approx \sqrt{- \nicefrac{1}{2} \cdot \log(\nicefrac{\alpha}{2})}$ für $\alpha$ klein.
    \item Das Supremum in $T_n$ liegt bei einer Sprungstelle von $\hat{F}_n$.
  \end{itemize}
\end{bemn}

\begin{test}[einseitiger Kolmogorow-(Smirnow-)1SP-Test] \mbox{}\\
  Wir testen \testh{$H_0 : F = F_0$ gegen $H_1 : F > F_0$} mit %der Statistik
  \[ T_n^{+} \coloneqq \sqrt{n} \cdot \sup_{x \in \R} (\hat{F}_n(x) - F(x)). \]
  Für alle $z \in \R^1$ gilt
  \[ K_n^{+}(z) \coloneqq \P(T_n^{+} \leq z) \xra[n \to \infty]{} K^{+}(z) \coloneqq 1 - \exp(-2 \max(0, z)^2). \]
\end{test}

\begin{entscheidungsregel}
  Ablehnung von $H_0$ $\iff$ $T_n^{+} > K^+_{1-\alpha}$
\end{entscheidungsregel}

\begin{acht}
  Der Kolmogorow-Test kann nicht verwendet werden, wenn die Parameter von $F_0$ aus der Stichprobe geschätzt werden.
\end{acht}

\begin{bem}
  Es gibt keine Entsprechung für mehrdimensionale ZVen
\end{bem}

% KS-(2-Stichproben)-Test
\section{Kolmogorow-Smirnow-2SP-Test}

\begin{situation}
  Gegeben seien zwei unabhängige SPn $X_1, \ldots, X_n \sim F$ \iid{} und $X_1^*, \ldots, X_m^* \sim F^*$ \iid{}, wobei $F$ und $F^*$ stetig sind. \\
  Wir wollen \testh{$H_0 : F = F^*$ vs. $H_1 : F \neq F^*$} testen, indem wir die empirischen VFen $\hat{F}_n$ und $\hat{F}_m^*$ vergleichen.
  Dazu verwenden wir
  \[ T_{m,n} \coloneqq \sqrt{\tfrac{m \cdot n}{m + n}} \sup_{x \in \R^1} \abs{\hat{F}_n(x) - \hat{F}_n^*(x)} \]
\end{situation}

\begin{satz}
  Falls $F = F^*$ stetig ist, so gilt
  \[ T_{m,n} \enspace\eqqd\enspace \sqrt{\tfrac{m \cdot n}{m + n}} \sup_{0 \leq u \leq 1} \abs{\tfrac{1}{n} \sum_{i=1}^n \ind_{\cinterval{0}{u}} (U_i) - \tfrac{1}{m} \sum_{j=1}^m \ind_{\cinterval{0}{u}} (U_j^*)}, \]
  wobei $U_k \coloneqq F(X_k)$, $k = 1, \ldots, n$ und $U^*_l \coloneqq F(X^*_l)$, $l = 1, \ldots, m$ jeweils $\Uniform \cinterval{0}{1}$-verteilt sind.
  \iffalse
  wobei $X_i \eqqd F^{-}(U_i)$, $i = 1, \ldots, n$, \enspace
  $X_j^* \eqqd F^{*,-}(U_j^*)$, $j = 1, \ldots, m$ und
  %$F^{-}$ die Quantilfunktion ist:
  \[
    F^{-}(t) \coloneqq \begin{cases}
      \min \Set{x \in \R^1}{F(x) \geq t} & 0 < t \leq 1, \\
      \lim_{t \downarrow 0} F^{-}(t) & t = 0.
    \end{cases}
    \tag{Quantilfunktion}
  \]
  % (nichtfallend, linksstetig)
  \fi
\end{satz}

\iffalse
\begin{bem}
  Asymptotik von $T_{n,m}$ für $m, n \to \infty$
  \[
    X_{m,n}(u) \coloneqq \sqrt{\tfrac{m \cdot n}{m + n}} \left( \hat{G}_n(u) - \hat{G}^*_m(u) \right), \quad
    0 \leq u \leq 1.
  \]
  \[
    \E X_{m,n} = \sqrt{\tfrac{m \cdot n}{m + n}} \left( \E \ind_{\cinterval{0}{u}}(U_1) - \E \ind_{\cinterval{0}{u}}(U^*_1) \right) = 0
  \]
  \[
    \var(X_{m,n}) = \tfrac{m \cdot n}{m + n} \left( \E (\hat{G}_n(u))^2 + \E (\hat{G}_m(u))^2 - 2 \E \hat{G}^*_n(u) \cdot \E \hat{G}^*_m(u) \right)
  \]
  % (...)

  Genauso wie oben ergibt sich
  \[
    (X_{m,n}(u_1), \ldots, X_{m,n}(u_k)) \xra[n \to \infty]{d} \Normal_k(0, \Sigma)
    \quad \text{mit} \quad
    \Sigma_{ij} = u_i \wedge u_j - u_i \cdot u_j.
  \]

  Daraus folgt die schwache Konvergenz
  \[ X_{m,n}(\blank) \xra[n \to \infty]{d} \dot{B}(\blank) \]
  im Skorodoch-Raum $\mathcal{D} \cinterval{0}{1}$.
\end{bem}
\fi

\begin{lem}
  $T_{m,n} \xra[n \to \infty]{d} \sup_{0 \leq u \leq 1} \abs{\dot{B}(u)} \sim K$
\end{lem}

\begin{entscheidungsregel}[\emph{Kolmogorow-(Smirnow-)2SP-Test}] \mbox{}\\
  $H_0 : F = F^*$ wird genau dann abgelehnt, falls $T_{m,n} > K_{1-\alpha}$.
\end{entscheidungsregel}

\section{Cramér-von-Mises-Test}

% Vorlesung vom 12.11.2015

\begin{defn}
  $\omega_n^2(g) = n \Int{\R^1}{}{g(F(x)) \left( \hat{F}_n(x) - F(x) \right)^2}{F(x)}$ \\
  heißt gewichtete \emph{Cramér-von-Mises-Statistik} oder $\omega^2$-Statistik.
  Dabei ist $g : \cinterval{0}{1} \to \cinterval{0}{\infty}$ eine \textit{Gewichtsfktn}.
  Häufig verwendet wird
  $g(x) \coloneqq 1$
  und die \emph{Anderson-Darling-Statistik} $g(x) \coloneqq \tfrac{1}{x (1-x)}$.
\end{defn}

\begin{satz}
  Sei $F$ stetig.
  Dann ist
  \[
    \omega_n^2(g) \eqqd n \Int{0}{1}{g(u) \left( \hat{G}_n(u) - u \right)^2}{u}
    \xra[n \to \infty]{d} \Int{0}{1}{g(u) (\dot{B}(u))^2}{u} =: \omega^2(g).
  \]
\end{satz}

\begin{entscheidungsregel}[\emph{CvM-Test}]
  Wir testen \testh{$H_0 : F = F_0$ vs. $H_1 : F \neq F_0$} anhand der CvM-Statistik.
  Wir lehnen $H_0$ genau dann ab, wenn $\omega_n^2(g) > \omega_{1-\alpha}^2(g)$.
\end{entscheidungsregel}

\begin{bem}
  Der rechte Wert ist tabelliert für wichtige Funktionen $g$.
\end{bem}

\section{2SP-Test von Wilcoxon-Mann-Whitney} % (U-Test)

% Vorlesung vom 16.11.2015

% §2.4. 2-Stichprobentest von Wilcoxon-Mann-Whitney (U-Test)

\begin{situation}[\emph{2-SP-Test von Wilcoxon-Mann-Whitney}, U-Test]
  Geg. seien zwei unabh. SPn $X_1, \ldots, X_n \sim F$ und $X_1^{*}, \ldots, X_m^{*} \sim F^{*}$, wobei $F$ und $F^*$ stetig sind.
  Ziel: Prüfen von \testh{$H_0 : F = F^{*}$ vs. $H_1 : F \neq F^{*}$}.
  Dazu konstruieren wir eine Rangstatistik für konkrete Stichproben $x_1, \ldots, x_n$ und $x_1^{*}, \ldots, x_m^{*}$:
  \begin{enumerate}
    \item Ordnen: $x_{1:n} < \ldots < x_{n:n}$ und $x_{1:m}^{*} < \ldots < x_{m:m}^{*}$
    \item $\nu_1, \ldots, \nu_m \in \{ 1, \ldots, m+n \}$ seien die Ränge der Werte $x_{i:m}^{*}$ innerhalb der Gesamtstichprobe, \dh{}
    \begin{align*}
      x_{1:n} & \!<\! \ldots \!<\! x_{\nu_1 - 1:n} \!<\! x_1^{*} \!<\! x_{\nu_1:n} \!<\! \ldots \!<\! x_{\nu_2 - 2:n} \!<\! x_{2:m}^{*} < x_{\nu_2 - 1:n} \\
      & < \ldots < x_{\nu_m-m:n} < x_{m:m}^{*} < x_{\nu_m - m + 1 : n} < \ldots < x_{n:n}.
    \end{align*}
  \end{enumerate}
  Heuristik: $H_0$ wird angenommen, falls sich die $x$- und $x^{*}$-Werte "`gut durchmischen"', \dh{} die Anzahl der $x$-Werte, die vor bzw. nach den $x^{*}$-Werten liegen, darf nicht zu groß werden.
  Die Testgröße dafür ist
  \begin{align*}
    W_{m,n} & \coloneqq \! \sum_{i=1}^n \sum_{j=1}^m \ind_{\{ X_i < X_j^{*} \}} \!=\! \abs{\Set{(i, j)}{ X_i \!<\! X_j^{*}}} \!=\! \sum_{j=1}^m \abs{\Set{i}{X_i \!<\! X_{j:m}^{*}}} \\
    & = \sum_{j=1}^m (\nu_j - j) = \nu_1 + \ldots + \nu_m - \tfrac{m (m+1)}{2}
  \end{align*}
\end{situation}

\begin{lem}
  Unter $H_0 : F = F^*$ stetig gilt:
  \begin{align*}
    \text{a)} \enspace
    & \E W_{m,n} = \tfrac{m \cdot n}{2} \qquad
    \text{b)} \enspace
    \var W_{m,n} = \tfrac{m \cdot n}{12} (m + n + 1) \\
    \text{c)} \enspace
    & g_{m,n}(z) \coloneqq \sum_{k=0}^{n \cdot m} \P(W_{m,n} = k) \cdot z^k = \\
    & = \frac{z^{-m (m+1) / 2}}{\binom{m + n}{m}} \enspace\qquad \sum_{\mathclap{1 \leq \nu_1 < \ldots < \nu_m \leq m+n}} \enspace z^{\nu_1 + \ldots + \nu_m}
    = \tfrac{1}{\binom{m + n}{m}} \prod_{k=1}^m \frac{1 - z^{n+k}}{1 - z^k}
  \end{align*}
\end{lem}

\begin{entscheidungsregel}
  Ablehnung von $H_0$, falls $w_{m,n} \leq c_{\nicefrac{\alpha}{2}}$ oder $w_{m,n} \geq m \cdot n - c_{\nicefrac{\alpha}{2}}$, wobei
  \[
    c_{\nicefrac{\alpha}{2}} = \min \Set{k \geq 0}{\P(W_{m,n} \leq k) = \P(W_{m,n} \geq m \cdot n - k) \geq \nicefrac{\alpha}{2}}.
  \]
  % ($c_{\nicefrac{\alpha}{2}} - \tfrac{m \cdot n}{2}$ ist Quantil der Ordnung $\tfrac{\alpha}{2}$ der ZG $W_{m,n} - \tfrac{m \cdot n}{2}$).
  Annahme von $H_0$ genau dann, wenn $\abs{w_{m,n} - \tfrac{m \cdot n}{2}} < \tfrac{m \cdot n}{2} - c_{\nicefrac{\alpha}{2}}$.
\end{entscheidungsregel}

\begin{satz}
  Unter $H_0 : F = F^{*}$ stetig gilt
  \[ T_{m,n} \coloneqq \frac{W_{m,n} - \tfrac{m \cdot n}{2}}{\sqrt{\tfrac{m \cdot n}{2} (m + n + 1)}} \xra[m, n \to \infty]{d} \Normal(0, 1). \]
\end{satz}

% XXX: Hat dieser Test auch einen Namen?
\begin{entscheidungsregel}
  Man erhält aus dem letzten Satz einen asymptotischen Test, den man für große $m$, $n$ verwenden kann: \\
  Wir lehnen genau dann $H_0 : F = F^*$ ab, falls $\abs{T_{m,n}} \geq z_{1 - \nicefrac{\alpha}{2}}$.
\end{entscheidungsregel}

% §2.5 Kruskal-Wallis-Test
\section{Kruskal-Wallis-Test}

\begin{test}[\emph{Kruskal-Wallis}]
  Gegeben seien $k$ Messreihen $X_{i,1}, \ldots, X_{i,n_i} \sim F_i$, $i = 1, \ldots, k$ unabhängige SPn, $F_i$ stetig. \\
  Ziel: Testen von \testh{$H_0 : F_1 = \ldots = F_k$}.
  Vorgehen:
  \begin{enumerate}
    \item Ordnen der Beobachtungen der Größe nach
    \item $\nu_{i,1} < \ldots < \nu_{i,n_i}$ Platznummern der $n_i$ Beobachtungen der $i$-ten Messreihe in der Gesamt-SP
    \item $\overline{\nu}_i \coloneqq \tfrac{1}{n_i} (\nu_{i,1} + \ldots + \nu_{i,n_i})$, $\overline{\nu} \coloneqq \tfrac{1}{n} \sum_{i=1}^k n_i \overline{\nu}_i$ mit $n \coloneqq n_1 + \ldots + n_k$.
  \end{enumerate}
  Heuristik: $H_0$ ist richtig, falls $\overline{\nu}_i \approx \overline{\nu}$ für alle $i$.
  Testgröße:
  \[
    H \coloneqq \tfrac{12}{n (n+1)} \sum_{i=1}^k n_i (\overline{\nu}_i - \tfrac{n+1}{2})^2
    \xra[n_i \to \infty]{d} \chi^2_{k-1}
  \]
  Wir lehnen $H_0$ genau dann ab, wenn $H > \chi^2_{k-1,1-\alpha}$.
\end{test}

\begin{faustregel}
  Die Approx. ist gut, wenn $\min_{1 \leq i \leq k} n_i \geq 5$ und $k \geq 4$.
\end{faustregel}

% §3. U-Statistiken
\section{Theorie der U-Statistiken}

% §3.1. Hoeffdings Projektionsmethode und ZGWS

\begin{situation}
  Sei $n \geq m$, $X_1, \ldots, X_n \sim F$ \iid{}, $h : \R^m \to \R^1$ Borel-messbar und symmetrisch, \dh{}
  \[
    h(x_1, \ldots, x_m) = h(x_{\sigma(1)}, \ldots, x_{\sigma(m)}) \quad
    \forall \, \sigma \in S_m.
  \]
  Gelte $\E \abs{h(X_1, \ldots, X_m)} < \infty$.
\end{situation}

\begin{defn}
  Die \emph{U-Statistik der Ordnung $m$} mit \textit{Kernfunktion} $h$ ist
  \[ U_n^{(m)} \coloneqq \tfrac{1}{\binom{n}{m}} \sum_{1 \leq i_1 < \ldots < i_m \leq n} h(X_{i_1}, \ldots, X_{i_m}). \]
\end{defn}

\begin{bem}
  Offenbar: $\E U_n^{(m)} = \E h(X_1, \ldots, X_m)$.
\end{bem}

\begin{bsp}
  Für $m=2$ gilt $\sigma^2 = \var(X_1) = \tfrac{1}{2} \E (X_1 - X_2)^2$.
  Davon inspiriert setzen wir $h(x_1, x_2) \coloneqq \tfrac{1}{2}(x_1 - x_2)^2$.
  Damit haben wir
  \[
    U_n^{(2)} = \tfrac{2}{n (n-1)} \sum_{1 \leq i < j \leq n} \tfrac{1}{2} (X_i - X_j)^2
    %= \tfrac{1}{n (n-1)} ( (n-1) \sum_{i=1}^n X_i^2 - (X_1 + \ldots X_n)^2 + \sum_{i=1}^n X_i^2 ) - X_n
    %= \tfrac{1}{n} (X_1 + \ldots + X_n)
    %= \tfrac{1}{n-1} ( \sum_{i=1}^n X_i^2 - n (\overline{X}_n)^2 )
    = \tfrac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X}_n)^2 = S_n^2
  \]
\end{bsp}

\begin{ziel}
  Wir würden gerne den ZGWS auf $U_n^{(m)}$ anwenden.
  Problem dabei: Die Summanden in der Def. von $U_n^{(m)}$ sind nicht unabhängig.
  Wir approximieren deshalb $U_n^{(m)}$ mit einer Summe von \iid{} ZGn.
\end{ziel}

\begin{lem}
  Sei \enspace
  $\tilde{U}_n^{(m)} = \theta + \sum_{i=1}^n (\underbrace{\E(U_n^{(m)} | X_i)}_{\text{\iid{}}} - \theta)$ \enspace
  mit $\theta \coloneqq \E U_n^{(m)}$ und
  \begin{align*}
    g(x) & = \E[h(X_1, \ldots, X_n) | X_1 = x ] = \E h(x, X_2, \ldots, X_m) \\
    & = \Int{}{}{\cdots \Int{}{}{h(x, x_2, \ldots, x_n)}{F(x_2)} \cdots}{F(x_n)}.
  \end{align*}
  Falls $\E h^2(X_1, \ldots, X_m) < \infty$, so gilt
  \begin{enumerate}[label=(\arabic*), itemindent=6pt]
    \item $\var(U_n^{(m)} - \tilde{U}_n^{(m)}) = \var(U_n^{(m)}) - \var(\tilde{U}_n^{(m)})$
    \item $\E (U_n^{(m)} | X_i = x) = \theta + \tfrac{m}{n} (g(x) - \theta)$
  \end{enumerate}
\end{lem}

\begin{lem}
  \begin{enumerate}[label=(\arabic*), itemindent=6pt]
    \setcounter{enumi}{1}
    \item $\var(\tilde{U}^{(m)}_n) = \tfrac{m^2}{n} \cdot \var(g(X_1)) = \tfrac{m^2}{2} ( \E g^2(X_1) - \theta^2 )$
    \item Falls $\E \abs{h(X_1, \ldots, X_m)} < \infty$, so gilt
    \begin{align*}
      \var(U_n^{(m)}) & = \tfrac{1}{\binom{n}{m}} \sum_{k=1}^m \binom{m}{k} \binom{n - m}{m - k} \cdot \zeta_k \quad \text{mit} \\
      h_k(x_1, \ldots, x_k) & \coloneqq \E(h(x_1, \ldots, x_k, X_{k+1}, \ldots, X_m) \\
      \zeta_k & \coloneqq \var(h_k(X_1, \ldots, X_k)) \\
      & =
      \arraycolsep=0.2pt
      \begin{array}[t]{l l}
        \E[ & h(X_1, \nldots, X_k, X_{k+1}, \nldots, X_m) \cdot \\
        & h(X_1, \nldots, X_k, X_{m+1}, \nldots, X_{2m-k})] - \theta^2
      \end{array}
    \end{align*}
  \end{enumerate}
\end{lem}

\begin{kor}
  Aus (1), (3) und (4) folgt für $m=2$:
  \[
    \var(U_n - \tilde{U}_n)
    = \var(U_n) - \var(\tilde{U}_n)
    %& = \tfrac{1}{n (n-1)} \left( \E (\overline{h}(X_1, X_2)^2) - 2 (n-2) \E \overline{h}(X_1, X_2) \overline{h}(X_2, X_3) \right) - \tfrac{4}{n} \var(g(X_1)) \\
    = \ldots
    = - \tfrac{4}{n (n-1)} \var(g(X_1))
  \]
  Für $m \geq 2$ gilt $\var(U_n^{(m)} - \tilde{U}_n^{(m)}) \leq \tfrac{c(m)}{n^2} \var(h(X_1, \ldots, X_m))$.
\end{kor}

% Vorlesung vom 23.11.2015

% §3.2. ZGWS für (nichtentartete) U-Statistiken
\subsection{ZGWS für U-Statistiken}

\begin{satz}[\emph{Hoeffding}]
  Sei $U_n^{(m)}$ eine U-Statistik mit Kern $h : \R^m \to \R$, sodass $\E h^2(X_1, \ldots, X_m) < \infty$ und $\sigma_g^2 \coloneqq \var(g(X_1)) > 0$.
  Dann gilt
  %Für eine U-Statistik $U_n^{(m)}$ mit Kern $h : \R^m \to \R$, sodass $\E h^2(X_1, \ldots, X_m) < \infty$ und $\sigma_g^2 \coloneqq \var(G(X_1)) > 0$, gilt
  \[ \sqrt{n} (U_n^{(m)} - \theta) \xra[n \to \infty]{d} \Normal(0, \sigma_g^2). \]
\end{satz}

\begin{bemn}
  \begin{itemize}
    \item Der Fall $\var(g(X_1)) = 0$ (entarteter Fall) zieht eine kompliziertere Asymptotik nach sich. % (Übungsaufgabe dazu)
    \item $\E g^2(X_1) < \infty$ ist schwächer als $\E h^2(X_1, \ldots, X_m) < \infty$.
    \item Aus $E \abs{h(X_1, \ldots, X_m)}^{1+q} < \infty$ für $0 < q \leq 1$ folgt %die Abschätzung
    \[
      \E \abs{\sqrt{n} (U_n^{(m)} - \tilde{U}_n^{(m)})}^{1+q} \leq \tfrac{c(q, m)}{n^{2q}} \E \abs{h(X_1, \ldots, X_m)}^{1+q}.
    \]
    Mit einer Abschneidetechnik zeigt man, dass $\E g^2(X^1) < \infty$ und $\E \abs{h(X_1, \ldots, X_m)}^{\frac{4}{3}} < \infty$ schon für $\P(\sqrt{n} \abs{U_n^{(m)} - \tilde{U}_n^{(m)}} < \epsilon) \to 0$ für alle $\epsilon > 0$ ausreichen und damit für den Satz von Hoeffding.
    \item U-Statistiken erweisen sich (unter gewissen Bedingungen) als suffiziente Schätzer mit minimaler Varianz.
  \end{itemize}
\end{bemn}

\begin{bsp}
  Wir betrachten die U-Statistik $S_n^2 = \binom{n}{2}^{-1} \sum_{i < j} \tfrac{1}{2} (X_i - X_j)^2$. \\
  Dann ist $g(x) = \tfrac{1}{2} (x - \E X_1)^2 + \tfrac{1}{2} \sigma^2$ mit $\sigma^2 \coloneqq \var(X_1)$.
  Es gilt
  \[ \sqrt{n} (S_n^2 - \sigma^2) \xra[n \to \infty]{d} \Normal(0, 4 \sigma_g^2) \]
  mit $\sigma_g^2 = \E g^2(X_2) - (\E g(X_2))^2 = \tfrac{1}{4} \mu_4 - \tfrac{1}{4} \sigma^4$, $\mu_4 \coloneqq \E (X_1 - \E X_2)^4$.
  
  Spezialfall: Ist $X_i \sim \Normal(\mu, \sigma^2)$, so gilt $\mu_4 = 3 \sigma^4$. \\
  Dann gilt $\sqrt{n} (S_n^2 - \sigma^2) \xra[n \to \infty]{d} \Normal(0, 2 \sigma^4)$.
  Es folgt
  \[ \frac{ \sqrt{n} (S_n^2 - \sigma^2) }{\sqrt{2 (S_n^2)^2}} = \sqrt{\nicefrac{n}{2}} \left( 1 - \frac{\sigma^2}{S_n^2} \right) \xra[n \to \infty]{d} \Normal(0, 1). \]
  Alternativ erhält man durch Anwenden einer varianzstab. Trafo:
  \[
    \sqrt{\nicefrac{n}{2}} (\log S_n^2 - \log \sigma^2) \xra[n \to \infty]{d} \Normal(0, 1).
  \]
\end{bsp}


\begin{defn}
  Die \emph{Kumulante} oder \emph{Semi-Invariante} $m$-ter Ordnung ist
  \[ \Cum_m(X) = \tfrac{1}{m! 2^m} \tfrac{\partial^m}{\partial t^m}|_{t=0} \log \E e^{it X}. \]
\end{defn}

\begin{bem}
  Falls $X_1$, \ldots, $X_n$ unabhängig sind, so gilt
  \[ \Cum_m(X_1 + \ldots + X_n) = \Cum_m(X_1) + \ldots + \Cum_m(X_n). \]
  Für $m=3$ gilt $\Cum_3(X) = \E X^3 - 3 \E X \cdot \E X^2 + 2 (\E X)^3$.
\end{bem}

\begin{bsp}
  Schätzung der Kumulante $m$-ter Ord. mit der SP $X_1, \ldots, X_n$:
  \begin{align*}
    (\widehat{\Cum_3(X)})_n & \coloneqq \tfrac{1}{n (n-1) (n-2)} (n^2 \hat{M}_3^{(n)} - 3 n \hat{M}_1^{(n)} \hat{M}_2^{(n)} - 2 (\hat{M}_1^{(n)})^3) \\
    & = \tfrac{1}{\binom{n}{3}} \sum_{1 \leq i < j < j \leq n} h(X_i, X_j, X_k) \\
    \text{mit} \enspace
    h(x, y, z) & \coloneqq \begin{array}[t]{l}
      - \tfrac{1}{2} (xy^2 + x^2 y + x z^2 + x^2 z + y z^2 + x^2 z + y z^2 + y^2 z) \\
      + \tfrac{1}{3} (x^3 + y^3 + z^3) + 2 xyz
    \end{array} \\
    \enspace \text{wobei} \enspace
    \hat{M}_j^{(n)} & \coloneqq \tfrac{1}{n} \sum_{i=1}^n X_i^j
  \end{align*}
\end{bsp}

\subsection{Test auf Symmetrie der VF}

\begin{defn}
  Eine VF $F$ heißt \emph{symmetrisch} bzgl. $\vartheta_0 \in \R^1$, falls
  \[
    F(\vartheta_0 - x) = 1 - F(\vartheta_0 + x) \quad
    \forall \, x \in \R^1.
  \]
\end{defn}

\begin{bsp}[\emph{Wilcoxon-1-SP-Test} auf Symmetrie]
  Sei $X_1, \ldots, X_n \sim F$ eine mathematische Stichprobe mit stetiger VF~$F$.
  Wir wollen \testh{$H_0 : \text{$F$ ist symmetrisch bzgl. $\vartheta_0$}$} testen.
  Es reicht dazu, die VF der $Z_i = X_i - \vartheta_0$ auf Symmetrie bzgl. $0$ zu prüfen. \\
  Seien $\nu_1^{+}, \ldots, \nu_n^{+}$ die Ränge der ZGn $\abs{Z_1}, \ldots, \abs{Z_n}$.
  Setze
  \[ T_n^{+} = \sum_{i=1}^n \ind_{\{ Z_i > 0 \}} \nu_i^{+}. \]
  Unter $H_0 : \text{$F$ ist symmetrisch bzgl. $\vartheta_0$}$ gilt
  \[
    \E T_n^{+} = \tfrac{1}{2} \sum_{i=1}^n \E \nu_i^{+} = \tfrac{n (n+1)}{4}, \quad
    \var(T_n^{+}) = \tfrac{n}{24} (n + 1) (2n + 1).
  \]
\end{bsp}

\begin{bsp}
  Alternativ können wir zum Test auf Symmetrie die U-Statistik
  \[
    U_n = \tfrac{1}{\binom{n}{2}} \sum_{1 \leq i < j \leq n} \ind_{\{ Z_i + Z_j > 0 \}}.
  \]
  Unter $H_0$ gilt für $h(x_1, x_2) \coloneqq \ind_{\{ x_1 + x_2 > 0 \}}$:
  \[
    \E h(Z_i, Z_j)
    = \P(Z_1 \!>\! - Z_2)
    = \IInt{(1 - F(-z))}{F(z)}
    = \IInt{F(z)}{F(z)}
    = \tfrac{1}{2}.
  \]
  Aus dem ZGWS für U-Statistiken folgt
  \[
    \sqrt{n} (U_n - \tfrac{1}{2}) \xra[n \to \infty]{d} \Normal(0, \tfrac{1}{3}).
  \]
\end{bsp}

\begin{entscheidungsregel}
  %Lehne $H_0$ g.\,d. ab, wenn \enspace
  %$\abs{U_n - \tfrac{1}{2}} > \tfrac{z_{1 - \nicefrac{\alpha}{2}}}{\sqrt{3n}}$.
  Ablehnung von $H_0$ $\iff$
  $\abs{U_n - \tfrac{1}{2}} > \tfrac{z_{1 - \nicefrac{\alpha}{2}}}{\sqrt{3n}}$.
\end{entscheidungsregel}

\subsection{Verallgemeinerte U-Statistiken}

% Vorlesung vom 26.11.2015

% Ausgelassen: Beispiel: U-Statistiken in der stochastischen Geometrie

\begin{defn}
  Sei $h : \R^{m_1} \times \R^{m_2} \to \R^1$ Borel-messbar, symmetrisch in den ersten $m_1$ und den letzten $m_2$ Argumenten.
  Seien $X_1, \ldots, X_{n_1} \sim F$ und $X_1^*, \ldots, X_{n_2}^* \sim F^*$ zwei unabh. math. SPn.
  Dann heißt
  \[
    U_{n_1, n_2}^{(m_1, m_2)} \coloneqq \left( \binom{n_1}{m_1} \binom{n_2}{m_2} \right)^{-1} \qquad
    \sum_{\mathclap{\substack{1 \leq i_1 < \ldots < i_{m_1} \leq n_1 \\ 1 \leq j_1 < \ldots < j_{m_2} \leq n_2}}} \enspace
    h(X_{i_1}, \nldots, X_{i_{m_1}}, X_{j_1}^*, \nldots, X_{j_{m_2}}^*)
  \]
  (verallg.) \emph{U-Statistik} der Ordnung $(m_1, m_2)$ mit Kernfunktion $h$.
\end{defn}

\begin{nota}
  Sei $m_1 = m_2 = 1$.
  Wir setzen
  \begin{align*}
    \theta & \coloneqq \E h(X_1, X_1^*) = \E U_{n_1, n_2}^{(1, 1)} \\
    g_1(x) & \coloneqq \E( h(X_1, X_1^*) \mid X_1 = x), \quad
    \sigma_1^2 \coloneqq \var g(X_1), \\
    g_2(y) & \coloneqq \E( h(X_1, X_1^*) \mid X_1^* = y), \quad
    \sigma_2^2 \coloneqq \var g(X_1^*), \\
    \tilde{U}_{n_1, n_2}^{(1,1)} & \coloneqq \tfrac{1}{n_1} \sum_{i=1}^{n_1} g_1(X_i) + \tfrac{1}{n_2} \sum_{j=1}^{n_2} g_2(X_j^*) - \theta
  \end{align*}
\end{nota}

\begin{lem}
  Es seien $\E h^2(X_1, X_1^*) < \infty$ und $\sigma_1^2, \sigma_2^2 \in \ointerval{0}{\infty}$.
  Dann gilt
  \[
    \sqrt{\frac{n_1 n_2}{n_2 \sigma_1^2 + n_1 \sigma_2^2}} \cdot
    (U_{n_1, n_2} - \theta) \xra[n_1, n_2 \to \infty]{d} \Normal(0, 1).
  \]
\end{lem}

\begin{bsp}
  Die Wilcoxon-2-SP-Statistik ist eine U-Statistik mit
  \[ h(x, y) \coloneqq \abs{\Set{\heartsuit}{x < y}}. \]
\end{bsp}

% §4. Das allgemeine lineare Modell
% (Modell I  der Regressions- und Varianzanalyse)
\section{Das allgemeine lineare Modell}

\begin{modell}[allgemein]
  Für Zufallsgrößen $X$ und $Y$ gilt $Y = g(X) + \epsilon$ mit einer Funktion $g$, wobei $\E \epsilon = 0$ und $\sigma^2 \coloneqq \var (Y - g(X)) = \E \epsilon^2$.
  %Bei einfacher linearer Regression nimmt man an, dass $g(x) = a + bx$.
\end{modell}

\begin{modell}[\emph{Lineare Regression}]
  $Y = X \beta + \epsilon$, wobei
  \[
    \arraycolsep=1.4pt
    \begin{array}{r c l l}
      Y &=& (Y_1, \ldots, Y_n)^T \quad & \text{\emph{Beobachtungsvektor},} \\
      X &=& (x_{ij}) \in \R^{n \times p} & \text{\emph{Einstellgrößen-}, Versuchsplanmatrix,} \\
      \beta &=& (\beta_1, \ldots, \beta_p)^T & \text{(unbek.) \emph{Parametervektor}, Regressionskoeff.,} \\
      \epsilon &=& (\epsilon_1, \ldots, \epsilon_n)^T & \text{(nicht beobachtbarer) \emph{Fehlervektor} heißt.}
    \end{array}
  \]
\end{modell}

% Bem: Zunächst keine Verteilungsvoraussetzungen

\begin{bem}
  Falls $Y$ eine bek. Kovarianzmatrix $K \in \R^{n \times n}$ hat, so können wir $X^* \coloneqq K^{\nicefrac{-1}{2}} X$, $Y^* \coloneqq K^{\nicefrac{-1}{2}} Y$, $\epsilon^* \coloneqq K^{\nicefrac{-1}{2}} \epsilon$ setzen und erhalten $Y^* = X^* \beta + \epsilon^*$ und $\cov(Y^*) = I_n$.
  Wir dürfen daher annehmen:
\end{bem}

\begin{voraussetzung}
  $\cov(Y_i, Y_j) = \cov(\epsilon_i, \epsilon_j) = \sigma^2 \delta_{ij}$.
  Dabei heißt $\sigma$ \emph{Modellstreuung}.
  Üblicherweise gilt $n > p$.
\end{voraussetzung}

% Aufgaben:
% * Schätzung von $\beta$ mit der Methode der kleinsten Quadrate (Problem dabei: $\rk X < p$)
% * Schätzung der linearen Funktion $c^T \beta = \sum_{j=1}^p c_j \beta_j$
% * Schätzung der Modellstreuung
% * Test- und Konfidenzintervalle

% §4.2 Die MkQ-Schätzung

\begin{problem}
  Gegeben seien $[Y, X \beta, \sigma^2 I_n]$. \\
  Gesucht sind Schätzungen $\hat{\beta}(y) = (\hat{\beta}_1(y), \ldots, \hat{\beta}_p(y))^T$ für $\beta$.
\end{problem}

\begin{defn}
  Eine Schätzfunktion $\hat{\beta}(y)$ heißt \emph{MkQ-Schätzung} (\textit{Methode der kleinsten Quadrate}) für $\beta$, falls
  $S(y, \hat{\beta}) = \min_{\beta \in \R^p} S(y, \beta)$, wobei
  \[
    S(y, \beta) \coloneqq \norm{y - X \beta}^2 = \sum_{i=1}^n (y_i - \sum_{j=1}^n x_{ij} \beta_j)^2.
  \]
\end{defn}

\begin{bem}
  $S(y, \beta)$ besitzt lokale Minima, da
  \[
    \tfrac{\partial}{\partial \beta} S(y, \beta) = - 2 X^T y + 2 X^T X \beta, \quad
    \tfrac{\partial^2}{\partial \beta^2} S(y, \beta) = 2 X^T X.
  \]
  Für die Minima gelten die Normalengleichungen
  \[
    X^T X \beta = X^T Y \iff \sum_{j=1}^p \xi_{ij} \beta_j = \sum_{j=1}^n x_{ji} y_j \enspace
    \text{mit } (\xi_{ij}) = X^T X.
    \tag{N}
  \]
\end{bem}

\begin{satz}
  (N) ist stets lösbar und jede Lsg ist eine MkQ-Schätzung.
  Falls $\rk X = p$, so ist $\hat{\beta}$ eind. bestimmt durch $\hat{\beta} = (X^T X)^{-1} X^T y$.
\end{satz}

\begin{bsp}[\emph{Einfache lineare Regression}] \mbox{} \\
  Annahme: \enspace
  $Y_i = \beta_1 + \beta_2 x_i + \epsilon_i$, \enspace
  $i = 1, \ldots, n$.
  Dann ist
  \[
    X = \begin{pmatrix}
      1 & x_1 \\
      \vdots & \vdots \\
      1 & x_n
    \end{pmatrix} \quad
    \begin{array}{l}
      \hat{X^T X} = n \sum x_i^2 - \left( \sum x_i \right)^2 = n \cdot \sum (x_i - \overline{x_n})^2 > 0 \\[4pt]
      \hat{\beta} = \det(X X^T)^{-1} \begin{pmatrix}
        \sum x_i^2 & - \sum x_i \\
        - \sum x_i & n
      \end{pmatrix} \begin{pmatrix}
        \sum Y_i \\
        \sum x_i Y_i
      \end{pmatrix}
    \end{array}
  \]
\end{bsp}

% §4.1 Definition und Aufgabenstellung

% Vorlesung vom 3.12.2015

\begin{bsp}[\emph{Multiple lineare Regression}]
  \[ Y_i = \beta_0 + \beta_1 X_1^{(i)} + \ldots + \beta_m X_m^{(i)} + \epsilon_i \]
  % $\beta = (\beta_1, \ldots, \beta_m)^T$
  % Regressionskoeffizienten $X = ...$
\end{bsp}


\begin{bsp}[\emph{Quasilineare (multiple) Regression}]
  \[ Y_i = \beta_0^{(i)} + \beta_1 f_1(X_1^{(i)}) + \ldots + \beta_m f_m(X_m^{(i)}) + \epsilon_i \]
  mit (nichtlinearen) Funktionen $f_1$, \ldots, $f_m$
\end{bsp}


\begin{defn}
  Eine Matrix $A^{-} \in \R^{n \times m}$ heißt \emph{g-Inverse} (\textit{g = generalized}) von $A \in \R^{m \times n}$, wenn für jedes $y \in \R^m$, für welches $Ax = y$ lösbar ist, auch $x = A^{-} y$ eine Lösung ist.
\end{defn}

\begin{satz}
  $A^{-}$ ist eine g-Inverse von $A$ $\iff$ $A A^{-} A = A$
\end{satz}

\begin{bem}
  \begin{itemize}
    \item Falls $n = m$ und $A^{-1}$ existiert, so ist $A^{-} = A^{-1}$ eindeutig.
    \item $A^{-}$ ist im Allgemeinen nicht eindeutig.
    Man erhält Eindeutigkeit durch Zusatzforderungen:
  \end{itemize}
\end{bem}

\begin{defn}
  Eine \emph{Moore-Penrose-Inverse} $A^{+}$ ist eine g-Inverse, welche folgende Bedingungen erfüllt:
  \[
    A^{+} A A^{+} = A^{+}, \quad
    (A A^{+})^T = A A^{+}, \quad
    (A^{+} A)^T = A^{+} A.
  \]
\end{defn}

% Allgemeine Lösung von (N)
\begin{samepage}

\begin{satz}
  Die allgemeine Lösung von (N) lautet mit $S \coloneqq X^T X$:
  \[
    \beta = S^{-} X^T y + (S^{-} S - I_p) z, \quad
    \text{wobei } z \in \R^p.
  \]
  % Andererseits kann jede Lösung von (N) in dieser Form angegeben werden.
  Für die spez. Lsg
  $\hat{\beta} = S^{-} X^T Y$
  (mit $z = 0$)
  der MkQ-Schätzung gilt
  \[
    \E \hat{\beta} = S^{-} S \beta
    \quad \text{und} \quad
    \cov(\hat{\beta}) = \sigma^2 S^{-} S S^{-}.
  \]
\end{satz}

\begin{bem}
  Bei Nichteindeutigkeit der Lsg von (N) gilt i.\,A. $S^{-} S \neq I_p$. \\
  Falls $\rk X = \rk S = p$, so gilt $\E \hat{\beta} = \beta$ und $\cov \hat{\beta} = \sigma^2 S^{-1}$
\end{bem}

% §4.2. Der Begriff der schätzbaren Funktion (estimable function)
\subsection{Schätzbare Funktionen}

\end{samepage}

\begin{defn}
  Eine Linearkombination $\ell(\beta) = c^T \beta$ mit $c \in \R^p$, $\beta \in \R^p$ heißt bzgl. des linearen Modells $[Y, X \beta, \sigma^2 I_n]$ \emph{schätzbare Funktion}, falls ein $a \in \R^n$ mit $c = X^T a$ existiert.
\end{defn}

% Die Bedeutung schätzbarer Funktionen ergibt sich aus

\begin{satz}
  Es sind äquivalent:
  \begin{itemize}
    \item $\ell(\beta) = c^T \beta$ ist eine schätzbare Funktion.
    \item $\hat{\ell} \coloneqq \ell(\hat{\beta}) \coloneqq c^T \hat{\beta}$ (wobei $\hat{\beta}$ MkQ-Schätzung) ist eine lineare Funktion von~$Y$ und eine erwartungstreue Schätzung für $\ell(\beta)$ %(\dh{} $\E l(\hat{\beta}) = l(\beta)$)
    \item $c \in \im(X^T) = \im(X^T X)$
    \item $\ell(\hat{\beta}) = c^T \hat{\beta}$ ist konstant für alle $\hat{\beta}$, die Lösung von (N) sind.
    \item Es existiert ein $a \in \R^n$ mit $\E (a^T Y) = c^T \beta$.
  \end{itemize}
\end{satz}

% Vorlesung vom 7.12.2015

\begin{satz}[\emph{Gauß-Markov}]
  In einem lin. Modell $[Y, X \beta, \sigma^2 I_n]$ ex. für jede schätzbare (lin.) Funktion $\ell(\beta) = c^T \beta$ eine eindeutig bestimmte, in~$Y$ lin. erwartungstreue Schätzung $\hat{\ell} = a_*^T Y$ (für genau ein $a_* \in \im(X) \subseteq \R^n$) und diese hat die Form $\hat{\ell} = \ell(\hat{\beta}) = c^T \hat{\beta}$, wobei~$\hat{\beta}$ eine MkQ-Schätzung ist.
  Außerdem besitzt $\hat{\ell}$ minimale Varianz in der Klasse aller linearen erwartungstreuen Schätzungen $\hat{\ell} = a^T Y$.
\end{satz}

\begin{konstr}
  $a_* = X (X^T X)^{-} c$
\end{konstr}

\begin{defn}
  Der Schätzer heißt \textit{Best Linear Unbiased Estimator} (\emph{BLUE}).
\end{defn}

% §4.3 Schätzung der Modellstreuung $\sigma^2$
\subsection{Schätzung der Modellstreuung $\sigma^2$}

\begin{bem}
  Es gilt
  %Darstellung von $S(Y, \hat{\beta})$:
  \begin{align*}
    S(Y, \hat{\beta}) & = \min_{\beta \in \R^p} \norm{Y - X \beta}^2 = \norm{Y - X \hat{\beta}}^2 = (Y - X \hat{\beta})^T (Y - X \hat{\beta}) = \\
    & = Y^T Y - \underbrace{Y^T X \hat{\beta}}_{\mathclap{= (X \hat{\beta})^T X \hat{\beta}}} - (X \hat{\beta})^T Y + \underbrace{(X \hat{\beta})^T X \hat{\beta}}_{\mathclap{= \hat{\beta}^T X^T X \hat{\beta} = \beta^T X^T Y}} = \norm{Y}^2 - \norm{X \hat{\beta}}^2.
  \end{align*}
\end{bem}

\begin{defn}
  $(Y - X \hat{\beta})$ \enspace
  heißt \emph{Restvektor} oder \emph{Residuum}.
\end{defn}

\begin{lem}
  Für die MkQ-Schätzung~$\hat{\beta}$ gilt
  \begin{itemize}
    \item $\E (Y - X \hat\beta) = 0$,
    \item $c^T \beta$ ist eine schätzbare Funktion und $\E (c^T \hat{\beta} (Y - X \hat\beta)) = 0$ \TODO{Was ist $c$???},
    \item $\cov(Y \!-\! X \hat\beta) \!=\! \E S(Y, \hat{\beta}) = \E [\norm{Y}^2 - \norm{X \beta}^2] = \cov(Y) - \cov(X \hat\beta)$.
  \end{itemize}
\end{lem}

\begin{verf}[Orthogonale Transformation eines linearen Modells] \mbox{} \\
  Sei $[Y, X \beta, \sigma^2 I_n]$ geg. und $r = \rk X \leq p$.
  Wähle eine orthonormale Basis $o_1, \ldots, o_r$ von $\im(X) \subseteq \R^n$.
  Ergänze diese zu einer ONB $o_1, \ldots, o_n$ von $\R^n$.
  Wir setzen
  \[
    O_1 = (o_1 \cdots o_r), \quad
    O_2 = (o_{r+1} \cdots o_n), \quad
    O = (O_1 \, O_2) = (o_1, \ldots, o_n).
  \]
  Wir betrachten nun das lineare Modell $[Z, O^T X \beta, \sigma I_n]$, wobei
  \[
    Z \coloneqq O^{-1} Y = O^T Y = \begin{psmallmatrix}
      O_1^T \\ O_2^T
    \end{psmallmatrix}.
  \]
  Es gilt \enspace
  \begin{minipage}[t]{0.7 \linewidth}
    $
      \setlength\arraycolsep{1.5pt}
      \begin{array}[t]{r l}
        \cov(Z) & = \cov(O^T Y) = O^T \cov(Y) O = \sigma^2 I \\
        \E Z & = O^T \E Y = O^T X \beta = \begin{psmallmatrix}
          O_1^T X \beta \\ O_2^T X \beta
        \end{psmallmatrix} = \begin{psmallmatrix}
          O_1^T X \beta \\ 0
        \end{psmallmatrix}
      \end{array}
    $
  \end{minipage}
\end{verf}

\begin{samepage}

\begin{satz}
  Sei $[Y, X \beta, \sigma^2 I_n]$ geg., $r \coloneqq \rk X$ und $\hat{\beta}$ eine MkQ-Schätzung.
  Dann ist eine erwartungstreue Schätzung für~$\sigma^2$ gegeben durch
  \[
    \hat{\sigma}^2
    = \tfrac{1}{n-r} S(Y, \hat{\beta})
    = \tfrac{1}{n-r} \norm{Y - X \hat\beta}^2
    = \tfrac{1}{n-r} \sum_{i=1}^n (Y_i - \sum_{j=1}^p x_{ij} \hat\beta_j)^2.
  \]
\end{satz}

% Vorlesung vom 10.12.2015

% §4.4 Das normalverteilte lineare Modell $[Y, \Normal_n(X \beta, \sigma^2 I_n)]$
\subsection{Normalverteilte lineare Modelle}

\end{samepage}

\begin{satz}
  Für ein normalverteiltes lineares Modell $[Y, \Normal_n(X \beta, \sigma^2 I_n)]$ mit $\rk X = r \leq p$ gilt:
  \begin{itemize}
    \item Die ML-Schätzung für $\beta \in \R^p$ stimmt mit der MkQ-Schätzung $\hat\beta$ überein und es gilt $\hat\beta \sim \Normal_p(\E \hat\beta, \cov(\hat{\beta}))$.
    \item Die ML-Schätzung für $\sigma^2$ lautet $\hat\sigma_n^2 = \tfrac{S(Y, \hat{\beta})}{n} = \tfrac{n-r}{n} \hat\sigma^2$.
    Es gilt $\E \hat\sigma_n^2 = \tfrac{n-r}{n} \sigma^2 \xra[n \to \infty]{} \sigma^2$ (asympt. erw.-treu) und $\tfrac{S(Y, \hat\beta)}{\sigma^2} \sim \chi_{n-r}^2$.
    \item Für einen Vektor $\ell^T(\beta) = (\ell_1(\beta), \ldots, \ell_q(\beta))$ von $q \leq r$ linear unabhängigen schätzbaren Funktionen $\ell_i(\beta) = c_i^T \beta$, $c_i \in \R^p$
    gilt
    \[
      \hat{\ell} \coloneqq \ell(\hat{\beta}) \sim \Normal_q(\ell(\beta), \sigma^2 A_* A_*^T) \quad
      \text{mit } q = \rk A_*.
    \]
    Dabei ist $A_* = (a_{*,1}, \ldots, a_{*,q})^T$ mit $a_{*,i} \in L(X)$ optimal gemäß dem Gauß-Markov-Theorem.
    \item Die Schätzungen $\hat{\ell} = \ell(\hat\beta)$ und $\hat\sigma^2$ (bzw. $\hat\sigma_n^2$) sind unabhängig.
  \end{itemize}
\end{satz}

\begin{kor}
  Für $\rk X = p$ gilt $\hat{\beta} \sim \Normal_p(\beta, \sigma^2 (X^T X)^{-1})$ und $\hat\beta$ und $\hat\sigma^2$ sind unabhängig.
  (Grund: $\beta_i = e_i^T \beta$ sind schätzbare Funktionen.)
\end{kor}

\begin{test}[{$\sigma^2$-Streuungstest im Modell $[Y, \Normal_n(X \beta, \sigma^2 I_n)]$}] \mbox{} \\
  Wir testen \testh{$H_0 : \sigma^2 = \sigma_0^2$ vs. $H_1 : \sigma^2 \neq \sigma_0^2$}.
  Wir verwenden dazu
  \[
    T \coloneqq \tfrac{\norm{Y - X \hat{\beta}}^2}{\sigma_0^2}
  \]
  Unter $H_0$ gilt $T \sim \chi_{n-r}^2$, wobei $r \coloneqq \rk X \leq p$.
\end{test}

\begin{entscheidungsregel}
  Wir lehnen $H_0$ genau dann ab, falls
  \[
    T \in K^* = \cinterval{0}{\chi_{n-r, \nicefrac{\alpha}{2}}^2} \cup \cointerval{\chi_{n-r, 1 - \nicefrac{\alpha}{2}}^2}{\infty}.
  \]
\end{entscheidungsregel}

\begin{bem}
  Sei $\ell(\beta) = (\ell_1(\beta), \ldots, \ell_q(\beta))^T$ ein Vektor von linear unabh. schätzbaren Fktn, wobei $1 \leq q \leq r \leq p < n$.
  Setze $w \coloneqq \ell(\hat\beta) - \ell(\beta)$. \\
  Die Konfidenzschätzung für $\ell(\beta)$ ist dann
  \[
    \P(w^T (A_* A_*^T)^{-1} w \leq \tfrac{q}{n-r} \norm{Y - X \hat\beta}^2 \cdot F_{q, n - r, 1 - \alpha}) = 1 - \alpha.
  \]
  % wobei $F_{n_1, n_2}$ die F-Verteilung mit $(n_1, n_2)$ Freiheitsgraden ist.
\end{bem}

% Vorlesung vom 14.12.2015

% §4.5 Anwendung auf das Modell I der Varianzanalyse (ANOVA)
\subsection{Anwendung auf das Modell I der Varianzanalyse}

% 4.5.1 Einfache Klassifikation, Einwegklassifikation

%\begin{bsp}
  \begin{minipage}{0.52 \linewidth}
    \begin{bsp}
      Ziel ist der Vergleich von Erwartungswerten von $p$ Stufen (\textit{Populationen}), je $\Normal(\mu_i, \sigma^2)$-verteilter unabhängiger Beobachtungen ($i = 1, \ldots, p$).
      Für alle Populationen~$i$ tragen wir alle $n_i$ Messergebnisse in den \textit{Versuchsplan} (rechts) ein.
    \end{bsp}
  \end{minipage} \quad
  \begin{minipage}{0.43 \linewidth}
    \renewcommand{\arraystretch}{1.5}
    \scalebox{0.9}{
      \begin{tabular}{c | c c c c}
        & $1$ & $2$ & $\cdots$ & $n_i$ \\ \hline
        $1$ & $y_{11}$ & $y_{12}$ & $\cdots$ & $y_{1,n_1}$ \\
        $2$ & $y_{21}$ & $y_{22}$ & $\cdots$ & $y_{2,n_2}$ \\
        \vdots & \vdots & \vdots & & \vdots \\
        $p$ & $y_{p1}$ & $y_{p2}$ & $\cdots$ & $y_{p,n_p}$ \\
      \end{tabular}
    }
  \end{minipage}
  \vspace{4pt}
  In unserem math. Modell gibt es einen (unbek.) Vektor $\mu \in \R^p$ mit
  \[
    Y_{ik} = \mu_i + \epsilon_{ik}, \enspace
    \epsilon_{ik} \sim \Normal(0, \sigma^2) \text{ (\iid{})} \quad
    \text{für $n = 1, \ldots, n_i$, $i = 1, \ldots, p$.}
  \]
  % ausgelassen: Matrixgleichung
  (Wichtig ist, zu prüfen, ob tatsächlich die Varianz der $\epsilon_{ik}$ gleich ist, etwa mit dem \textit{Bartlett-Test}.)
  Die Transponierte~$X^T$ der Versuchs- planmatrix $X \in \R^{n \times p}$, $n \coloneqq n_1 + \ldots + n_p$, ist in Zeilenstufenform, wobei die $i$-te Zeile aus genau $n_i$ Einsen besteht.
  Es gilt
  \[
    X^T X = \begin{psmallmatrix}
      n_1 &&& 0 \\
      & n_2 && \\
      && \ddots && \\
      0 &&& n_p
    \end{psmallmatrix}, \quad
    X^T Y = \begin{psmallmatrix}
      Y_{1 \bullet} = {\sum}_{k=1}^{n_1} Y_{1k} \\
      \vdots \\
      Y_{p \bullet} = {\sum}_{k=1}^{n_p} Y_{pk}
    \end{psmallmatrix}
  \]
  Aus der Normalengleichung $X^T X \beta = X^T Y$ folgt
  \[
    \hat{\mu}_i = \overline{Y}_{i \bullet} = \tfrac{1}{n_i} {\sum}_{k=1}^{n_i} Y_{ik} \quad
    \text{für $i = 1, \ldots, p$}.
  \]
  Die Schätzung der Modellstreuung ist
  \[
    \hat{\sigma}^2 = \tfrac{1}{n-p} \norm{Y - X \hat{\beta}}^2 = \tfrac{1}{n-p} \sum_{i=1}^p \sum_{k=1}^{n_i} (Y_{ik} - \overline{Y}_{i \bullet})^2.
  \]
  Es gilt $(n-p) \tfrac{\hat{\sigma}^2}{\sigma^2} \sim \chi_{n-p}^2$.
%\end{bsp}

\begin{test}
  Wir testen \testh{$H_0 : \mu_1 = \mu_2 = \ldots = \mu_p$ vs. $H_1 : \ex{i, j} \mu_i \neq \mu_j$}.
  Als Testgröße verwenden wir
  \begin{align*}
    & T \coloneqq \tfrac{n-p}{p-1} \cdot \tfrac{S_1^2 - S_0^2}{S_0^2}
    \quad \text{mit} \enspace
    S_0^2 \coloneqq \norm{Y - X \hat{\beta}}^2 = \sum_{i=1}^p \sum_{k=1}^{n_i} (Y_{ik} - \overline{Y}_{i \bullet})^2, \\
    & S_1^2 \coloneqq \sum_{i=1}^p \sum_{k=1}^{n_i} (Y_{ik} - \overline{Y}_{\bullet \bullet})^2 \quad
    \rightsquigarrow S_1^2 - S_0^2 = \ldots = \sum_{i=1}^p n_i (\overline{Y}_{i \bullet} - \overline{Y}_{\bullet \bullet})^2
  \end{align*}
  % ausgelassen: Herleitung
  Unter $H_0$ gilt $T \sim F_{p-1, n-p}$.
\end{test}

\begin{entscheidungsregel}
  Ablehnung von $H_0$ $\iff$ $T \geq F_{p-1,n-p,1-\alpha}$
\end{entscheidungsregel}

\begin{sprechweise}
  Die üblichen Bezeichnungen sind
  \[
    \arraycolsep=1.4pt
    \begin{array}{r l l}
      S_1^2 & = \text{SQG} & = \text{S.\,d.\,Q.\,d.\,A. in der Gesamtheit} \\
      S_1^2 - S_0^2 & = \text{SQA} & = \text{S.\,d.\,Q.\,d.\,A. zwischen den Stufen des Faktors $A$} \\
      S_0^2 & = \text{SQR} & = \text{S.\,d.\,Q.\,d.\,A. innerhalb der Stufen des Faktors $A$} \\
      && = \text{Restquadratesumme},
      \end{array}
  \]
  \begin{align*}
  \end{align*}
  wobei "`S.\,d.\,Q.\,d.\,A."' = "`Summe der Quadrate der Abweichungen"'.
\end{sprechweise}

\begin{test}[\emph{Bartlett}]
  Seien $X_i \sim \Normal(\mu_i, \sigma_i^2)$, $i = 1, \ldots, p$, unabh. ZGn. \\
  Wir prüfen \testh{$H_0 : \sigma_1^2 = \ldots = \sigma_p^2 = \sigma^2$ vs. $H_1 : \ex{i, j} \sigma_i^2 \neq \sigma_j^2$}. \\
  Dazu verwenden wir die Testgröße
  \begin{align*}
    T_{n_1, \ldots, n_p} & \coloneqq \tfrac{1}{D} \left( (n-p) \log S^2 - {\sum}_{i=1}^p (n_i - 1) \log S_i^2 \right), \\
    D & \coloneqq 1 + \tfrac{1}{3 (- 1)} \left( {\sum}_{i=1}^p \tfrac{1}{n_i - 1} - \tfrac{1}{n-p} \right) \\
    S_i^2 & \coloneqq \tfrac{1}{n_i - 1} {\sum}_{k=1}^{n_i} (X_{ik} - \overline{X}_{i \bullet})^2 \\
    S^2 & \coloneqq \tfrac{1}{n-p} {\sum}_{i=1}^p (n_i - 1) S_i^2
  \end{align*}
  Unter $H_0$ gilt $T_{n_1, \ldots, n_p} \xra[\min(n_1, \ldots, n_p) \to \infty]{d} \chi_{p-1}^2$.
\end{test}

\begin{samepage}

\begin{faustregel}
  Die Näherung ist gut, falls $\min(n_1, \ldots, n_p) \geq 5$.
\end{faustregel}

\begin{entscheidungsregel}
  Ablehnung v. $H_0$ $\Leftrightarrow$ $T_{n_1, \ldots, n_p} > \chi_{p-1, 1-\alpha}^2$.
\end{entscheidungsregel}

\iffalse
Beispiel: Wirkung vonschmerzstillenden Medikamenten
Verabreichung von 2 Medikamenten und einem Placebo.
Gemessen wird die Zeitdauer nach Einnahme, in der sich der Patient schmerzfrei fühlt

5 Patienten mit Placebo \\ 
4 Patienten mit Droge A \\
6 Patienten mit Droge B

\begin{tabular}
  Placebo & 2,2 & 0,3 & 1,1 & 2,0 & 3,4 && $\sim \Normal(\mu_1, \sigma^2)$ \\
  Droge A & 2,8 & 1,4 & 1,7 & 4,3 &&& $\sim \Normal(\mu_2, \sigma^2)$ \\
  Droge B & 1,1 & 4,2 & 3,8 & 2,6 & 0,5 & 4,3 & $\sim \Normal(\mu_2, \sigma^2)$
\end{tabular}

$H_0 : \mu_1 = \mu_2 = \mu_3$

% Rechnen...

$T = 0,652$, damit Annahme von $H_0$
\fi

% Vorlesung vom 17.12.2015
\subsection{Zweifache Varianzanalyse (\textit{Zweiwegklassifikation})}

\end{samepage}

\begin{situation}
  Wir wollen die Wirkung eines Faktors~$A$ in~$p$ Stufen und die Wirkung eines Faktors~$B$ in~$q$ Stufen mit~$s$ Wiederholungen in jeder Stufe von Faktor~$A$ und~$B$ untersuchen.
\end{situation}

\begin{bsp}
  Wir untersuchen den Ernteertrag abhängig vom Düngemittel (Faktor~A) und der Bodenart (Faktor~B).
  Insbesondere sind wir an den Wechselwirkungen der Faktoren interessiert.
\end{bsp}

\begin{modell}
  $Y_{ijk} = \mu_0 + \alpha_i + \beta_j + \gamma_{ij} + \epsilon_{ijk}$, \enspace wobei
  \[
    \begin{array}{r l}
      \mu_0 & \text{Grundniveau} \\
      \alpha_i & \text{mittlerer Effekt in Stufe~$i$ von Faktor~A} \\
      \beta_j & \text{mittlerer Effekt in Stufe~$j$ von Faktor~B} \\
      \gamma_{ij} & \text{mittlerer Effekt aus Wechselwirkung der Stufen $i$ und $j$} \\
      \epsilon_{ijk} & \text{ZG mit $\E \epsilon_{ijk} = 0$ und $\E \epsilon_{ijk}^2 = \sigma^2$ (\iid{})}
    \end{array}
  \]
  % ausgelassen: Versuchsplan
  Dies lässt sich als lineares Modell $Y = X \beta + \epsilon$ mit
  \begin{align*}
    & \beta = (\mu_0, \alpha_1, \ldots, \alpha_p, \beta_1, \ldots, \beta_q, \gamma_{11}, \ldots, \gamma_{pq}) \in \R^{1+p+q+pq}, \\
    & X \in \R^{pqs \times (1+p+q+pq)}, \quad
    \rk X = pq - 1 < \min \{ pqs, 1 + p + q + pq \}
  \end{align*}
  schreiben.
  Wegen des zu kleinen Ranges ist eine Reparametrisierung notwendig, \dh{} es werden Gleichungen zwischen den Parametern hinzugefügt, die die eindeutige Lösbarkeit von (N) garantieren:
  \[
    \alpha_\bullet = 0, \quad
    \beta_\bullet = 0, \quad
    \gamma_{1 \bullet} = \ldots = \gamma_{p \bullet} = 0, \quad
    \gamma_{\bullet 1} = \ldots = \gamma_{\bullet q} = 0.
  \]
  \iffalse
    Diese Bedingungen bedeuten keine Einschränkung der Allgemeinheit der Darstellung $Y_{ijk} = \mu_0 + \alpha_i + \beta_j + \gamma_{ij} + \epsilon_{ijk}$ denn:
    $\mu_0^* = \mu_0 + \overline{\alpha}_\bullet + \overline{\beta}_\bullet + \overline{\gamma}_\bullet$,
    $\alpha_i^* = \alpha_i - \overline{\alpha}_\bullet + \overline{\gamma}_{i \bullet} - \overline{\gamma}_{\bullet \bullet}$,
    $\beta_j^* = \beta_j - \overline{\beta}_\bullet + \overline{\gamma}_{\bullet j} - \overline{\gamma}_{\bullet \bullet}$ und
    $\gamma_{ij}^* = \gamma_{ij} - \overline{\gamma_{i \bullet}} - \overline{\gamma_{\bullet j}} + \overline{\gamma}_{\bullet \bullet}$
    für $i = 1, \ldots, p$, $j = 1, \ldots, q$.
    Es ergeben sich die Gleichungen
    $Y_{ijk} = \mu_0^* + \alpha_i^* + \beta_j^* + \gamma_{ij}^* + \epsilon_{ijk}$ für $i = 1, \ldots, p$, $j = 1, \ldots, q$, $k = 1, \ldots, s$
  \fi
\end{modell}

\begin{nota}
  $\hat{\mu}_0 = \overline{Y}_{\bullet \bullet \bullet}$, \enspace
  $\hat{\alpha}_i = \overline{Y}_{i \bullet \bullet} - \overline{Y}_{\bullet \bullet \bullet}$, \enspace
  $\hat{\beta}_j = \overline{Y}_{\bullet j \bullet} - \overline{Y}_{\bullet \bullet \bullet}$, \enspace
  $\hat{\gamma}_{ij} = \overline{Y}_{ij \bullet} - \overline{Y}_{i \bullet \bullet} - \overline{Y}_{\bullet j \bullet} + \overline{Y}_{\bullet \bullet \bullet}$
\end{nota}

\begin{test}
  Wir testen die Hypothesen
  \[
    \text{\testh{$H_{AB} : \gamma_{11} = \ldots = \gamma_{pq}$}}, \quad
    \text{\testh{$H_A : \alpha_1 = \ldots = \alpha_p$}}, \quad
    \text{\testh{$H_B : \beta_1 = \ldots = \beta_q$}}
  \]
  mit den Testgrößen
  \begin{align*}
    F_{AB} & \coloneqq \frac{pq (s-1)}{(p-1)(q-1)} \cdot \frac{s}{S_{pqs}^2} \cdot \sum_{i=1}^p \sum_{j=1}^q (\overline{Y}_{i j \bullet} - \overline{Y}_{i \bullet \bullet} - \overline{Y}_{\bullet j \bullet} + \overline{Y}_{\bullet \bullet \bullet})^2, \\
    F_A & \coloneqq \frac{pq \cdot (s-1)}{p-1} \cdot \frac{qs}{S_{pqs}^2} \cdot \sum_{i=1}^p (\overline{Y}_{i \bullet \bullet} - \overline{Y}_{\bullet \bullet \bullet})^2, \\
    F_B & \coloneqq \frac{pq \cdot (s-1)}{q-1} \cdot \frac{ps}{S_{pqs}^2} \cdot \sum_{j=1}^q (\overline{Y}_{\bullet j \bullet} - \overline{Y}_{\bullet \bullet \bullet})^2 \quad \text{wobei} \\
    S_{pqs}^2 & \coloneqq \sum_{i=1}^p \sum_{j=1}^q \sum_{k=1}^s (Y_{ijk} - \overline{Y}_{i j \bullet})^2.
  \end{align*}
  Unter $H_A$ gilt $F_A \sim F_{p-1,pq(s-1)}$,
  unter $H_B$ gilt $F_B \sim F_{q-1,pq(s-1)}$ und
  unter $H_{AB}$ gilt $F_{AB} \sim F_{(p+1)(q+1), pq(s-1)}$.
\end{test}

\begin{entscheidungsregel}
  % $\alpha > 0$ sei ein gegebenes Signifikanzniveau
  Die Hypothesen $H_A$, $H_B$ bzw. $H_{AB}$ werden genau dann abgelehnt, falls
  \begin{align*}
    & F_A > F_{p-1,pq(s-1),1-\alpha}, \quad
    F_B > F_{q-1,pq(s-1),1-\alpha}
    \enspace \text{bzw.} \\
    & F_{AB} > F_{(p-1)(q-1),pq(s-1),1-\alpha}.
  \end{align*}
\end{entscheidungsregel}

\begin{bem}
  Die Anzahl der Wiederholungen kann auch in den einzelnen Stufen variieren.
  % für Details siehe: V. Nollau, Statistische Analysen, Leipzig, 1978, H. Ahrens, J. Läuter: Mehrdimensionale Varianzanalyse, Berlin, 1981
\end{bem}

% Vorlesung vom 21.12.2015

% §4.6 Regressionkurvenschätzer
\subsection{Regressionskurvenschätzer}

\begin{problem}
  Zu~$n$ Messwerten $(x_1, y_1)$, \ldots, $(x_n, y_n)$ soll eine Fktn $\mu(x)$ gefunden werden, deren Funktionswerte~$\mu(x_i)$ die~$y_i$ möglichst gut approximieren.
\end{problem}

\begin{modell}[nichtparametrisches Regressionsmodell, \emph{Modell I}]
  \[
    Y_i = \mu(x_i) + \epsilon_i \quad
    \text{für } i = 1, \ldots, n
  \]
  mit unbekannter Regressionsfunktion $\mu : \cinterval{a}{b} \to \R^1$. \\
  Dabei gilt
  $\E \epsilon_i = 0$ und
  $\E \epsilon_i \epsilon_j = \sigma^2 \delta_{ij}$.
\end{modell}

\begin{bem}
  Im \emph{Modell II} werden die $x_i$'s durch ZGn $X_i$ ersetzt.
\end{bem}

\begin{voraussetzung}
  $K \!\in\! L^1(\R)$ ist eine \emph{Kernfktn}, \dh{} $\IInt{K(x)}{x} = 1$, $\fa{x} K(-x) = K(x)$, $\supp K$ ist beschränkt und $\sup \abs{K(x)} < \infty$.
\end{voraussetzung}

\begin{nota}
  $K_h(x) \coloneqq \tfrac{1}{h} K(\tfrac{x}{h})$
\end{nota}

\begin{defn}
  Seien \textit{Messstellen} $a \leq x_1 < x_2 < \ldots < x_n \leq b$ gewählt. \\
  Der \emph{Gasser-Müller-Schätzer} mit \textit{Bandweite} $h_n > 0$ ist
  \[
    \hat{\mu}^{\text{(GM)}}_n(x) \coloneqq \tfrac{1}{n h_n} \sum_{i=1}^n Y_i \Int{s_{i-1}}{s_i}{K(\tfrac{x-s}{h_n})}{s}, \quad
    a \leq x \leq b
  \]
  wobei $s_0 \coloneqq a$, $s_{n+1} \coloneqq b$ und $s_i \coloneqq \nicefrac{(x_{i-1} + x_i)}{2}$.
\end{defn}

\begin{bem}
  Es gilt $\hat{\mu}_n(x) = (K_{h_n} * h)(x)$ mit $h(x) = Y_i$ für $x \in \cointerval{s_i}{s_{i+1}}$.
\end{bem}

% Verbessert: $\hat{\mu}_n(x) = \tfrac{b-a}{n h_n} \sum_{i=1}^n Y_i K(\tfrac{x-x_i}{h_n})$. % warum soll das besser sein?

\iffalse
  Motivation: $\E \hat{\mu}_n(x) = \tfrac{1}{n h_n} \sum_{i=1}^n \E Y_i \Int{s_{i-1}}{s_i}{K(\tfrac{x-s}{h_n})}{s} = \tfrac{1}{n} \sum_{i=1}^n \mu(x_i) \Int{s_{i-1}/h_n}{s_i/h_n}{K(\tfrac{x}{h_n} - t)}{t} = \tfrac{1}{h} \sum_{i=1}^n \mu(x_i) \Int{(x-s_{i-1})/h_n}{(x-s_i)/h_n}{K(y)}{y} \approx \mu(x)$
\fi

Für Modell II gibt es folgenden Kernschätzer:

\begin{defn}
  Seien $(X_1, Y_1)$, \ldots, $(X_n, Y_n)$ \iid{} mit Dichte $f_{(X,Y)}$, $K$ eine Kernfunktion und $hn > 0$.
  Der \emph{Nadaraya-Watson-Schätzer} ist
  \[
    \hat{\mu}^{\text{(NW)}}_n (x) = \frac{{\sum}_{i=1}^n Y_i K(\tfrac{x - X_i}{h_n})}{{\sum}_{i=1}^n \hphantom{Y_i} K(\tfrac{x - X_i}{h_n})}, \qquad
    a \leq x \leq b.
  \]
\end{defn}

% Bedingte Erwartung: $\E (Y - \E(Y | \tilde{\sigma}))^2 = \min_{Z \text{ $\mathcal{F}$-messbar}, \E Z^2 < \infty} \E (Y-Z)^2$, $\mathcal{F} \subseteq \Alg$, $(\Omega, \Alg, \P)$
% Falls $\mathcal{F} = \sigma(X) = $ von $X$ erzeugte $\sigma$-Algebra, so schreiben wir $\E(Y|X)$ anstelle $\E(Y|\sigma(X)$ und es gilt: eine Borel-messbare Funktion $g : \R \to \R \cup \{ \pm \infty \}$ mit $\E(Y|X) = g(X)$ (Faktorisierungslemma).
% Wir können dann schreiben: $g(x) = \E(Y|X=x) = \IInt{y f_{Y|X}(y|x)}{y}$, wobei $f_{Y|X}(y|x) =$ bedingte Dichte von Y bei gegebenen $X=x$ $= \frac{f_{(X, Y)}(x, y)}{f_X(x)}$ mit $f_X(x) = \IInt{f_{(X, Y)}(x,y)}{y}$.

\begin{motivation}
  Unter geeigneten Voraussetzungen gilt
  \[
    \mu(x) = \E(Y|X=x) = \IInt{y \cdot f_{(Y|X)}(y)}{y} = \IInt{y \tfrac{f_{(X,Y)}(x,y)}{f_X(x)}}{y}
  \]
  Wir schätzen $f_{(X,Y)}(x,y)$ mit einem \textit{Produktkern}, also $K(x, y) = K_1(x) \cdot K_2(y)$ mit Kernfunktionen $K_1$, $K_2$:
  \begin{align*}
    \hat{f}_n(x, y) & \coloneqq \tfrac{1}{n h_n^2} \sum_{i=1}^n K_1(\tfrac{x-X_i}{h_n}) K_2(\tfrac{y-Y_i}{h_n}), \\
    \hat{f}_n(x) & = \tfrac{1}{n h_n} \sum_{i=1}^n K_1(\tfrac{x - X_i}{h_n}) = \IInt{\hat{f}_n(x,y)}{y} \\
    \hat{\mu}_n (x) & = \IInt{y \tfrac{\hat{f}_n(x,y)}{\hat{f}_n(x)}}{y} = \tfrac{1}{\hat{f}_n(x)} \tfrac{1}{n h_n^2} \sum_{i=1}^n K_1(\tfrac{x - X_i}{h_n}) \IInt{y K_2(\tfrac{y - Y_i}{h_n})}{y} \\
    & \xra[n \to \infty]{} \mu(x) \qquad
    \text{(bei $h_n \to 0$, $h_n \cdot n \to \infty$ für $n \to \infty$)}
  \end{align*}
  % genauer in Helmut Pruscha, Vorlesungen über Mathematische Statistik, S. 312f
\end{motivation}

\begin{defn}
  %\begin{minipage}[t]{0.8 \linewidth}
    $\MSE(\hat{\theta}_n) = \E (\hat{\theta}_n - \theta)^2$ heißt \textit{mean squared error},
    $\MASE(\hat{\mu}_n) = \tfrac{1}{n} \sum_{i=1}^n \MSE(\hat{\mu}_n (x_i))$
     heißt \textit{mean averaged squared error}
  %\end{minipage}
\end{defn}

\begin{bem}
  Es gilt \enspace
  $\MSE(\hat{\theta}_n) = \var(\hat{\theta}_n) + (\underbrace{\E (\hat{\theta} - \theta)}_{\text{\emph{Bias}}})^2$
\end{bem}

\begin{satz}
  Seien die Annahmen des Modell I erfüllt und $\mu \in \Cont^2(\cinterval{0}{1})$.
  Die Messstellen seien $x_i = \nicefrac{(i - \nicefrac{1}{2})}{n}$ für $i = 1, \ldots, n$.
  Der Kern~$K$ sei Lipschitz-stetig und es gelte $\supp K \subseteq \cinterval{-1}{1}$.
  Angenommen, $h_n \downarrow 0$, $n h_n \to \infty$ für $n \to \infty$.
  Dann gilt für den Gasser-Müller-Schätzer:
  \begin{align*}
    \E \hat{\mu}_n(x) - \mu(x) & = \tfrac{h_n^2}{2} \IInt{x^2 K(x)}{x} \mu''(x) + o(h_n^2) + O(\tfrac{1}{n}) \\
    \var \hat{\mu}_n (x) & = \tfrac{1}{n h_n} \sigma^2 \IInt{K^2(x)}{x} + O(\tfrac{1}{n^2 h_n^2})
  \end{align*}
  % ausgelassen: "optimal" = ?
\end{satz}
% siehe Helmut Pruscha, Vorlesungen über Mathematische Statistik, S. 314ff

\begin{kor}
  $
    \arraycolsep=0.2pt
    \begin{array}[t]{r l}
      \MSE(\hat{\mu}(x)) = & \tfrac{\sigma^2}{n h_n} \IInt{K^2 (x)}{x} + \tfrac{h_n^4}{4} \left( \IInt{x^2 K(x)}{x} \cdot \mu''(x) \right)^2 \\
      & + o(\tfrac{1}{n h_n} + h_n^4)
    \end{array}
  $
\end{kor}

\columnbreak

% Vorlesung vom 7.1.2016

% §5. Dichteschätzungen
\section{Dichteschätzungen}

% §5.1 Einführungen

\begin{nota}
  Sei $\mathcal{P}$ die Menge aller bezüglich des Lebesgue-Maß $\lambda_1$ absolut stetigen Wahrscheinlichkeitsmaße auf $\R^1$ und
  \[ \mathcal{F}_c \coloneqq \Set{f \in \Cont(\R^1)}{f = \nicefrac{\d P}{\d \lambda_1} \text{ für ein } P \in \mathcal{P}} \]
  die Menge der stetigen W-Dichtefunktionen.
\end{nota}

\begin{ziel}
  Finden einer "`guten"' Dichteschätzung $\hat{f}_n(X, \blank) : \R^1 \to \R^1$, wobei $X = (X_1, \ldots, X_n)$ eine math. Stichprobe ist, in Form einer Borel-messbaren Abbildung $\hat{f}_n(\blank, \blank) : \R^n \times \R^1 \to \R^1$.
\end{ziel}

\begin{nota}
  $\hat{f}_n(t) \coloneqq \hat{f}_n(X_1, \ldots, X_n, t)$
\end{nota}

\begin{lem}
  Es gibt keinen Dichteschätzer $\hat{f}_n(\blank)$ mit
  \[
    \E_f \hat{f}_n(t) = f(t) \quad
    \text{für $\lambda_1$-fast alle $t$ für alle $f \in \mathcal{F}_c$.}
  \]
\end{lem}

% §5.2. Naiver Dichteschätzer-Histogramm (Säulendiagramm)
\subsection{Histogramm-Schätzer}

\begin{defn}
  Sei $x_0 \in \R$ und $h > 0$ fest.
  Setze $I_j \coloneqq \cointerval{x_0 + jh}{x_0 + (j\!+\!1) h}$ für $j \in \Z$.
  Das \emph{Histogramm} ist der (naive) Dichte-Schätzer
  \[
    \hat{f}_n(t) \coloneqq \tfrac{1}{nh} \sum_{i=1}^n \ind_{I_j}(X_i), \quad
    \text{wobei $j \in \Z$ so ist, dass $t \in I_j$}.
  \]
\end{defn}

\begin{bem}
  Der Graph der geschätzten Histogramm-Dichte ist ein Säulendiagramm.
  Verbindet man die Mitten der Säulen mit einer Linie, so bekommt man einen \emph{Häufigkeitspolygonzug}.
\end{bem}

\begin{bem}
  Nach dem Gesetz der großen Zahlen gilt
  \[
    \hat{f}_n(t) \xra[n \to \infty]{\text{$\P_f$-f.s.}}
    h^{-1} \Int{I_j}{}{f(x)}{x} \quad
    \text{für $t \in I_j$.}
  \]
\end{bem}

\begin{defn}
  Sei $\hat{f}_n(\blank)$ ein Dichteschätzer und $f \in \mathcal{F}_c$.
  Dann heißt
  \[
    \underbrace{\MISE(\hat{f}_n)}_{\Delta_n \coloneqq} \coloneqq
    \E_f \Int{\R}{}{(\hat{f}_n(t) - f(t))^2}{t} =
    \Int{\R}{}{\underbrace{\E_f (\hat{f}_n(t) - f(t))^2}_{=: \MSE(\hat{f}_n(t))}}{t}
  \]
  \emph{MISE} (\textit{mean integrated squared error}) von $\hat{f}_n$ bzgl. $f$.
\end{defn}

% Original-Paper: http://math.sjtu.edu.cn/faculty/chengwang/files/2015fall/1.pdf
\begin{satz}[Freedman, Diaconis]
  Sei $f : \R \to \R$ eine Dichtefkt mit
  \begin{enumerate}[label=(\roman*), itemindent=10pt]
    \item $f \in L^2(\R)$ und $f$ absolut stetig, \dh{} f.\,ü. diff'bar,
    \item $f' \in L^2(\R)$ und $f'$ absolut stetig, \dh{} f.\,ü. diff'bar und
    \item $f'' \in L^p(\R)$ für ein $p \in \cinterval{1}{2}$.
  \end{enumerate}
  Wir schreiben
  \[
    \alpha \coloneqq \sqrt[3]{6} \cdot \gamma^{- \nicefrac{1}{3}}, \quad
    \beta \coloneqq \frac{3}{2 \sqrt[3]{6}} \gamma^{\nicefrac{1}{3}}, \quad
    \gamma \coloneqq \Int{\R^1}{}{(f'(t))^2}{t}.
  \]
  Gelte $\gamma > 0$.
  Dann ist \enspace
  $\min_{h = h_n > 0} \Delta_n^2 = \tfrac{\beta}{n^{\nicefrac{2}{3}}} + O \left( \tfrac{1}{n} \right)$. \\
  Das Minimum wird angenommen für \enspace
  $h_n = \tfrac{\alpha}{\sqrt[3]{n}} + O \left( \tfrac{1}{\sqrt{n}} \right)$.

  Angenommen, es gilt nur (i), $f' \in L^2(\R)$ und $\gamma > 0$.
  Dann gilt
  \[
    \min_{h = h_n > 0} \Delta_n^2 = \tfrac{\beta}{n^{\nicefrac{2}{3}}} + o \left( \tfrac{1}{n^{\nicefrac{2}{3}}} \right)
    \enspace \text{mit Minimum bei} \enspace
    h_n = \tfrac{\alpha}{\sqrt[3]{n}} + o \left( \tfrac{1}{\sqrt[3]{n}} \right).
  \]
\end{satz}

\begin{samepage}

% §5.3. Kerndichteschätzer
\subsection{Kerndichteschätzer}

\begin{defn}
  Sei $K \in L^1(\R)$ eine Fktn mit $\IInt{K(t)}{t} = 1$ und $(h_n)_{n \in \N}$ eine Folge in $\ointerval{0}{\infty}$ mit $h_n \downarrow 0$.
  Dann heißt
  \[
    \hat{f}_n(t) =
    \hat{f}_n(X_1, \ldots, X_n; t) \coloneqq
    \tfrac{1}{n \cdot h_n} \sum_{i=1}^n K \left( \tfrac{X_i - t}{h_n} \right) =
    \tfrac{1}{n} \sum_{i=1}^n K_{h_n} (X_i - t)
  \]
  \emph{Kerndichteschätzer} für $f$ mit \textit{Kernfunktion} $K$.
  % auch: Parzen-Rosenblatt-Schätzung
\end{defn}

\end{samepage}

% Vorlesung vom 11.1.2016

\begin{bspe}
  Mit der \emph{empirischen Dichte} $K(x) \coloneqq \tfrac{1}{2} \ind_{\ocinterval{-1}{1}}(x)$ gilt
  \[ \hat{f}_n(t) = \tfrac{1}{2 h_n} \left( \hat{F}_n(t + h_n) - \hat{F}_n(t - h_n) \right), \]
  mit dem \emph{Gauß-Kern} $K(x) \coloneqq \tfrac{1}{\sqrt{2 \pi}}\exp( \tfrac{- x^2}{2})$ gilt $\hat{f}_n(\blank) \in \Cont^\infty(\R)$.
\end{bspe}

\begin{lem}
  Sei zusätzlich $K \in L^2(\R)$.
  Dann gilt:
  \begin{itemize}
    \item $\E_f \hat{f}_n(t) = \IInt{K(x) f(t + h_n x)}{x}$
    \item
    $\var_f (\hat{f}_n(t)) = 
    \begin{array}[t]{l}
      \tfrac{1}{n \cdot h_n} \cdot \IInt{K^2(x) \cdot f(t + h_n x)}{x} \\
      - \tfrac{1}{n} \cdot \left( \IInt{K(x) \cdot f(t + h_n x)}{x} \right)^2
    \end{array}$
  \end{itemize}
\end{lem}

\begin{satz}
  Sei $f$ eine beschränkte W-Dichtefktn, $\fa{x \in \R} f(x) \leq M$, mit Stetigkeitsstellen $C \subseteq \R$.
  Sei $K \in L^2(\R)$ und $(h_n)_{n \in \N}$ eine Folge mit $h_n \downarrow 0$ für $n \to \infty$.
  Angenommen, $n \cdot h_n \xra[n \to \infty]{} \infty$.
  Dann gilt:
  \begin{align*}
    \E_f \hat{f}_n(t) & \xra[n \to \infty]{} f(t) \quad \forall \, t \in C, \\
    n \cdot h_n \cdot \var_f (\hat{f}_n(t)) & \xra[n \to \infty]{} f(t) \cdot \IInt{K^2(x)}{x} \quad \forall \, t \in C, \\
    \sup_{t \in \R} \E_f \left( \hat{f}_n(t) - f(t) \right)^2 & \xra[n \to \infty]{} 0, \quad \text{falls $f$ glm. stetig auf $\R$.}
  \end{align*}
\end{satz}

\begin{satz}
  Sei $f \in \Cont^2(\R^1)$, $f, \abs{f'}, \abs{f''} \leq M < \infty$ und $\IInt{x^2 \abs{K(x)}}{x} < \infty$.
  Dann gilt für alle $t \in \R^1$:
  \[
    \E_f \hat{f}_n(t) = f(t) + h_n f'(t) \IInt{x K(x)}{x} + \tfrac{1}{2} h_n^2 f''(t) \IInt{x^2 K(x)}{x} + o(h_n^2).
  \]
  wobei die Konvergenz von $o(\blank)$ sogar glm. in abg. $t$-Intervallen ist.
\end{satz}

\begin{ziel}
  Bestimmung einer optimalen Bandbreite $h_n$
\end{ziel}

\begin{satz}
  Sei $f \in \Cont^2(\R^1)$ mit $f'' \in L^2(\R^1)$ und $f, \abs{f'} \leq A < \infty$. \\
  Für $K$ gelte $0 \leq K(x) \leq B$, $K(-x) = K(x)$ und $\IInt{x^2 K(x)}{x} < \infty$. \\
  Dann gilt für $(h_n)_{n \in \N}$ mit $h_n \downarrow 0$ und $n h_n \xra[n \to \infty]{} \infty$:
  \begin{align*}
    \MISE(\hat{f}_n) = & \tfrac{1}{n h_n} \IInt{K^2(x)}{x} + \tfrac{h_n^4}{4} \left( \IInt{x^2 K(x)}{x} \right)^2 \IInt{(f''(t))^2}{t} \\
    & + o(h_n^4) + o(\tfrac{1}{n h_n}) \qquad
    \text{für } n \to \infty.
  \end{align*}
\end{satz}

% Vorlesung vom 14.1.2016

\begin{bem}
  Wir wollen zunächst $h_n$ irgendwie optimal wählen. \\
  Setzen wir die beiden echten Fehlerterme gleich, also
  \[
    \tfrac{1}{n h_n} \IInt{K^2(x)}{x} = \tfrac{h_n^4}{4} \left( \IInt{x^2 K(x)}{x} \right) \IInt{(f''(t))^2}{t},
  \]
  so folgt $h_n^{*} = \tfrac{c^*}{n^{\nicefrac{1}{5}}}$ mit $c^* \coloneqq \left( \frac{4 \IInt{K(x)^2}{x}}{(\IInt{x^2 K(x)}{x})^2 \IInt{(f''(t))^2}{t}} \right)^{\nicefrac{1}{5}}$
  und
  \begin{align*}
    \MISE(\hat{f}_n) = \, & \tfrac{\sqrt[5]{8}}{n^{\nicefrac{4}{5}}} \left( \IInt{(f''(t))^2}{t} \right)^{\nicefrac{1}{5}} \left( \IInt{x^2 K(x)}{x} \right)^{\nicefrac{2}{5}} \left( \IInt{K(x)^2}{x} \right)^{\nicefrac{4}{5}} \\
    & + o(n^{- \nicefrac{4}{5}}).
  \end{align*}
\end{bem}

\begin{bem}
  Nun wollen wir die Kernfunktion $K$ optimal wählen, also so, dass der IMSE in der letzten Gleichung möglichst klein wird. \\
  Dazu suchen wir eine Funktion $K$, sodass
  \[
    \IInt{x^2 K(x)}{x} \left( \IInt{K(x)^2}{x} \right)^2
  \]
  minimal wird unter den Nebenbed. $\IInt{K(x)}{x} = 1$ und $K(x) \geq 0$.
\end{bem}

\begin{satz}
  Sei $K$ eine Kernfkt mit $\IInt{K(x)}{x} = 1$.
  Dann gilt für $\alpha > 0$:
  \[
    \left( \IInt{K(x)^2}{x} \right)^\alpha \IInt{\abs{x}^\alpha \abs{K(x)}}{x} \geq \tfrac{1}{2 \alpha + 1} \left( \tfrac{\alpha + 1}{2 \alpha + 1} \right)^\alpha
  \]
  Gleichheit gilt für $K_0(x) = \tfrac{\alpha+1}{2 \alpha} (1 - \abs{x}^\alpha) \ind_{\cinterval{-1}{1}} (x)$.
\end{satz}

\begin{defn}
  Im Speziallfall $\alpha = 2$ heißt
  \[
    K_0(x) \coloneqq \tfrac{3}{4} (1 - x^2) \ind_{\cinterval{-1}{1}} (x) \quad
    \text{\emph{Epanetschnikow-Kern}}.
  \]
\end{defn}

% Vorlesung vom 18.1.2016

\begin{bem}
  Wichtige Kerne neben dem Epanetschnikow-Kern:
  \begin{itemize}
    \item \emph{Dirichlet-Kern}: \enspace $K_N(x) = \tfrac{1}{2} \sum_{k=-N}^N e^{i 2 \pi k x} = \tfrac{\sin((2 N + 1) \pi x)}{2 \sin (\pi x)}$ % für $\abs{x} \leq 1$ 
    % $1 + z + \ldots + z^N = \tfrac{z^{N+1} - 1}{z - 1}$, $z \neq 1$
    \item \emph{Fejér-Kern}: \enspace $K_N(x) = \tfrac{1}{2 N} \abs{\sum_{k=0}^N e^{i k \pi x}}^2 = \tfrac{\sin^2((2 N + 1) \pi x)}{2 N \sin^2(\pi x)}$ % für $\abs{x} \leq 1$
    \item \emph{Dreieckskern}: \enspace $K(x) = (1 - \abs{x}) \ind_{\cinterval{-1}{1}}(x)$
    \item \emph{Kosinuskern}: \enspace $K(x) = \begin{cases}
      \tfrac{\pi}{4} \cos(\tfrac{\pi}{2} x) & \abs{x} \leq 1 \\
      0 & \abs{x} > 1
    \end{cases}$
    \item \emph{Sobolev-Kern}: \enspace $K_\epsilon(x) = c_d \epsilon^{-d} \exp \left( - \tfrac{\epsilon^2}{\epsilon^2 - \norm{x}^2} \right) \ind_{B_\epsilon(0)}(x)$ \\
    Das Besondere an ihm ist $K_\epsilon \in \Cont^\infty(\R^d)$.
  \end{itemize}
\end{bem}

\begin{bemn}
  \begin{itemize}
    \item Es gibt Kerne mit der Eigenschaft $\IInt{x^k K(x)}{x} = 0$ für $k = 1, \ldots, m - 1$ und $\IInt{\abs{x}^m K(x)}{x} < \infty$ für $m \geq 2$.
    \item Es gibt Kerne mit $\IInt{x^n K(x)}{x} = 0$ für alle $n \in \N$, \zB{}
    \[
      K(x) = \tfrac{1}{2 \pi} \Int{- \infty}{\infty}{\cos(t x) g(t)}{t}
      \quad \text{mit} \quad
      g(t) \coloneqq \begin{cases}
        1 - e^{- \nicefrac{1}{t^2}} & \text{falls } t \neq 0, \\
        1 & \text{falls } t = 0
      \end{cases}
    \]
  \end{itemize}
\end{bemn}

\begin{satz}[H. Müntz]
  Sei $K : \R \to \R$ eine Fkt mit $\IInt{x^{n_i} K(x)}{x} = 0$ für $0 < n_1 < n_2 < \ldots$ mit ${\sum}_{i=1}^\infty \tfrac{1}{n_i} = \infty$, $\supp(K) = \cinterval{a}{b}$, $K \in \Cont \cinterval{a}{b}$. Dann gilt $K \equiv 0$.
\end{satz}

% 5.4 Orthogonale Schätzer für W-Dichten
\subsection{Orthogonale Schätzer für W-Dichten}

\begin{voraussetzung}
  Sei $M = \cinterval{a}{b}$ mit $- \infty < a < b < \infty$.
\end{voraussetzung}

\begin{bem}
  Der Hilbertraum $L^2(M)$ besitzt eine abzählbare ONB $e_1, e_2, \ldots$, sodass für alle $f \in L^2(M)$ gilt:
  \[
    f(x) = \sum_{k=1}^\infty \alpha_k e_k(x)
    \quad \text{mit } \alpha_k = \scp{f}{e_k}
    \quad \text{für } x \in \cinterval{a}{b}
  \]
\end{bem}

\begin{problem}
  Gegeben sei eine math. SP $X_1, \ldots, X_n$ aus einer Grundgesamtheit für ein zuf. Merkmal mit Dichte $f \in L^2(M)$.
  Gesucht ist eine Schätzung der Dichte~$f$.
\end{problem}

\begin{verf}
  Schätzung von~$f$ erfolgt in zwei Schritten:
  \begin{enumerate}
    \item Wähle einen Parameter $N = N_n$.
    Sei $f_N$ die Projektion von $f$ auf $L(e_1, \ldots, e_N) \subseteq L^2(M)$,
    \[
      f_N(x) = \sum_{k=1}^N \alpha_k e_k(x), \enspace x \in M
      \quad \text{mit } \alpha_k = \scp{f}{e_k}
    \]
    \item Schätzung der Koeffizienten:
    \[
      \hat{\alpha}_k = \hat{\alpha}_{k,n} = \tfrac{1}{n} \sum_{k=1}^n e_k(X_i), \enspace k = 1, \ldots, N_n
    \]
  \end{enumerate}
\end{verf}

\begin{defn}
  Der \emph{ON-Schätzer} (oder \textit{Projektionsschätzer}) ist
  \[
    \hat{f}_n (x) = \hat{f}_{n,N} (x) \coloneqq \sum_{k=1}^N \hat{\alpha}_{k,n} e_k(x) = \tfrac{1}{n} \sum_{i=1}^n \sum_{k=1}^N e_k(X_i) e_k(x).
  \]
\end{defn}

Eigenschaften des Schätzers $\hat{f}_n$:

% $\hat{f}_n$ hängt (stetig) von der Wahl der ONB ab

\begin{lem}
  Für $f \in L^2 (M)$ und $e_1 (x) \equiv \mathrm{const} = \tfrac{1}{\sqrt{\abs{M}}}$, $\abs{M} = b - a$ gilt
  \[
    \Int{M}{}{\hat{f}_n(x)}{x} = 1
    \quad \text{und} \quad
    \E \hat{f}_n(x) = f_N(x) \, \forall x \in M.
  \]
\end{lem}

% Vorlesung vom 21.1.2016

\begin{kor}[Pktweise asympt. Erwartungstreue]
  Für $N_n \xra[n \to \infty]{} \infty$ gilt $\E \hat{f}_n(x) \xra[n \to \infty]{\text{f.\,ü.}} f(x)$, falls $f_N(x) \xra[N \to \infty]{\text{f.\,ü.}} f(x)$.
\end{kor}

\begin{lem}
  Sei $f \in L^2(M)$.
  Dann gilt für ON-Schätzer
  \[
    \norm{\hat{f}_n - f}_2 = \sum_{k=1}^N (\hat{\alpha}_{k,n} - \alpha_k)^2 + \sum_{k = N+1}^\infty \alpha_k^2
  \]
\end{lem}

% ausgelassen: Beispiel Intervall und trigonometrische Funktionen

\begin{satz}
  Es sind äquivalent:
  \begin{itemize}
    \item $J_n = \E \norm{\hat{f}_{n,N_n} - f}_2^2 = \E \AInt{M}{(\hat{f}_{n,N_n}(x) - f(x))^2}{x} \xra[n \to \infty]{} 0$
    \item $\tfrac{1}{n} \sum_{i=1}^{N_n} \AInt{M}{e_k^2(x) f(x)}{x} \xra[n \to \infty]{} 0$
  \end{itemize}
\end{satz}

\begin{kor}
  Aus $J_n \xra[n \to \infty]{} 0$ folgt $\norm{\hat{f}_{n,N_n} - f}_2 \xra[n \to \infty]{\P} 0$.
\end{kor}

% Im Skript von Lichtenstern kommt hier noch ein Satz von E. Liebscher

% §6. Erzeugung von Zufallszahlen und Simulationstests
\section{Erz. von Zufallszahlen und Simulationstests}

% §6.1. Erzeugung von nach einer VF verteilter Zufallszahlen

\begin{ziel}
  Erzeugung von nach einer VF~$F$ verteilten Zufallszahlen
\end{ziel}

\begin{defn}
  Sei $F$ eine VF auf $\R^1$.
  Dann heißt $F^{-} : \cinterval{0}{1} \to \cointerval{-\infty}{\infty}$ mit
  \[
    F^{-}(y) \coloneqq \min \Set{x \in \R^1}{F(x) \geq y}
    \text{ für } y \in \ocinterval{0}{1}, \quad
    F^{-}(0) = \lim_{y \downarrow 0} F^{-}(y)
  \]
   \emph{Quantilfunktion} oder (verallgemeinerte) \emph{Pseudo-Inverse} zu~$F$.
\end{defn}

\begin{eign}
  \begin{itemize}
    \item $F^{-}$ ist monoton und linksseitig stetig.
    \miniitem{0.48 \linewidth}{$F(F^{-}(y)) \geq y \enspace \forall \, y \in \cinterval{0}{1}$}
    \miniitem{0.48 \linewidth}{$U \sim \Uniform \cinterval{0}{1}$ $\implies$ $F^{-}(U) \sim F$}
  \end{itemize}
\end{eign}

\begin{verf}[\emph{Inversionsmethode}]
  Ist $(U_i)_{i \in \N}$ eine Folge unabhängiger gleichverteilter Zufallszahlen auf $\cinterval{0}{1}$, dann ist $(F^{-}(U_i))_{i \in \N}$ eine Folge $F$-verteilter Zufallszahlen.
\end{verf}

% Vorlesung vom 25.1.2016

% §6.2. Erzeugung von Zufallszahlen über Transformation
\subsection{Erzeugung von Zufallszahlen über Transformation}

% 1. Verwerfungsmethode (rejection method)
\begin{verf}[\emph{Verwerfungsmethode}]
  Angenommen, $X$ besitze eine Dichte $f$ mit beschr. Träger $\supp f \subseteq \cinterval{a}{b}$ und $\sup f(x) \leq M$.
  Wir erzeugen unabh. Zufallszahlen $U_1, U_2 \sim \Uniform \cinterval{0}{1}$ und setzen
  \[
    V_1 = a + (b-a) U_1 \sim \Uniform \cinterval{a}{b}
    \quad \text{und} \quad
    V_2 = M U_2 \sim \Uniform \cinterval{0}{M}.
  \]
  Wir nehmen $X \coloneqq V_1$ falls $(V_1, V_2) \in \Graph(f) \iff V_2 \leq f(V_1)$, andernfalls beginnen wir mit neuen Werten~$V_1$ und~$V_2$ von vorn.
\end{verf}

% Ausgelassen: Beweis

\begin{verf}[\emph{Box-Muller}] \mbox{} \\
  Seien $U_1, U_2 \sim \Uniform \cinterval{0}{1}$ unabhängig.
  Dann sind auch unabhängig:
  \begin{align*}
    X_1 & \coloneqq g_1(U_1, U_2) \coloneqq \sqrt{- 2 \log U_1} \sin(2 \pi u_2) \sim \Normal(0, 1), \\
    X_2 & \coloneqq g_2(U_1, U_2) \coloneqq \sqrt{- 2 \log U_1} \cos(2 \pi U_2) \sim \Normal(0, 1)
  \end{align*}
\end{verf}

\begin{bem}
  Allgemein sei $h = \begin{psmallmatrix} h_1 \\ h_2 \end{psmallmatrix}$ die Inverse von $g = \begin{psmallmatrix} g_1 \\ g_2 \end{psmallmatrix}$.
  Dann gilt
  \[
    f_{(X_1, X_2)}(x_1, x_2) = \frac{f_{U_1, U_2}(u_1, u_2)}{\det(J_g (u_1, u_2))}
    \quad \text{mit} \quad
    \begin{psmallmatrix}
      u_1 \\ u_2
    \end{psmallmatrix} = \begin{psmallmatrix}
      h_1(x_1, x_2) \\ h_2(x_1, x_2)
    \end{psmallmatrix}
  \]
wobei $J_g (x, y)$ die Jacobi-Matrix von~$g$ im Punkt $(x, y)$ ist.
\end{bem}

% Ausgelassen: Rechnung für Box-Muller

\begin{verf}[Erzeugung eines $n$-dim. ZV $Y \sim \Normal_n(m, S)$] \mbox{} \\
  Sei $m \in \R^n$ und $S \in \R^{n \times n}$ eine Kovarianzmatrix, \dh{} positiv semidefinit und symmetrisch.
  Mit Cholesky-Zerlegung bekommt man eine untere Dreiecksmatrix $L \in \R^{n \times n}$ mit $S = L \cdot L^T$. \\
  Wir erzeugen \iid{} Zufallszahlen $X_1, \ldots, X_n \sim \Normal(0, 1)$ und setzen
  \[
    Y = (Y_1, \ldots, Y_n)^T \coloneqq m + X L^T, \quad
    X = (X_1, \ldots, X_n).
  \]
  Dann gilt $X \sim \Normal_n(0, I_n)$ und $Y \sim \Normal_n(m, \cov(Y))$ mit
  \[
    \cov(Y) = \cov(X L^T) = L \cov(X) L^T = L L^T = S.
  \]
\end{verf}

% Vorlesung vom 28.1.2016

\begin{satz}
  Seien $X_1, \ldots, X_n \sim \Normal_n(0, I_n)$ \iid{} \\
  Dann ist der ZV $Y \coloneqq \nicefrac{X}{\norm{X}}$ gleichverteilt auf $S^{n-1}$.
\end{satz}

\begin{satz}
  Seien $E_1, \ldots, E_{n+1} \sim \Exp(1)$.
  \begin{itemize}
    \item Sei $Z_i \coloneqq \tfrac{E_i}{E_1 + \ldots + E_n}$.
    Dann ist $Z = (Z_1, \ldots, Z_n)$ gleichverteilt auf
    \[
      \Delta^n = \Set{x \in \cinterval{0}{1}}{x_1 + \ldots + x_n = 1} \subset \R^n.
    \]
    \item Sei $Z_i^* \coloneqq \tfrac{E_i}{E_1 + \ldots + E_{n+1}}$.
    Dann ist $Z^* = (Z_1^*, \nldots, Z_n^*)$ gleichvert. auf
    \[
      \conv \{ 0, e_1, \ldots, e_n \} = \Set{x \in \cinterval{0}{1}}{x_1 + \ldots + x_n  1} \subset \R^n.
    \]
  \end{itemize}
\end{satz}

\begin{bem}
  Seien $U_1, \ldots, U_n \sim \Uniform \cinterval{0}{1}$.
  Wir ordnen die Werte:
  \[
    U_{0:n} \coloneqq 0 < U_{1:n} < \ldots U_{n:n} < 1 =: U_{n+1:n}.
  \]
  Dann sind die ZVn $Z_i^* \coloneqq U_{i:n} - U_{i-1 : n}$ für $i = 1, \ldots, n+1$ unabh. und können für Punkt 2 im letzten Satz verwendet werden.
\end{bem}

% §6.3 Erzeugung von Zufallszahlen über Grenzwertsätze
\subsection{Erzeugung von Zufallszahlen über GWS}

\begin{verf}
  Seien $U_1, \ldots, U_n, \ldots \sim \Uniform \cinterval{0}{1}$ \iid{} \\
  Es gilt $\E U_i = \tfrac{1}{2}$ und $\var U_i = \tfrac{1}{12}$.
  Aus dem ZGWS folgt
  \[
    \frac{U_1 + \ldots + U_n - \tfrac{n}{2}}{\sqrt{\nicefrac{n}{12}}}
    \xra[d \to \infty]{\d} \Normal(0, 1).
  \]
\end{verf}

\begin{bsp}
  $U_1 + \ldots + U_{12} - 6 \approx \Normal(0, 1)$
\end{bsp}

\begin{satz}
  Für $\lambda > 0$ und $U_1, \ldots, U_n, \ldots \sim \Uniform \cinterval{0}{1}$ \iid{} gilt
  \[
    \tfrac{n}{\lambda} \min \{ U_1, \ldots, U_n \} \xra[n \to \infty]{\d} \Exp(\lambda).
  \]
\end{satz}

% §6.4 Erzeugung von stabil-verteilten Zufallsvariablen
\subsection{Erzeugung von stabil-verteilten Zufallsvariablen}

Sei $F$ eine Verteilungsfunktion, die sich als mögliche Grenzverteilung einer geeigneten zentrierten und normierten Summe ergibt, \dh{} für \iid{} ZVn $X_1, \ldots, X_n, \ldots$ gilt
\[
  \P(\tfrac{X_1 + \ldots + X_n - a_n}{b_n} \leq x) \xra[n \to \infty]{} F(x)
  \quad \text{wobei $b_n \to \infty$}.
\]

\begin{satz}
  Die charakteristische Funktion~$\varphi(t)$ von~$F$ hat die Gestalt
  \[
    \varphi(t) = \exp \{ - \lambda \abs{t}^\alpha (1 - i \beta \tfrac{t}{\abs{t}} \Phi) \}, \qquad
    \Phi = \begin{cases}
      \tan(\tfrac{\pi \alpha}{2}) & \text{für } \alpha \neq 1, \\
      - \tfrac{2}{\pi} \log \abs{t} & \text{für } \alpha = 1
    \end{cases}
  \]
  für Parameter $\lambda \in \ointerval{0}{\infty}$, $\alpha \in \ocinterval{0}{2}$ und $\beta \in \cinterval{-1}{1}$.
\end{satz}

\begin{bsp}
  Für $\alpha = 2$ ist $\varphi(t) = \exp \{ - \lambda t^2 \}$ und $F$ die Normalverteilung.
\end{bsp}

\begin{bemn}
  \begin{itemize}
    \item Für $F$ existiert eine "`glockenförmige"' Dichte $f \in \Cont^\infty(\R)$.
    \item Für $0 < \alpha < 2$ ex. nur die Momente $\IInt{\abs{x}^\delta}{F(x)}$ für $0 < \delta < \alpha$.
  \end{itemize}
\end{bemn}

% Vorlesung vom 1.2.2016

% §5.6. Monte-Carlo-Tests (Simulationstests)
\subsection{Monte-Carlo-Tests (Simulationstests)}

Wir betrachten ein parametrisches Modell mit Parametermenge~$\Theta$.
Angenommen, $X$ ist ein zufälliges Merkmal mit $P_X \in \Set{P_\theta}{\theta \in \Theta}$ und $X^{(n)} = (X_1, \ldots, X_n)$ eine mathematische Stichprobe von~$S$.

\begin{ziel}
  Konstr. eines $\alpha$-Tests für \testh{$H_0 : \theta \in \Theta_0 \subset \Theta$ vs. $H_1 : \theta \not\in \Theta_0$} mittels einer Teststatistik $T^{(n)}(X_1, \ldots, X_n)$.
\end{ziel}

\begin{nota}
  $
    \arraycolsep=0.4pt
    \begin{array}[t]{r l}
      F_\theta^{(n)} (t) \coloneqq & \P(T^{(n)}(X_1, \ldots, X_n) \leq t) \\
      = & P_\theta^{(n)}(\Set{x^{(n)} \in \R^n}{T^{(n)}(x_1, \ldots, x_n) \leq t})
    \end{array}
  $
\end{nota}

\begin{voraussetzung}
  Es existiert ein $\theta_0 \in \Theta_0$ mit \quad
  \inlineitem{$F_{\theta_0}^{(n)}$ ist stetig.}
  \begin{itemize}
    \item $F_{\Theta_0}^{(n)} (t) \leq F_\theta^{(n)} (t) \quad \forall \, t \in \R, \, \theta \in \Theta_0$
    \item $F_{\Theta_0}^{(n)} (t) \geq F_\theta^{(n)} (t) \quad \forall \, t \in \R, \, \theta \in \Theta \setminus \Theta_0$
  \end{itemize}
\end{voraussetzung}

\begin{bem}
  Die zweite Bedingung ist für $\Theta_0 = \{ \theta_0 \}$ trivial.
\end{bem}

\begin{test}
  Sei $F_{\theta_0}^{-1}$ die Quantilfkt zu $F^{(n)}_{\theta_0}$.
  Wir verwenden die Regel
  $
    \varphi^{(n)} (x_1, \ldots, x_n) \coloneqq \begin{cases}
      1 & \text{falls } T^{(n)} (x_1, \ldots, x_n) > F_{\theta_0}^{-1}(1-\alpha), \\
      0 & \text{falls } T^{(n)} (x_1, \ldots, x_n) \leq F_{\theta_0}^{-1}(1-\alpha)
    \end{cases}.
  $
\end{test}

\begin{lem}
  $\varphi^{(n)}$ ist ein unverfälschter $\alpha$-Test.
\end{lem}

\begin{problem}
  Die VF $F_{\theta_0}^{(n)}$ ist oft zu komplex, um sie explizit zu berechnen.
  Gleiches gilt dann auch für $F_{\theta_0}^{-1}$.
\end{problem}

\begin{verf}[\emph{Monte-Carlo-Test}]
  \begin{itemize}
    \item Erzeugung von $N$ Realisierungen $x_1^{(n)}, \ldots, x_N^{(n)}$ gemäß der Verteilung~$P_{\theta_0}^{(n)}$.
    \item Berechnen der Ordnungsstatistik $T_{N-k+1 : N}^{(n)}$ von $T^{(n)}(x_i^{(n)})$. \\
    (Dies ist eine Schätzung von $F_{\theta_0}^{-1}(1 - \alpha)$ mit $\alpha = \nicefrac{k}{N+1}$.)
    \item Die Entscheidungsregel des MC-Tests ist
    \[
      \varphi(x^{n}; x_1^{(n)}, \ldots, x_N^{(n)}) = \begin{cases}
        1 & \text{falls } T^{(n)} (x^{(n)}) > T_{N-k+1 : N}^{(n)}, \\
        0 & \text{falls } T^{(n)} (x^{(n)}) \leq T_{N-k+1 : N}^{(n)}
      \end{cases}
    \]
  \end{itemize}
\end{verf}

% ausgelassen: Bemerkung zum praktischen Vorgehen

\begin{lem}
  Der MC-Test ist ein unverfälschter $\alpha$-Test.
\end{lem}

\end{document}

TODO:

Cochran's Theorem
Verschiedene Arten der Konvergenz von ZGn
Charakteristische Funktionen
Was ist Konsistenz von Schätzern?
Summenkonvention erklären

Folgendes Kapitel an passende Stelle verschieben:

% 1.5. Mehrdimensionale Normalverteilung

\begin{defn}
  Die $k$-dim \emph{(Gaußsche) Normalverteilung} $\Normal_k(m, C)$ mit EW~$m \in \R^k$ und einer nichtnegativ-definiten, symmetrischen Kovarianzmatrix~$C \in \R^{k \times k}$ ist gegeben durch die Dichte
  \[ f_{\Normal_k(m, C)}(x) \coloneqq \tfrac{1}{(2\pi)^{k/2} \sqrt{\det(C)}} \exp \left( - \tfrac{1}{2} (x-m) C^{-1} (x-m)^T \right). \]
\end{defn}

\begin{bem}
  Bei $k=2$ schreibt man oft
  \[
    C = \begin{psmallmatrix}
      \sigma_1^2 & \sigma_1 \sigma_2 \rho \\
      \sigma_1 \sigma_2 \rho & \sigma_2^2
    \end{psmallmatrix}
    \quad \text{mit} \quad
    \rho \coloneqq \cor(X_1, X_2).
  \]
\end{bem}

\begin{defn}
  Die \emph{charakteristische Fkt} eines ZV $X = (X_1, \ldots, X_k)^T$ ist
  \[
    \varphi : \R^k \to \R, \enspace
    t \mapsto \E e^{i \scp{t}{X}} = \Int{\R^k}{}{e^{i (t_1 x_1 + \ldots + t_k x_k)}}{F_X(x_1, \ldots, x_k)}.
  \]
\end{defn}

\begin{bem}
  Die charakteristische Funktion von $\Normal_k(m, C)$ ist
  \[ \varphi_{\Normal_k(m, C)}(t) = \exp \left( i \sum_{i=1}^k t_i m_i - \tfrac{1}{2} \sum_{i,j=1}^k t_i c_{ij} t_j \right). \]
\end{bem}

\begin{satz}
  Für $A \in \R^{k \times l}$ gilt $\Normal_k(m, C) \cdot A = \Normal_l(m \cdot A, A^T C A)$.
\end{satz}