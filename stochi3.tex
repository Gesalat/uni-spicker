\documentclass{cheat-sheet}

\pdfinfo{
  /Title (Zusammenfassung Stochastik 3)
  /Author (Tim Baumann)
}

\usepackage{bbm} % Für 1 mit Doppelstrich (Indikatorfunktion)
\usepackage{mathtools} % psmallmatrix environment

% Kleinere Klammern
\delimiterfactor=701

% TODO: Include-File für Stochastik

\newcommand{\Alg}{\mathfrak{A}} % (Mengen-)Algebra
%\newcommand{\Ring}{\mathfrak{R}} % (Mengen-)Ring
%\newcommand{\LebAlg}{\mathfrak{L}} % Lebesgue-Borel-Mengen
\renewcommand{\P}{\mathbb{P}} % Wahrscheinlichkeitsmaß
\newcommand{\E}{\mathbb{E}} % Erwartungswert
\newcommand{\Bor}{\mathfrak{B}} % Borel
%\newcommand{\Leb}{\mathcal{L}} % Lebesgue
\newcommand{\ind}{\mathbbm{1}} % Indikatorfunktion
\newcommand{\Cont}{\mathcal{C}} % Menge der stetigen/diff'baren Funktionen
\newcommand{\scp}[2]{\left( #1 \!\mid\! #2 \right)} % Skalarprodukt

\DeclareMathOperator{\var}{Var} % Varianz
\DeclareMathOperator{\cov}{Cov} % Kovarianz
\DeclareMathOperator{\cor}{Cor} % Korrelation

% Verteilungen
\newcommand{\Normal}{\mathcal{N}} % Gaußsche Normalverteilung
\DeclareMathOperator{\Exp}{Exp} % Exponentialverteilung

\begin{document}

\maketitle{Zusammenfassung Stochastik 3}

% Vorlesung vom 12.10.2015

% 1. Wiederholung

% 1.1 Grundbegriffe der Testtheorie

\begin{modell}
  Gegeben sei ein Parametrisches Modell, \dh eine Zufallsgröße $X$, deren Verteilungsfunktion $P_X \in \Set{P_\vartheta}{\vartheta \in \Theta \subset \R^n}$ von einem Parameter $\vartheta$ abhängt.
\end{modell}

\begin{prob}
  Anhand einer \emph{Stichprobe} $x_1, \ldots, x_n \in \R^1$ von $X$ (\dh{} $x_1, \ldots, x_n$ sind Realisierung von iid ZGen $X_1, \ldots, X_n \sim P_X$) ist zu entscheiden, ob die sogenannte \emph{Nullhypothese} $H_0 : \vartheta \in \Theta_0 \subset \Theta$ oder eine \emph{Gegenhypothese} $H_1 : \vartheta \in \Theta_1 = \Theta \setminus \Theta_0$ angenommen oder abgelehnt werden soll.
\end{prob}

\begin{defn}
  Der \emph{Stichprobenraum} ist $(\R^n, \Bor(\R^n), P_\vartheta \times \ldots \times P_\vartheta)$
\end{defn}

\begin{terminologie}
  Die Hypothese $H_i$ heißt \emph{einfach}, falls $\abs{H_i} = 1$, andernfalls \emph{zusammengesetzt}.
\end{terminologie}

\begin{defn}
  Ein (nichtrandomisierter) \emph{Test} für $H_0$ gegen $H_1$ ist eine Entscheidungsregel über die Annahme von $H_0$ basierend auf einer Stichprobe, die durch eine messbare Abbildung $\varphi : \R^n \to \{ 0, 1 \}$ augedrückt wird und zwar durch
  \[ \varphi(x_1, \ldots, x_n) = \begin{cases}
    0 & \text{bei Annahme von $H_0$,} \\
    1 & \text{bei Ablehung von $H_0$.}
  \end{cases} \]
\end{defn}

\begin{defn}
  Der \emph{Ablehungsbereich} oder \emph{kritische Bereich} von $\varphi$ ist
  \[ K_n \coloneqq \Set{(x_1, \ldots, x_n) \in \R^n}{\varphi(x_1, \ldots, x_n) = 1}. \]
\end{defn}

\begin{bem}
  Es gilt $\varphi = \ind_{K_n}$.
\end{bem}

\begin{defn}
  Ein \emph{Fehler 1. Art} ist eine Ablehnung der Nullhypothese $H_0$, obwohl $H_0$ richtig ist;
  ein \emph{Fehler 2. Art} ist eine Annahme von $H_0$, obwohl $H_0$ falsch ist.
\end{defn}

% Ausgelassen: Stichprobenraum ist [R^n, B(R^n), P_\vartheta \times \ldots \times P_\vartheta]

\begin{defn}
  Die \emph{Güte- oder Machtfunktion} des Tests $\varphi$ ist
  \begin{align*}
    m_\varphi : \Theta \to \cinterval{0}{1},
    m_\varphi(\vartheta) & \coloneqq
    \E_\vartheta \varphi(X_1, \ldots, X_n) \\
    & = \P_\vartheta ((X_1, \ldots, X_n) \in K_n) \\
    & = (P_\vartheta \times \ldots \times P_\vartheta)(K_n)
  \end{align*}
  Die Gegenwsk. $(1 {-} m_\varphi(\vartheta))$ heißt \emph{Operationscharakteristik} von $\varphi$.
\end{defn}

\begin{bem}
  Es gilt
  \begin{alignat*}{2}
    \P_\vartheta(\text{Fehler 1. Art}) &= m_\varphi(\vartheta) & \enspace\text{für $\vartheta \in \Theta_0$,} \\
    \P_\vartheta(\text{Fehler 2. Art}) &= 1 - m_\varphi(\vartheta) & \enspace\text{für $\vartheta \in \Theta_1$.}
  \end{alignat*}
\end{bem}

% Ausgelassen: Graph einer "fast idealen Kurve"

\begin{defn}
  Ein Test $\varphi : \R^n \to \{ 0, 1 \}$ mit
  \[ \sup_{\vartheta \in \Theta_0} m_\varphi(\vartheta) \leq \alpha \]
  heißt \emph{$\alpha$-Test} o. \emph{Signifikanztest} zum \emph{Signifikanzniveau} $\alpha \in \ointerval{0}{1}$. \\[2pt]
  Ein $\alpha$-Test $\varphi$ heißt \emph{unverfälscht} (erwartungstreu, unbiased), falls
  \[ \inf_{\vartheta \in \Theta_1} m_\varphi(\vartheta) \geq \alpha. \]
\end{defn}

% Konstruktion nichtrandomisierter Tests mittels Stichprobenfunktionen (= Statistiken) im Falle einfacher Nullhypothesen

\begin{situation}
  Sei nun eine Stichprobenfunktion oder \emph{Teststatistik} $T : \R^n \to \R^1$ gegeben.
  Wir wollen einen Test der einfachen Nullhypothese $H_0 : \vartheta \in \Theta_0 = \{ \vartheta_0 \}$ entwickeln.
\end{situation}

\begin{defn}
  $K_n^T \subset \R^1$ heißt \emph{kritischer Bereich der Teststatistik}, falls
  \[ K_n = T^{-1}(K_n^T). \]
\end{defn}

\begin{bem}
  Es gilt
  \begin{alignat*}{4}
    m_\varphi(\vartheta_0) &= \P_{\vartheta_0} \left( (X_1, \ldots, X_n) \in K_n \right)
    &&= \\
    &= \P_{\vartheta_0} \left( (T(X_1), \ldots, T(X_n)) \in K_n^T \right)
    &&= \Int{K_n^T}{}{f_T(x)}{x} \leq \alpha,
  \end{alignat*}
  wobei $f_T$ die Dichte von $T(X_1, \ldots, X_n)$ unter $H_0$ ist.
\end{bem}

\begin{bsp}
  Sei $X \sim \Normal(\mu, \sigma^2)$, $\sigma$ bekannt und $\alpha \in \ointerval{0}{1}$ vorgegeben. \\
  Zum Test von $H_0 : \mu = \mu_0$ vs. $H_1 : \mu \neq \mu_0$ wählen wir als Statistik
  \[
    T(X_1, \ldots, X_n) \coloneqq \tfrac{\sqrt{n}}{\sigma} \left( \overline{X}_n - \mu_0 \right) \enspace
    \text{mit }
    \overline{X}_n \coloneqq \tfrac{1}{n} \left( X_1 + \ldots + X_n \right).
  \]
  Unter Annahme von $H_0$ gilt $T(X_1, \ldots, X_n) \sim \Normal(0,1)$. \\
  Der Ablehnungsbereich der Statistik ist
  \[
    K_n^T = \Set{t \in \R^1}{\abs{t} > z_{1 - \alpha/2}}
    \quad \text{mit} \quad
    z_{1 - \alpha/2} \coloneqq \Phi^{-1}(1 - \alpha/2).
  \]
  Für $\alpha = 0,5$ gilt beispielsweise $z_{1 - \alpha/2} \approx 1,96$.
\end{bsp}

% Vorlesung vom 15.10.2015

\begin{bem}
  Es gilt
  \begin{align*}
    t \in (K_n^T)^c
    &\iff \abs{t} \leq z_{1-\alpha/2}
    \iff \abs{\overline{X}_n - \mu_0} \leq \tfrac{\sigma}{\sqrt{n}} z_{1-\alpha/2} \\
    &\iff \mu_0 \in \cinterval{\overline{X}_n - \tfrac{\sigma}{\sqrt{n}} z_{1-\alpha/2}}{\overline{X}_n + \tfrac{\sigma}{\sqrt{n}} z_{1-\alpha/2}}.
  \end{align*}
  Letzteres Intervall wird \emph{Konfidenzintervall} für $\mu_0$ zum Konfidenz- niveau $1-\alpha$ genannt.
\end{bem}

\begin{bsp}
  Sei wieder $X \sim \Normal(\mu, \sigma^2)$, $\sigma^2$ aber diesmal unbekannt. \\
  Zum Testen von $H_0 : \mu = \mu_0$ vs. $H_1 : \mu \neq \mu_0$ verwenden wir
  \[
    \hat{T}(X_1, \ldots, X_n) = \tfrac{\sqrt{n}}{S_n} \left( \overline{X}_n - \mu_0 \right), \quad
    S_n^2 \coloneqq \tfrac{1}{n-1} \sum_{i=1}^n \left( X_i - \overline{X}_n \right)^2.
  \]
  Dabei ist $S_n$ die \emph{(korrigierte) Stichprobenvarianz}.
  Man kann zeigen, dass $\hat{T}(X_1, \ldots, X_n) \sim t_{n-1}$ unter $H_0$.
  Dabei ist $t_m$ die \emph{Student'sche $t$-Verteilung} mit $m$ Freiheitsgraden. \\
  Der Ablehnungsbereich ist
  \[ K_n^T = \Set{t \in \R^1}{\abs{t} > t_{n-1,1-\alpha/2}}. \]
\end{bsp}

\begin{bem}
  $S_n^2$ und $\overline{X}_n$ sind unabhängig für $n \geq 2$. % und umgekehrt?
\end{bem}

\begin{diskussion}
  \begin{itemize}
    \item Je kleiner $\alpha$ ist, desto "`nullhypothesenfreundlicher"' ist der Test.
    Häufig verwendet wird $\alpha \in \{ 10\%, 5\%, 1\%, 0,5\% \}$.
    \item Einseitige Tests: Die Gegenhypothese zu $H_0 \!:\! \mu \!=\! \mu_0$ ist $H_1 \!:\! \mu \!>\! \mu_0$.
    Die Nullhypothese wird nur abgelehnt, falls zu große Stichproben- mittelwerte $\overline{x}_n$ vorliegen. Es ist dann $K_n^T = \ointerval{z_{1-\alpha}}{\infty}$.
  \end{itemize}
\end{diskussion}

% 1.2. Prüfverteilung bei normalverteilten Grundgesamtheit

\begin{defn}
  Es seien $X_1, \ldots, X_n \sim \Normal(0, 1)$.
  Dann heißt die Summe $X_1^2 + \ldots + X_n^2 \sim \chi_n^2$ \emph{Chi-Quadrat-verteilt} mit $n$ Freiheitsgraden.
\end{defn}

\begin{defn}
  Falls $X \sim \Normal(0,1)$ und $Y_n \sim \chi_n^2$ unabhängig sind, so heißt
  \[ \tfrac{X}{\sqrt{\tfrac{Y_n}{n}}} \sim t_n \]
  \emph{$t$-verteilt} mit $n$-Freiheitsgraden.
\end{defn}

\begin{lem}
  $\tfrac{n-1}{\sigma^2} S_n^2 \sim \chi_{n-1}^2$
\end{lem}

\begin{kor}
  $\hat{T}$ aus dem zweiten obigen Bsp ist tatsächlich $t$-verteilt.
\end{kor}

\begin{defn}
  Seien $Y_{n_i} \sim \chi_{n_i}^2$, $i = 1, 2$ zwei unabhängige ZGen.
  Dann heißt %der Quotient
  \[ \tfrac{Y_{n_1} / n_1}{Y_{n_2} / n_2} \sim F_{n_1, n_2} \]
  \emph{F-verteilt} (wie Fisher) mit $(n_1, n_2)$ Freiheitsgraden.
  % auch: Suedecor-verteilt
\end{defn}

% Vorlesung vom 19.10.2015

\begin{bsp}
  Sei $X \sim \Normal(\mu, \sigma^2)$ mit $\mu$ unbekannt.
  Wir testen $H_0 : \sigma = \sigma_0$ vs. $H_1 : \sigma \neq \sigma_0$ mit
  \[ T \coloneqq \tfrac{n-1}{\sigma_0^2} S_n^2 \]
  Unter Annahme von $H_0$ gilt $T ~ \chi_{n-1}^2$.
  Falls $\mu$ bekannt ist, muss man
  \[
    \widetilde{T} \coloneqq \tfrac{n}{\sigma_0^2} \widetilde{S}_n^2, \quad
    \widetilde{S}_n^2 \coloneqq \tfrac{1}{n} \sum_{i=1}^n (X_i - \mu)^2.
  \]
  als Statistik wählen. Unter Annahme von $H_0$ ist $\widetilde{T} \sim \chi_n^2$.
\end{bsp}

\begin{bsp}
  Seien Stichproben $X_1^{(1)}, \ldots, X_{n_1}^{(1)} \sim \Normal(\mu_1, \sigma_1^2)$ und $X_1^{(2)}, \ldots, X_{n_2}^{(2)} \sim \Normal(\mu_2, \sigma_2^2)$ gegeben.
  Wir wollen $H_0 : \sigma_1 = \sigma_2$ gegen $H_1 : \sigma_1 \neq \sigma_2$ testen.
  Dazu verwenden wir
  \[
    T = \frac{S_{X^{(1)}}^2}{S_{X^{(2)}}^2}, \quad
    S_{X^{(j)}}^2 \coloneqq \tfrac{1}{n-1} \sum_{i=1}^{n_j} \left( X_i^{(j)} - \overline{X}^{(j)}_n \right)^n.
  \]
  Falls $H_0$ gilt, so ist $T \sim F_{n_1-1,n_2-1}$.
\end{bsp}

\begin{bsp}
  Situation wie im letzten Beispiel mit $\sigma_1 = \sigma_2$.
  Wir testen $H_0 : \mu_1 = \mu_2$ vs. $H_1 : \mu_1 \neq \mu_2$ mit
  \[
    T = \sqrt{\frac{n_1 \cdot n_2}{n_1 + n_2}} \cdot \frac{\overline{X}_{n_1}^{(1)} - \overline{X}_{n_2}^{(2)}}{S_{n_1,n_2}}, \quad
    S_{n_1,n_2}^2 = \frac{(n_1{-}1) S_{X^{(1)}}^2 + (n_2{-}1) S_{X^{(2)}}^2}{n_1 + n_2 - 2}
  \]
  Unter $H_0$ gilt $T \sim t_{n_1 + n_2 - 2}$.
\end{bsp}

\begin{bsp}
  Seien $\begin{psmallmatrix} X_1 \\ Y_1 \end{psmallmatrix}, \ldots, \begin{psmallmatrix} X_n \\ Y_n \end{psmallmatrix} \sim \Normal\left(
    \begin{psmallmatrix}
      \mu_1\vphantom{\sigma_1^2} \\
      \mu_2\vphantom{\sigma_2^2}
    \end{psmallmatrix},
    \begin{psmallmatrix}
      \sigma_1^2 & \sigma_1 \sigma_2 \rho \\
      \sigma_1 \sigma_2 \rho & \sigma_2^2
    \end{psmallmatrix} \right)$. \\[2pt]
  Wir testen $H_0 : \rho = 0$ vs. $H_1 : \rho \neq 0$ mit
  \[
    T \coloneqq \frac{\sqrt{n-2} \cdot \hat{\rho}_n}{\sqrt{1 - \hat{\rho}_n^2}}, \quad
    \hat{\rho}_n \coloneqq \frac{\tfrac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X}_n) (Y_i - \overline{Y}_n)}{S_{X,n} \cdot S_{Y,n}}.
    % Nenner ausgeschrieben: \sqrt{\tfrac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X}_n)^2 \tfrac{1}{n-1} \sum_{i=1}^n (Y_i - \overline{Y}_n)^2}
  \]
  Falls $H_0$ richtig ist, so gilt $T \sim t_{n-2}$. \\[2pt]
  Um $H_0 : \rho = \rho_0 \in \ointerval{0}{1}$ vs. $H_1 : \rho \neq \rho_0$ zu testen, kann man
  \[ T = \tfrac{\sqrt{n-3}}{2} \left( \log \tfrac{1 + \hat{\rho}_n}{1 - \hat{\rho}_n} - \log \tfrac{1+\rho_0}{1-\rho_0} \right) \]
  verwenden. Für $n$ groß gilt $T \sim \Normal(0, 1)$ unter $H_0$.
\end{bsp}

% 1.4. Lemma von Slutzky und varianzstabilisiernde Transformationen

\begin{lem}
  Seien $(X_n)$, $(Y_n)$ zwei Folgen von ZGn über $(\Omega, \Alg, \P)$ mit $X_n \xra[n \to \infty]{\P} c = \text{const}$ (\dh{} $\fa{\epsilon > 0} \P(\abs{X_n - c} > \epsilon) \to 0$) und $Y_n \xra[n \to \infty]{d} Y$ (\dh{} $\P(Y_n \leq y) \to \P(Y \leq y)$ für alle Stetigkeitspunkte $y$ der VF $y \mapsto \P(Y \leq y)$). Dann gilt:
  \[
    X_n + Y_n \xra{d} c + Y, \quad
    X_n \cdot Y_n \xra{d} c \cdot Y, \quad
    Y_n / X_n \xra{d} Y / c \enspace \text{(falls $c \neq 0$)}
  \]
  und allgemeiner $f(X_n, Y_n) \xra[n \to \infty]{d} f(c, Y)$ für jede Fkt $f \in \Cont(\R^2, \R)$.
\end{lem}

\begin{bem}
  Unabhängigkeit von $(X_n)$ und $(Y_n)$ wird nicht vorausgesetzt!
\end{bem}

% Vorlesung vom 22.10.2015

% Varianzstabilisierende Transformationen

\begin{situation}
  Sei $T_n = T(X_1, \ldots, X_n)$ eine Statistik.
  Falls der ZGWS für $T_n$ die Form
  \[ \sqrt{n} (T_n - \vartheta) \xra[n \to \infty]{d} \Normal(0, g(\vartheta)) \]
  besitzt, so benötigen wir für Hypothesentests eine Möglichkeit, die Abhängigkeit der Varianz von Parameter $\vartheta$ zu beseitigen.
  Man sagt, man führt eine \emph{varianzstabilisierende Transformation} durch. \\
  Wir suchen dazu eine stetig diff'bare Funktion $f : \Theta \to \R^1$, sodass
  \[ \sqrt{n} (f(T_n) - f(\vartheta)) \xra[n \to \infty]{d} \Normal(0, 1). \]
  Man zeigt, dass dafür gelten muss:
  \[
    f'(\vartheta) = \tfrac{1}{\sqrt{g(\vartheta)}}, \quad \text{also} \quad
    f(\theta) = \myint{}{} \tfrac{\d \vartheta}{\sqrt{g(\vartheta)}}.
  \]
\end{situation}

\begin{bspe}
  \begin{itemize}
    \item Sei $X \sim \Exp(\mu)$, $\hat{\mu}_n \coloneqq \tfrac{1}{\overline{X}_n}$.
    Dann gilt
    \begin{align*}
      & \sqrt{n} (\overline{X}_n - \tfrac{1}{\mu}) \xra{d} \Normal(0, g(\tfrac{1}{\mu}))
      \quad \text{mit} \quad
      g(\vartheta) \coloneqq \vartheta^2. \\
      \leadsto \enspace & \text{Mit } f(\theta) \coloneqq \myint{}{} \tfrac{\d \vartheta}{\sqrt{g(\vartheta)}} = \myint{}{} \tfrac{\d \vartheta}{\vartheta} = \log \theta \\
      & \text{gilt } \sqrt{n} (\log(\overline{X}_n - \log(\tfrac{1}{\mu}))) \xra[n \to \infty]{d} \Normal(0, 1).
    \end{align*}
    \item Wir wollen eine unbek. Wahrscheinlichkeit~$p$ schätzen, etwa durch Wurf einer Münze.
    Der ZGWS von de-Moirre-Laplace besagt
    \[ \sqrt{n} ( \hat{p}_n - p) \xra[n \to \infty]{d} \Normal(0, p (1-p)), \]
    wobei~$\hat{p}_n$ die relative Häufigkeit ist.
    Zur Stabilisierung der Varianz verwenden wir nun
    \[ f(\theta) \coloneqq \myint{}{} \tfrac{\d p}{\sqrt{p (1-p)}} = 2 \arcsin(\sqrt{\theta}). \]
  \end{itemize}
\end{bspe}

% 1.5. Mehrdimensionale Normalverteilung

\begin{defn}
  Die $k$-dim (Gaußsche) Normalverteilung $\Normal_k(m, C)$ mit EW~$m \in \R^k$ und einer nichtnegativ-definiten, symmetrischen Kovarianzmatrix~$C \in \R^{k \times k}$ ist gegeben durch die Dichte
  \[ f_{\Normal_k(m, C)}(x) \coloneqq \left( (2\pi)^{k/2} \sqrt{\det(C)} \right)^{-1} \exp \left( - \tfrac{1}{2} (x-m) C^{-1} (x-m)^T \right). \]
\end{defn}

\begin{bem}
  Bei $k=2$ schreibt man oft $C = \begin{psmallmatrix}
    \sigma_1^2 & \sigma_1 \sigma_2 \rho \\
    \sigma_1 \sigma_2 \rho & \sigma_2^2
  \end{psmallmatrix}$ mit $\rho \coloneqq \cor(X_1, X_2)$.
\end{bem}

\begin{defn}
  Die \emph{charakteristische Fkt} eines ZV $X = (X_1, \ldots, X_k)^T$ ist
  \[
    \varphi : \R^k \to \R, \enspace
    t \mapsto \E e^{i \scp{t}{X}} = \Int{\R^k}{}{e^{i (t_1 x_1 + \ldots + t_k x_k)}}{F_X(x_1, \ldots, x_k)}.
  \]
\end{defn}

\begin{bem}
  Die charakteristische Funktion von $\Normal_k(m, C)$ ist
  \[ \varphi_{\Normal_k(m, C)}(t) = \exp \left( i \sum_{i=1}^k t_i m_i - \tfrac{1}{2} \sum_{i,j=1}^k t_i c_{ij} t_j \right). \]
\end{bem}

\begin{satz}
  Für $A \in R^{k \times l}$ gilt $\Normal_k(m, C) \cdot A = \Normal_l(m \cdot A, A^T C A)$.
\end{satz}

% Vorlesung vom 26.10.2015

% 2. Anpassungstests und weitere nichtparametrische Tests

% 2.1. Chi-Quadrat-Anpassungstest

\begin{aufgabe}
  Prüfe, ob eine vorliegende Stichprobe $x_1, \ldots, x_n$ aus einer bestimmten (stetig oder diskret verteilten) Grundgesamtheit gezogen wurde. Wir testen also $H_0 : F = F_0$ vs. $H_1 : F \neq F_0$.
\end{aufgabe}

% Ausgelassen: Bemerkung über Einordnung in parametrische Tests via. $\Theta \coloneqq \{ \text{Verteilungsfunktionen auf $\R$} \}}$ und $\Theta_0 \coloneqq \{ F_0 \}$.

\begin{verf}
  Wir teilen zunächst $\R$ in Klassen ein,
  \begin{align*}
    & \R = \bigcup_{i=1}^{s+1} I_j
    \quad \text{mit} \quad
    I_j \coloneqq \ocinterval{y_{j-1}}{y_j},
    \quad
    \text{wobei} \\
    & - \infty = y_0 < y_1 < \ldots < y_s < y_{s+1} = + \infty.
  \end{align*}
  Wir setzen
  \begin{align*}
    & h_{n_j} \coloneqq \abs{\Set{k \in \{ 1, \ldots, n \}}{X_k \in I_j}} \tag{absolute Klassenhäufigkeit} \\
    & p_j^{(0)} \coloneqq \P(X \in I_j) = F_0(y_j) - F_0(y_{j-1}) \tag{Klassenwktn unter $H_0$}
  \end{align*}
  Die Klassenhäufigkeiten sind nun multinomialverteilt:
  \[ \P(h_{n_1} \!=\! n_1, \nldots, h_{n_{s+1}} \!=\! n_{s+1}) = {n \choose n_1, \nldots, n_{s+1}} (p_1^{(0)})^{n_1} \cdots (p_{s+1}^{(0)})^{n_{s+1}}. \]
  Als Maß für die Abweichung einer empirischen Verteilung von $F_0$ bei gegebener Klasseneinteilung dient
  \[ T_{n,s+1} \coloneqq \sum_{j=1}^{s+1} \frac{(h_{n_j} - n p_j^{(0)})^2}{n p_j^{(0)}}. \]
\end{verf}

\begin{satz}
  $T_{n,s+1} \xra[n \to \infty]{d} \chi_s^2$
\end{satz}

\begin{faustregel}
  Für $n p_j^{(0)} \geq 5$, $j = 1, \ldots, s+1$ ist $T_{n,s+1}$ mit guter Näherung $\chi_s^2$-verteilt.
\end{faustregel}

\end{document}