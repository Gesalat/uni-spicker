\documentclass{cheat-sheet}

\pdfinfo{
  /Title (Zusammenfassung Stochastik 3)
  /Author (Tim Baumann)
}

\usepackage{bbm} % Für 1 mit Doppelstrich (Indikatorfunktion)
\usepackage{mathtools} % psmallmatrix environment
\usepackage{nicefrac}

% Kleinere Klammern
\delimiterfactor=701

% TODO: Include-File für Stochastik

\newcommand{\Alg}{\mathfrak{A}} % (Mengen-)Algebra
%\newcommand{\Ring}{\mathfrak{R}} % (Mengen-)Ring
%\newcommand{\LebAlg}{\mathfrak{L}} % Lebesgue-Borel-Mengen
\renewcommand{\P}{\mathbb{P}} % Wahrscheinlichkeitsmaß
\newcommand{\E}{\mathbb{E}} % Erwartungswert
\newcommand{\Bor}{\mathfrak{B}} % Borel
%\newcommand{\Leb}{\mathcal{L}} % Lebesgue
\newcommand{\ind}{\mathbbm{1}} % Indikatorfunktion
\newcommand{\Cont}{\mathcal{C}} % Menge der stetigen/diff'baren Funktionen
\newcommand{\scp}[2]{\left( #1 \!\mid\! #2 \right)} % Skalarprodukt
%\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\eqqd}{\stackrel{d}{=}} % Gleichheit in Verteilung (equality in distribution)
\newcommand{\iid}{i.\,i.\,d.} % identisch unabhängig verteilt
\newcommand{\Uniform}{\mathcal{R}} % Gleichverteilung

\DeclareMathOperator{\var}{Var} % Varianz
\DeclareMathOperator{\cov}{Cov} % Kovarianz
\DeclareMathOperator{\cor}{Cor} % Korrelation

% Verteilungen
\newcommand{\Normal}{\mathcal{N}} % Gaußsche Normalverteilung
\DeclareMathOperator{\Exp}{Exp} % Exponentialverteilung
\newcommand{\MN}{\mathcal{M}} % Multinomialverteilung

\begin{document}

\maketitle{Zusammenfassung Stochastik 3}

% Vorlesung vom 12.10.2015

% 1. Wiederholung

% 1.1 Grundbegriffe der Testtheorie

\begin{modell}
  Gegeben sei ein Parametrisches Modell, \dh eine Zufallsgröße $X$, deren Verteilungsfunktion $P_X \in \Set{P_\vartheta}{\vartheta \in \Theta \subset \R^n}$ von einem Parameter $\vartheta$ abhängt.
\end{modell}

\begin{prob}
  Anhand einer \emph{Stichprobe} $x_1, \ldots, x_n \in \R^1$ von $X$ (\dh{} $x_1, \ldots, x_n$ sind Realisierung von iid ZGen $X_1, \ldots, X_n \sim P_X$) ist zu entscheiden, ob die sogenannte \emph{Nullhypothese} $H_0 : \vartheta \in \Theta_0 \subset \Theta$ oder eine \emph{Gegenhypothese} $H_1 : \vartheta \in \Theta_1 = \Theta \setminus \Theta_0$ angenommen oder abgelehnt werden soll.
\end{prob}

\begin{defn}
  Der \emph{Stichprobenraum} ist $(\R^n, \Bor(\R^n), P_\vartheta \times \ldots \times P_\vartheta)$
\end{defn}

\begin{terminologie}
  Die Hypothese $H_i$ heißt \emph{einfach}, falls $\abs{\Theta_i} = 1$, andernfalls \emph{zusammengesetzt}.
\end{terminologie}

\begin{defn}
  Ein (nichtrandomisierter) \emph{Test} für $H_0$ gegen $H_1$ ist eine Entscheidungsregel über die Annahme von $H_0$ basierend auf einer Stichprobe, die durch eine messbare Abbildung $\varphi : \R^n \to \{ 0, 1 \}$ augedrückt wird und zwar durch
  \[ \varphi(x_1, \ldots, x_n) = \begin{cases}
    0 & \text{bei Annahme von $H_0$,} \\
    1 & \text{bei Ablehung von $H_0$.}
  \end{cases} \]
\end{defn}

\begin{defn}
  Der \emph{Ablehungsbereich} oder \emph{kritische Bereich} von $\varphi$ ist
  \[ K_n \coloneqq \Set{(x_1, \ldots, x_n) \in \R^n}{\varphi(x_1, \ldots, x_n) = 1}. \]
\end{defn}

\begin{bem}
  Es gilt $\varphi = \ind_{K_n}$.
\end{bem}

\begin{defn}
  Ein \emph{Fehler 1. Art} ist eine Ablehnung der Nullhypothese $H_0$, obwohl $H_0$ richtig ist;
  ein \emph{Fehler 2. Art} ist eine Annahme von $H_0$, obwohl $H_0$ falsch ist.
\end{defn}

% Ausgelassen: Stichprobenraum ist [R^n, B(R^n), P_\vartheta \times \ldots \times P_\vartheta]

\begin{defn}
  Die \emph{Güte- oder Machtfunktion} des Tests $\varphi$ ist
  \begin{align*}
    m_\varphi : \Theta \to \cinterval{0}{1},
    m_\varphi(\vartheta) & \coloneqq
    \E_\vartheta \varphi(X_1, \ldots, X_n) \\
    & = \P_\vartheta ((X_1, \ldots, X_n) \in K_n) \\
    & = (P_\vartheta \times \ldots \times P_\vartheta)(K_n)
  \end{align*}
  Die Gegenwsk. $(1 {-} m_\varphi(\vartheta))$ heißt \emph{Operationscharakteristik} von $\varphi$.
\end{defn}

\begin{bem}
  Es gilt
  \begin{alignat*}{2}
    \P_\vartheta(\text{Fehler 1. Art}) &= m_\varphi(\vartheta) & \enspace\text{für $\vartheta \in \Theta_0$,} \\
    \P_\vartheta(\text{Fehler 2. Art}) &= 1 - m_\varphi(\vartheta) & \enspace\text{für $\vartheta \in \Theta_1$.}
  \end{alignat*}
\end{bem}

% Ausgelassen: Graph einer "fast idealen Kurve"

\begin{defn}
  Ein Test $\varphi : \R^n \to \{ 0, 1 \}$ mit
  \[ \sup_{\vartheta \in \Theta_0} m_\varphi(\vartheta) \leq \alpha \]
  heißt \emph{$\alpha$-Test} o. \emph{Signifikanztest} zum \emph{Signifikanzniveau} $\alpha \in \ointerval{0}{1}$. \\[2pt]
  Ein $\alpha$-Test $\varphi$ heißt \emph{unverfälscht} (erwartungstreu, unbiased), falls
  \[ \inf_{\vartheta \in \Theta_1} m_\varphi(\vartheta) \geq \alpha. \]
\end{defn}

% Konstruktion nichtrandomisierter Tests mittels Stichprobenfunktionen (= Statistiken) im Falle einfacher Nullhypothesen

\begin{situation}
  Sei nun eine Stichprobenfunktion oder \emph{Teststatistik} $T : \R^n \to \R^1$ gegeben.
  Wir wollen einen Test der einfachen Nullhypothese $H_0 : \vartheta \in \Theta_0 = \{ \vartheta_0 \}$ entwickeln.
\end{situation}

\begin{defn}
  $K_n^T \subset \R^1$ heißt \emph{kritischer Bereich der Teststatistik}, falls
  \[ K_n = T^{-1}(K_n^T). \]
\end{defn}

\begin{bem}
  Es gilt
  \begin{alignat*}{4}
    m_\varphi(\vartheta_0) &= \P_{\vartheta_0} \left( (X_1, \ldots, X_n) \in K_n \right)
    &&= \\
    &= \P_{\vartheta_0} \left( (T(X_1), \ldots, T(X_n)) \in K_n^T \right)
    &&= \Int{K_n^T}{}{f_T(x)}{x} \leq \alpha,
  \end{alignat*}
  wobei $f_T$ die Dichte von $T(X_1, \ldots, X_n)$ unter $H_0$ ist.
\end{bem}

\begin{bsp}
  Sei $X \sim \Normal(\mu, \sigma^2)$, $\sigma$ bekannt und $\alpha \in \ointerval{0}{1}$ vorgegeben. \\
  Zum Test von $H_0 : \mu = \mu_0$ vs. $H_1 : \mu \neq \mu_0$ wählen wir als Statistik
  \[
    T(X_1, \ldots, X_n) \coloneqq \tfrac{\sqrt{n}}{\sigma} \left( \overline{X}_n - \mu_0 \right) \enspace
    \text{mit }
    \overline{X}_n \coloneqq \tfrac{1}{n} \left( X_1 + \ldots + X_n \right).
  \]
  Unter Annahme von $H_0$ gilt $T(X_1, \ldots, X_n) \sim \Normal(0,1)$. \\
  Der Ablehnungsbereich der Statistik ist
  \[
    K_n^T = \Set{t \in \R^1}{\abs{t} > z_{1 - \alpha/2}}
    \quad \text{mit} \quad
    z_{1 - \alpha/2} \coloneqq \Phi^{-1}(1 - \alpha/2).
  \]
  Für $\alpha = 0,5$ gilt beispielsweise $z_{1 - \alpha/2} \approx 1,96$.
\end{bsp}

% Vorlesung vom 15.10.2015

\begin{bem}
  Es gilt
  \begin{align*}
    t \in (K_n^T)^c
    &\iff \abs{t} \leq z_{1-\alpha/2}
    \iff \abs{\overline{X}_n - \mu_0} \leq \tfrac{\sigma}{\sqrt{n}} z_{1-\alpha/2} \\
    &\iff \mu_0 \in \cinterval{\overline{X}_n - \tfrac{\sigma}{\sqrt{n}} z_{1-\alpha/2}}{\overline{X}_n + \tfrac{\sigma}{\sqrt{n}} z_{1-\alpha/2}}.
  \end{align*}
  Letzteres Intervall wird \emph{Konfidenzintervall} für $\mu_0$ zum Konfidenz- niveau $1-\alpha$ genannt.
\end{bem}

\begin{bsp}
  Sei wieder $X \sim \Normal(\mu, \sigma^2)$, $\sigma^2$ aber diesmal unbekannt. \\
  Zum Testen von $H_0 : \mu = \mu_0$ vs. $H_1 : \mu \neq \mu_0$ verwenden wir
  \[
    \hat{T}(X_1, \ldots, X_n) = \tfrac{\sqrt{n}}{S_n} \left( \overline{X}_n - \mu_0 \right), \quad
    S_n^2 \coloneqq \tfrac{1}{n-1} \sum_{i=1}^n \left( X_i - \overline{X}_n \right)^2.
  \]
  Dabei ist $S_n$ die \emph{(korrigierte) Stichprobenvarianz}.
  Man kann zeigen, dass $\hat{T}(X_1, \ldots, X_n) \sim t_{n-1}$ unter $H_0$.
  Dabei ist $t_m$ die \emph{Student'sche $t$-Verteilung} mit $m$ Freiheitsgraden. \\
  Der Ablehnungsbereich ist
  \[ K_n^T = \Set{t \in \R^1}{\abs{t} > t_{n-1,1-\alpha/2}}. \]
\end{bsp}

\begin{bem}
  $S_n^2$ und $\overline{X}_n$ sind unabhängig für $n \geq 2$. % und umgekehrt?
\end{bem}

\begin{diskussion}
  \begin{itemize}
    \item Je kleiner $\alpha$ ist, desto "`nullhypothesenfreundlicher"' ist der Test.
    Häufig verwendet wird $\alpha \in \{ 10\%, 5\%, 1\%, 0,5\% \}$.
    \item Einseitige Tests: Die Gegenhypothese zu $H_0 \!:\! \mu \!=\! \mu_0$ ist $H_1 \!:\! \mu \!>\! \mu_0$.
    Die Nullhypothese wird nur abgelehnt, falls zu große Stichproben- mittelwerte $\overline{x}_n$ vorliegen. Es ist dann $K_n^T = \ointerval{z_{1-\alpha}}{\infty}$.
  \end{itemize}
\end{diskussion}

% 1.2. Prüfverteilung bei normalverteilten Grundgesamtheit

\begin{defn}
  Es seien $X_1, \ldots, X_n \sim \Normal(0, 1)$.
  Dann heißt die Summe $X_1^2 + \ldots + X_n^2 \sim \chi_n^2$ \emph{Chi-Quadrat-verteilt} mit $n$ Freiheitsgraden.
\end{defn}

\begin{defn}
  Falls $X \sim \Normal(0,1)$ und $Y_n \sim \chi_n^2$ unabhängig sind, so heißt
  \[ \tfrac{X}{\sqrt{\tfrac{Y_n}{n}}} \sim t_n \]
  \emph{$t$-verteilt} mit $n$-Freiheitsgraden.
\end{defn}

\begin{lem}
  $\tfrac{n-1}{\sigma^2} S_n^2 \sim \chi_{n-1}^2$
\end{lem}

\begin{kor}
  $\hat{T}$ aus dem zweiten obigen Bsp ist tatsächlich $t$-verteilt.
\end{kor}

\begin{defn}
  Seien $Y_{n_i} \sim \chi_{n_i}^2$, $i = 1, 2$ zwei unabhängige ZGen.
  Dann heißt %der Quotient
  \[ \tfrac{Y_{n_1} / n_1}{Y_{n_2} / n_2} \sim F_{n_1, n_2} \]
  \emph{F-verteilt} (wie Fisher) mit $(n_1, n_2)$ Freiheitsgraden.
  % auch: Suedecor-verteilt
\end{defn}

% Vorlesung vom 19.10.2015

\begin{bsp}
  Sei $X \sim \Normal(\mu, \sigma^2)$ mit $\mu$ unbekannt.
  Wir testen $H_0 : \sigma = \sigma_0$ vs. $H_1 : \sigma \neq \sigma_0$ mit
  \[ T \coloneqq \tfrac{n-1}{\sigma_0^2} S_n^2 \]
  Unter Annahme von $H_0$ gilt $T ~ \chi_{n-1}^2$.
  Falls $\mu$ bekannt ist, muss man
  \[
    \widetilde{T} \coloneqq \tfrac{n}{\sigma_0^2} \widetilde{S}_n^2, \quad
    \widetilde{S}_n^2 \coloneqq \tfrac{1}{n} \sum_{i=1}^n (X_i - \mu)^2.
  \]
  als Statistik wählen. Unter Annahme von $H_0$ ist $\widetilde{T} \sim \chi_n^2$.
\end{bsp}

\begin{bsp}
  Seien Stichproben $X_1^{(1)}, \ldots, X_{n_1}^{(1)} \sim \Normal(\mu_1, \sigma_1^2)$ und $X_1^{(2)}, \ldots, X_{n_2}^{(2)} \sim \Normal(\mu_2, \sigma_2^2)$ gegeben.
  Wir wollen $H_0 : \sigma_1 = \sigma_2$ gegen $H_1 : \sigma_1 \neq \sigma_2$ testen.
  Dazu verwenden wir
  \[
    T = \frac{S_{X^{(1)}}^2}{S_{X^{(2)}}^2}, \quad
    S_{X^{(j)}}^2 \coloneqq \tfrac{1}{n-1} \sum_{i=1}^{n_j} \left( X_i^{(j)} - \overline{X}^{(j)}_n \right)^n.
  \]
  Falls $H_0$ gilt, so ist $T \sim F_{n_1-1,n_2-1}$.
\end{bsp}

\begin{bsp}
  Situation wie im letzten Beispiel mit $\sigma_1 = \sigma_2$.
  Wir testen $H_0 : \mu_1 = \mu_2$ vs. $H_1 : \mu_1 \neq \mu_2$ mit
  \[
    T = \sqrt{\frac{n_1 \cdot n_2}{n_1 + n_2}} \cdot \frac{\overline{X}_{n_1}^{(1)} - \overline{X}_{n_2}^{(2)}}{S_{n_1,n_2}}, \quad
    S_{n_1,n_2}^2 = \frac{(n_1{-}1) S_{X^{(1)}}^2 + (n_2{-}1) S_{X^{(2)}}^2}{n_1 + n_2 - 2}
  \]
  Unter $H_0$ gilt $T \sim t_{n_1 + n_2 - 2}$.
\end{bsp}

\begin{bsp}
  Seien $\begin{psmallmatrix} X_1 \\ Y_1 \end{psmallmatrix}, \ldots, \begin{psmallmatrix} X_n \\ Y_n \end{psmallmatrix} \sim \Normal\left(
    \begin{psmallmatrix}
      \mu_1\vphantom{\sigma_1^2} \\
      \mu_2\vphantom{\sigma_2^2}
    \end{psmallmatrix},
    \begin{psmallmatrix}
      \sigma_1^2 & \sigma_1 \sigma_2 \rho \\
      \sigma_1 \sigma_2 \rho & \sigma_2^2
    \end{psmallmatrix} \right)$. \\[2pt]
  Wir testen $H_0 : \rho = 0$ vs. $H_1 : \rho \neq 0$ mit
  \[
    T \coloneqq \frac{\sqrt{n-2} \cdot \hat{\rho}_n}{\sqrt{1 - \hat{\rho}_n^2}}, \quad
    \hat{\rho}_n \coloneqq \frac{\tfrac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X}_n) (Y_i - \overline{Y}_n)}{S_{X,n} \cdot S_{Y,n}}.
    % Nenner ausgeschrieben: \sqrt{\tfrac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X}_n)^2 \tfrac{1}{n-1} \sum_{i=1}^n (Y_i - \overline{Y}_n)^2}
  \]
  Falls $H_0$ richtig ist, so gilt $T \sim t_{n-2}$. \\[2pt]
  Um $H_0 : \rho = \rho_0 \in \ointerval{0}{1}$ vs. $H_1 : \rho \neq \rho_0$ zu testen, kann man
  \[ T = \tfrac{\sqrt{n-3}}{2} \left( \log \tfrac{1 + \hat{\rho}_n}{1 - \hat{\rho}_n} - \log \tfrac{1+\rho_0}{1-\rho_0} \right) \]
  verwenden. Für $n$ groß gilt $T \sim \Normal(0, 1)$ unter $H_0$.
\end{bsp}

% 1.4. Lemma von Slutzky und varianzstabilisiernde Transformationen

\begin{lem}
  Seien $(X_n)$, $(Y_n)$ zwei Folgen von ZGn über $(\Omega, \Alg, \P)$ mit $X_n \xra[n \to \infty]{\P} c = \text{const}$ (\dh{} $\fa{\epsilon > 0} \P(\abs{X_n - c} > \epsilon) \to 0$) und $Y_n \xra[n \to \infty]{d} Y$ (\dh{} $\P(Y_n \leq y) \to \P(Y \leq y)$ für alle Stetigkeitspunkte $y$ der VF $y \mapsto \P(Y \leq y)$). Dann gilt:
  \[
    X_n + Y_n \xra{d} c + Y, \quad
    X_n \cdot Y_n \xra{d} c \cdot Y, \quad
    Y_n / X_n \xra{d} Y / c \enspace \text{(falls $c \neq 0$)}
  \]
  und allgemeiner $f(X_n, Y_n) \xra[n \to \infty]{d} f(c, Y)$ für jede Fkt $f \in \Cont(\R^2, \R)$.
\end{lem}

\begin{bem}
  Unabhängigkeit von $(X_n)$ und $(Y_n)$ wird nicht vorausgesetzt!
\end{bem}

% Vorlesung vom 22.10.2015

% Varianzstabilisierende Transformationen

\begin{situation}
  Sei $T_n = T(X_1, \ldots, X_n)$ eine Statistik.
  Falls der ZGWS für $T_n$ die Form
  \[ \sqrt{n} (T_n - \vartheta) \xra[n \to \infty]{d} \Normal(0, g(\vartheta)) \]
  besitzt, so benötigen wir für Hypothesentests eine Möglichkeit, die Abhängigkeit der Varianz von Parameter $\vartheta$ zu beseitigen.
  Man sagt, man führt eine \emph{varianzstabilisierende Transformation} durch. \\
  Wir suchen dazu eine stetig diff'bare Funktion $f : \Theta \to \R^1$, sodass
  \[ \sqrt{n} (f(T_n) - f(\vartheta)) \xra[n \to \infty]{d} \Normal(0, 1). \]
  Man zeigt, dass dafür gelten muss:
  \[
    f'(\vartheta) = \tfrac{1}{\sqrt{g(\vartheta)}}, \quad \text{also} \quad
    f(\theta) = \myint{}{} \tfrac{\d \vartheta}{\sqrt{g(\vartheta)}}.
  \]
\end{situation}

\begin{bspe}
  \begin{itemize}
    \item Sei $X \sim \Exp(\mu)$, $\hat{\mu}_n \coloneqq \tfrac{1}{\overline{X}_n}$.
    Dann gilt
    \begin{align*}
      & \sqrt{n} (\overline{X}_n - \tfrac{1}{\mu}) \xra{d} \Normal(0, g(\tfrac{1}{\mu}))
      \quad \text{mit} \quad
      g(\vartheta) \coloneqq \vartheta^2. \\
      \leadsto \enspace & \text{Mit } f(\theta) \coloneqq \myint{}{} \tfrac{\d \vartheta}{\sqrt{g(\vartheta)}} = \myint{}{} \tfrac{\d \vartheta}{\vartheta} = \log \theta \\
      & \text{gilt } \sqrt{n} (\log(\overline{X}_n - \log(\tfrac{1}{\mu}))) \xra[n \to \infty]{d} \Normal(0, 1).
    \end{align*}
    \item Wir wollen eine unbek. Wahrscheinlichkeit~$p$ schätzen, etwa durch Wurf einer Münze.
    Der ZGWS von de-Moirre-Laplace besagt
    \[ \sqrt{n} ( \hat{p}_n - p) \xra[n \to \infty]{d} \Normal(0, p (1-p)), \]
    wobei~$\hat{p}_n$ die relative Häufigkeit ist.
    Zur Stabilisierung der Varianz verwenden wir nun
    \[ f(\theta) \coloneqq \myint{}{} \tfrac{\d p}{\sqrt{p (1-p)}} = 2 \arcsin(\sqrt{\theta}). \]
  \end{itemize}
\end{bspe}

% 1.5. Mehrdimensionale Normalverteilung

\begin{defn}
  Die $k$-dim (Gaußsche) Normalverteilung $\Normal_k(m, C)$ mit EW~$m \in \R^k$ und einer nichtnegativ-definiten, symmetrischen Kovarianzmatrix~$C \in \R^{k \times k}$ ist gegeben durch die Dichte
  \[ f_{\Normal_k(m, C)}(x) \coloneqq \left( (2\pi)^{k/2} \sqrt{\det(C)} \right)^{-1} \exp \left( - \tfrac{1}{2} (x-m) C^{-1} (x-m)^T \right). \]
\end{defn}

\begin{bem}
  Bei $k=2$ schreibt man oft
  \[
    C = \begin{psmallmatrix}
      \sigma_1^2 & \sigma_1 \sigma_2 \rho \\
      \sigma_1 \sigma_2 \rho & \sigma_2^2
    \end{psmallmatrix}
    \quad \text{mit} \quad
    \rho \coloneqq \cor(X_1, X_2).
  \]
\end{bem}

\begin{defn}
  Die \emph{charakteristische Fkt} eines ZV $X = (X_1, \ldots, X_k)^T$ ist
  \[
    \varphi : \R^k \to \R, \enspace
    t \mapsto \E e^{i \scp{t}{X}} = \Int{\R^k}{}{e^{i (t_1 x_1 + \ldots + t_k x_k)}}{F_X(x_1, \ldots, x_k)}.
  \]
\end{defn}

\begin{bem}
  Die charakteristische Funktion von $\Normal_k(m, C)$ ist
  \[ \varphi_{\Normal_k(m, C)}(t) = \exp \left( i \sum_{i=1}^k t_i m_i - \tfrac{1}{2} \sum_{i,j=1}^k t_i c_{ij} t_j \right). \]
\end{bem}

\begin{satz}
  Für $A \in R^{k \times l}$ gilt $\Normal_k(m, C) \cdot A = \Normal_l(m \cdot A, A^T C A)$.
\end{satz}

% Vorlesung vom 26.10.2015

% 2. Anpassungstests und weitere nichtparametrische Tests

% 2.1. Chi-Quadrat-Anpassungstest

\begin{aufgabe}
  Prüfe, ob eine vorliegende Stichprobe $x_1, \ldots, x_n$ aus einer bestimmten (stetig oder diskret verteilten) Grundgesamtheit gezogen wurde. Wir testen also $H_0 : F = F_0$ vs. $H_1 : F \neq F_0$.
\end{aufgabe}

% Ausgelassen: Bemerkung über Einordnung in parametrische Tests via. $\Theta \coloneqq \{ \text{Verteilungsfunktionen auf $\R$} \}}$ und $\Theta_0 \coloneqq \{ F_0 \}$.

\begin{verf}
  Wir teilen zunächst $\R$ in Klassen ein,
  \begin{align*}
    & \R = \bigcup_{i=1}^{s+1} I_j
    \quad \text{mit} \quad
    I_j \coloneqq \ocinterval{y_{j-1}}{y_j},
    \quad
    \text{wobei} \\
    & - \infty = y_0 < y_1 < \ldots < y_s < y_{s+1} = + \infty.
  \end{align*}
  Wir setzen
  \begin{align*}
    & h_{n_j} \coloneqq \abs{\Set{k \in \{ 1, \ldots, n \}}{X_k \in I_j}} \tag{absolute Klassenhäufigkeit} \\
    & p_j^{(0)} \coloneqq \P(X \in I_j) = F_0(y_j) - F_0(y_{j-1}) \tag{Klassenwktn unter $H_0$}
  \end{align*}
  Die Klassenhäufigkeiten sind nun multinomialverteilt:
  \[ \P(h_{n_1} \!=\! n_1, \nldots, h_{n_{s+1}} \!=\! n_{s+1}) = {n \choose n_1, \nldots, n_{s+1}} (p_1^{(0)})^{n_1} \cdots (p_{s+1}^{(0)})^{n_{s+1}}. \]
  Als (näherungsweises) Maß für die Abweichung einer empirischen Verteilung von $F_0$ bei gegebener Klasseneinteilung dient
  \[ T_{n,s+1} \coloneqq \sum_{j=1}^{s+1} \frac{(h_{n_j} - n p_j^{(0)})^2}{n p_j^{(0)}}. \]
\end{verf}

\begin{satz}
  $T_{n,s+1} \xra[n \to \infty]{d} \chi_s^2$
\end{satz}

\begin{faustregel}
  Für $n p_j^{(0)} \geq 5$, $j = 1, \ldots, s+1$ ist $T_{n,s+1}$ mit guter Näherung $\chi_s^2$-verteilt.
\end{faustregel}

% Vorlesung vom 29.10.2015

\begin{entscheidungsregel}[$\chi^2$-Anpassungstest]
  Die Nullhypothese $H_0 : F = F_0$ wird genau dann verworfen, wenn $T_{n,s+1} > \chi^2_{s,1-\alpha}$.
\end{entscheidungsregel}

\begin{bemn}
  \begin{itemize}
    \item $T_{n,s+1}$ misst eigentlich nicht die Abweichung von der VF~$F_0$, sondern von der Multinomialverteilung $\MN(n, p^{(0)})$.
    \item Der $\chi^2$-Anpassungstest gilt als hypothesenfreundlich.
    \item Es ist üblich, zunächst die Parameter $\vartheta = (\vartheta_1, \ldots, \vartheta_r)$ der VF~$F_0$ durch MLE zu schätzen, also durch
    \begin{align*}
      & \hat{\vartheta}_n \coloneqq \argmax L(h_{n_1}, \ldots, h_{n_{s+1}}; \vartheta), \quad \text{wobei} \\
      & L(h_{n_1}, \ldots, h_{n_{s+1}}; \vartheta) \coloneqq \prod_{j=1}^{s+1} \left( p_j^{(0)} \right)^{h_{n_j}}.
    \end{align*}
    % Natürliche Bedingungen: Rao-Cramér-Regularität
    Es kann (unter "`natürlichen"' Bedingungen) gezeigt werden, dass
    \[ T_{n,s+1}(\hat{\vartheta}_n) = \sum_{j=1}^{s+1} \frac{(h_{n_j} - n p_j^{(0)}(\hat{\vartheta}_n))^2}{n p_j^{(0)}(\hat{\vartheta}_n)} \xra[n \to \infty]{d} \chi_{s-r}^2, \]
    wobei $r$ die Anzahl der geschätzten Parameter ist. %$\hat{\vartheta}_n = (\hat{\vartheta}_{n,1}, \ldots, \hat{\vartheta}_{n,r})$.
    \item Manchmal wird die Parameter-Schätzung auch direkt aus der SP $x_1, \ldots, x_n$ ermittelt (\zB{} $\tilde{\mu}_n \coloneqq \tfrac{1}{n} (x_1 + \ldots + x_n)$ für den MW einer Normalverteilung).
    In manchen Fällen kann dann auf die Reduktion der Freiheitsgrade von $s$ auf $s-r$ verzichtet werden.
  \end{itemize}
\end{bemn}

% Klassisches Beispiel: Hufschlagtote in der preußischen Armee

% 2.2. Chi-Quadrat-Unabhängigkeitstest, Kontingenztafeln

\begin{ziel}
  Überprüfen, ob die Komponenten $X$ und $Y$ eines zweidim. Zufallsvektors $(X, Y)^T$ unabhängig sind.
\end{ziel}

\begin{verf}
  Seien $I_1, \ldots, I_k \subset \R^{n_1}$ und $J_1, \ldots, J_l \subset \R^{n_2}$ jeweils Familien paarweise disjunkter Mengen mit $\P(X \in I_1 \cup \ldots \cup I_k) = 1$ bzw. $\P(Y \in J_1 \cup \ldots \cup J_l) = 1$.
  Wir setzen
  \begin{align*}
    & p_{ij} \coloneqq \P((X, Y) \in I_i \times J_j) = \P(\{ X \in I_i \} \cap \{ X_j \in J_j \}), \\
    & p_{i\bullet} \coloneqq \sum_{j=1}^l p_{ij} = \P(X \in I_i), \quad
    p_{\bullet j} \coloneqq \sum_{i=1}^k p_{ij} = \P(Y \in J_j).
  \end{align*}
  Wir wollen nun die Nullhypothese $H_0 : \fa{(i, j)} p_{ij} = p_{i \bullet} \cdot p_{\bullet j}$ gegen $H_1 : \ex{(i,j)} p_{ij} \neq p_{i \bullet} \cdot p_{\bullet j}$ testen.
  Wir zählen dazu die Häufigkeiten einer Stichprobe $(X_1, Y_1), \ldots, (X_n, Y_n)$:
  \begin{align*}
    & h_{ij}^{(n)} \coloneqq \abs{\Set{m \in \{ 1, \ldots, n \}}{(X_m, Y_m) \in I_i \times J_j}}, \\
    & h_{i \bullet} \coloneqq \sum_{j=1}^l h_{ij}, \quad
    h_{\bullet j} \coloneqq \sum_{i=1}^k h_{ij}.
  \end{align*}
  Diese Häufigkeiten werden in einer \emph{Kontingenztafel} dargestellt:
  \begin{center}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{c | c c c c | c}
      & $1$ & $2$ & $\cdots$ & $l$ & \\ \hline
      $1$ & $h_{11}^{(n)}$ & $h_{12}^{(n)}$ & $\cdots$ & $h_{1l}^{(n)}$ & $h_{1 \bullet}^{(n)}$ \\
      $2$ & $h_{21}^{(n)}$ & $h_{22}^{(n)}$ & $\cdots$ & $h_{2l}^{(n)}$ & $h_{2 \bullet}^{(n)}$ \\
      \vdots & \vdots & \vdots & & \vdots & $\vdots$ \\
      $k$ & $h_{k1}^{(n)}$ & $h_{k2}^{(n)}$ & $\cdots$ & $h_{kl}^{(n)}$ & $h_{k \bullet}^{(n)}$ \\ \hline
      & $h_{\bullet 1}^{(n)}$ & $h_{\bullet 2}^{(n)}$ & $\cdots$ & $h_{\bullet l}^{(n)}$ & n
    \end{tabular}
  \end{center}
  Wir können den Test nun wie folgt als Spezialfall des $\chi^2$-Anpas- sungstests verstehen: Die Nullhypothese ist, dass die Verteilung von $(X, Y)$ das Produkt der Verteilungen von $X$ und $Y$ ist.
  Dabei schätzen wir zunächst die Verteilungen von $X$ und $Y$ mit
  \begin{align*}
    & L(h_{1 \bullet}^{(n)}, \ldots, h_{k \bullet}^{(n)}, h_{\bullet 1}^{(n)}, \ldots, h_{\bullet l}^{(n)}; p_{1 \bullet}, \ldots p_{k-1, \bullet}, p_{\bullet 1}, \ldots, p_{\bullet, l-1}) \\
    \coloneqq & \prod_{i=1}^{k} (p_{i \bullet})^{h_{i \bullet}^{(n)}} \cdot \prod_{j=1}^l (p_{\bullet j})^{h_{\bullet j}^{(n)}}.
  \end{align*}
  Diese Funktion wird maximal bei $\hat{p}_{i \bullet} = \nicefrac{h_{i \bullet}^{(n)}}{n}$ und $\hat{p}_{\bullet j}^{(n)} = \nicefrac{h_{\bullet j}^{(n)}}{n}$. \\
  % Das sind insgesamt $k+l-2$ zu schätzende Parameter.
  % Vorlesung vom 2.11.2015
  Als Test-Statistik verwenden wir
  \begin{align*}
    \hat{T}_{k,l}^{(n)} \coloneqq &
    \sum_{i=1}^k \sum_{j=1}^l \frac{(h_{ij}^{(n)} - n \hat{p}_{i \bullet} \hat{p}_{\bullet j})^2}{n \cdot \hat{p}_{i \bullet} \hat{p}_{\bullet j}} =
    n \sum_{i=1}^k \sum_{j=1}^l \frac{\left( h_{ij}^{(n)} - \nicefrac{h_{i \bullet}^{(n)} \cdot h_{\bullet j}^{(n)}}{n} \right)^2}{h_{i \bullet}^{(n)} \cdot h_{\bullet j}^{(n)}} \\
    & \xra[n \to \infty]{d} \chi^2_{kl - 1 - (k{-}1) - (l{-}1)} = \chi^2_{(k-1)(l-1)}
  \end{align*}
  Testregel: Die Nullhypothese wird genau dann abgelehnt, falls
  \[ \hat{T}_{k,l}^{(n)} > \chi^2_{(k-1)(l-1),1-\alpha}. \]
\end{verf}

\begin{bemn}
  \begin{itemize}
    \item Zum Testen eines höherdim. ZV $(X_1, \ldots, X_r)$ auf Unabhängigkeit aller Komponenten untersuchen wir die Ereignisse
    \[
      (X_1, \ldots, X_r) \in I_{i_1}^{(1)} \times \ldots \times I_{i_r}^{(r)} \quad
      \text{für $(i_1, \ldots, i_r) \in \bigtimes_{j=1}^r \{ 1, \ldots, k_j \}$}
    \]
    für eine passende Intervalleinteilung.
    Wir verwenden dann
    \begin{align*}
      \hat{T}_{k_1, \ldots, k_r}^{(n)} \coloneqq
      & n^{r-1} \sum_{i_1=1}^{k_1} \cdots \sum_{i_r=1}^{k_r} \frac{\left(h_{i_1 \cdots i_r}^{(n)} - n^{-r + 1} \prod_{s=1}^r h_{\bullet \cdots i_j \cdots \bullet}^{(n)}\right)^2}{\prod_{s=1}^r h_{\bullet \cdots i_j \cdots \bullet}} \\
      & \xra[n \to \infty]{d} \chi^2_{k_1 \cdots k_s - k_1 - \ldots - k_r + r - 1}
    \end{align*}
    \item Im Spezialfall $k \!=\! l \!=\! 2$ (Vierfeldertafel) hat die Statistik die Form
    \[
      \hat{T}_{2,2}^{(n)} =
      n \cdot \frac{\left(h_{11}^{(n)} \cdot h_{22}^{(n)} - h_{12}^{(n)} \cdot h_{21}^{(n)}\right)^2}{h_{\bullet 1}^{(n)} \cdot h_{\bullet 2}^{(n)} \cdot h_{1 \bullet}^{(n)} \cdot h_{2 \bullet}^{(n)}}
      \xra[n \to \infty]{d} \chi^2_n = \Normal^2(0, 1)
    \]
    und wir lehnen $H_0$ genau dann ab, wenn $\hat{T}_{2,2}^{(n)} > \chi^2_{1,1-\alpha} = z^2_{1 - \nicefrac{\alpha}{2}}$.
  \end{itemize}
\end{bemn}

% 2.3. Kolmogorow-Smirnow-Test

\begin{situation}
  Sei $X_1, \ldots, X_n \sim F$ eine math. SP.
  Wir sortieren die dabei gezogenen Werte aufsteigend: $X_{1:n} \leq X_{2:n} \leq \ldots \leq X_{n:n}$.
  Dann heißt $\hat{F}_n(x) \coloneqq \nicefrac{1}{n} \sum_{i=1}^n \ind_{\ocinterval{-\infty}{x}}(X_{i:n})$ \emph{empirische VF}.
\end{situation}

\begin{satz}[Gliwenko, Hauptsatz der math. Statistik]
  \[ \sup_{x \in \R^1} \abs{\hat{F}_n(x) - F(x)} \xra[n \to \infty]{\text{$\P$-f.\,s.}} 0 \]
\end{satz}

\begin{lem}
  Sei $F$ stetig.
  Dann ist die Verteilung von $\sup_x \abs{\hat{F}_n(x) - F(x)}$ nicht von der VF $F$ abhängig.
  Genauer:
  \[ \sup_x \abs{\hat{F}_n(x) - F(x)} \eqqd \sup_{0 \leq y \leq 1} \abs{\hat{G}_n(y) - G(y)}, \]
  wobei $\hat{G}_n(y) \coloneqq \nicefrac{1}{n} \sum_{i=1}^n \ind_{\cinterval{0}{y}}(U_i)$ für $U_1, \ldots, U_n \sim \Uniform \cinterval{0}{1}$ \iid{}
\end{lem}

% Vorlesung vom 10.11.2015

\begin{kor}
  Sei $F$ stetig, $n \geq 1$.
  Dann ist die VF
  \[ K_n(z) \coloneqq \P(\sqrt{n} \cdot \sup_{x \in \R} \abs{\hat{F}_n(x) - F(x)} \leq z) \]
  unabhängig von $F$.
\end{kor}

\begin{satz}
  Falls $F$ stetig ist, gilt
  \[ K_n(z) \xra[n \to \infty]{} K(z) \coloneqq \sum_{k = - \infty}^\infty (-1)^k \exp(-2 k^2 z^2). \]
\end{satz}

\begin{defn}
  Dabei ist $K$ die VF der \emph{Kolmogorow-Verteilung}.
\end{defn}

\begin{bem}
  Man zeigt dazu, dass die Folge $X_n : y \mapsto \sqrt{n} \cdot (\hat{G}_n(x) - x)$ gegen die \emph{Brownsche Brücke} $\dot{B}$ konvergiert.
  Für diese gilt
  \[ \sup_{0 \leq x \leq 1} \abs{\dot{B}(x)} \sim K. \]
\end{bem}

\begin{entscheidungsregel}[\emph{Kolmogorow(-Smirnow)-1SP-Test}] \mbox{}\\
  Wir testen $H_0 : F = F_0$ gegen $H_1 : F \neq F_1$.
  Dabei muss $F_0$ eine stetige VF sein.
  Wir verwenden dazu
  \[ T_n \coloneqq \sqrt{n} \cdot \sup_{x \in \R} \abs{\hat{F}_n(x) - F(x)}. \]
  Wir lehnen $H_0$ genau dann ab, wenn $T_n > K_{1-\alpha}$.
\end{entscheidungsregel}

\begin{bemn}
  \begin{itemize}
    \item Für kleine $n \in \N$ sollte man $K_{n,1-\alpha}$ verwenden.
    \item Für große $z$ ist $K(z) \approx 1 - 2 \exp(-2 z^2)$, also $K_{1-\alpha} \approx \sqrt{- \nicefrac{1}{2} \cdot \log(\nicefrac{\alpha}{2})}$ für $\alpha$ klein.
    \item Das Supremum in $T_n$ liegt bei einer Sprungstelle von $\hat{F}_n$.
  \end{itemize}
\end{bemn}

\begin{entscheidungsregel}
  Um $H_0 : F = F_0$ gegen $H_1 : F > F_0$ mit %der Statistik
  \[ T_n^{+} \coloneqq \sqrt{n} \cdot \sup_{x \in \R} (\hat{F}_n(x) - F(x)). \]
  Es gilt
  \[ K_n^{+}(z) \coloneqq \P(T_n^{+} \leq z) \xra[n \to \infty]{} K^{+}(z) \coloneqq 1 - \exp(-2 \max(0, z)^2) \]
  Wir lehnen $H_0$ ab, falls $T_n^{+} > K^+_{1-\alpha}$.
\end{entscheidungsregel}

\begin{acht}
  Der Kolmogorow-Test kann nicht verwendet werden, wenn die Parameter von $F_0$ aus der Stichprobe geschätzt werden.
\end{acht}

% Bemerkung: Es gibt keine Entsprechung für mehrdimensionale ZVen

% Vorlesung vom 12.11.2015

% Analoges Vorgehen beim Cramér-von-Mises-Test:

\begin{defn}
  $\omega_n^2(g) = n \Int{\R^1}{}{g(F(x)) \left( \hat{F}_n(x) - F(x) \right)^2}{F(x)}$ \\
  heißt gewichtete \emph{Cramér-von-Mises-Statistik} oder $\omega^2$-Statistik.
  Dabei ist $g : \cinterval{0}{1} \to \cinterval{0}{\infty}$ eine Gewichtsfktn.
  Häufig verwendet wird
  $g(x) \coloneqq 1$
  und die \emph{Anderson-Darling-Statistik} $g(x) \coloneqq \tfrac{1}{x (1-x)}$.
\end{defn}

\begin{satz}
  Sei $F$ stetig.
  Dann ist
  \[
    \omega_n^2(g) \eqqd n \Int{0}{1}{g(u) \left( \hat{G}_n(u) - u \right)^2}{u}
    \xra[n \to \infty]{d} \Int{0}{1}{g(u) (\dot{B}(u))^2}{u} =: \omega^2(g).
  \]
\end{satz}

\begin{entscheidungsregel}[\emph{CvM-Test}]
  Wir testen $H_0 : F = F_0$ vs. $H_1 : F \neq F_0$ anhand der CvM-Statistik.
  Wir lehnen $H_0$ genau dann ab, wenn $\omega_n^2(g) > \omega_{1-\alpha}^2(g)$.
\end{entscheidungsregel}

\begin{bem}
  Der rechte Wert ist tabelliert für wichtige Funktionen $g$.
\end{bem}

% KS-(2-Stichproben)-Test

\begin{situation}
  Gegeben seien zwei unabhängige SPn $X_1, \ldots, X_n \sim F$ \iid{} und $X_1^*, \ldots, X_m^* \sim F^*$ \iid{}, wobei $F$ und $F^*$ stetig sind. \\
  Wir wollen testen, ob $H_0 : F = F^*$ oder $H_1 : F \neq F^*$ gilt, indem wir die empirischen VFen $\hat{F}_n$ und $\hat{F}_m^*$ vergleichen.
  Dazu verwenden wir
  \[ T_{m,n} \coloneqq \sqrt{\tfrac{m \cdot n}{m + n}} \sup_{x \in \R^1} \abs{\hat{F}_n(x) - \hat{F}_n^*(x)} \]
\end{situation}

\begin{satz}
  Falls $F = F^*$ stetig ist, so gilt
  \[ T_{m,n} \eqqd \sqrt{\tfrac{m \cdot n}{m + n}} \sup_{0 \leq u \leq 1} \abs{\tfrac{1}{n} \sum_{i=1}^n \ind_{\cinterval{0}{u}} (U_i) - \tfrac{1}{m} \sum_{j=1}^m \ind_{\cinterval{0}{u}} (U_j^*)}, \]
  wobei $X_i \eqqd F^{-}(U_i)$, $i = 1, \ldots, n$, \enspace
  $X_j^* \eqqd F^{*,-}(U_j^*)$, $j = 1, \ldots, m$ und
  %$F^{-}$ die Quantilfunktion ist:
  \[
    F^{-}(t) \coloneqq \begin{cases}
      \min \Set{x \in \R^1}{F(x) \geq t} & 0 < t \leq 1, \\
      \lim_{t \downarrow 0} F^{-}(t) & t = 0.
    \end{cases}
    \tag{Quantilfunktion}
  \]
  % (nichtfallend, linksstetig)
\end{satz}

\iffalse
\begin{bem}
  Asymptotik von $T_{n,m}$ für $m, n \to \infty$
  \[
    X_{m,n}(u) \coloneqq \sqrt{\tfrac{m \cdot n}{m + n}} \left( \hat{G}_n(u) - \hat{G}^*_m(u) \right), \quad
    0 \leq u \leq 1.
  \]
  \[
    \E X_{m,n} = \sqrt{\tfrac{m \cdot n}{m + n}} \left( \E \ind_{\cinterval{0}{u}}(U_1) - \E \ind_{\cinterval{0}{u}}(U^*_1) \right) = 0
  \]
  \[
    \var(X_{m,n}) = \tfrac{m \cdot n}{m + n} \left( \E (\hat{G}_n(u))^2 + \E (\hat{G}_m(u))^2 - 2 \E \hat{G}^*_n(u) \cdot \E \hat{G}^*_m(u) \right)
  \]
  % (...)

  Genauso wie oben ergibt sich
  \[
    (X_{m,n}(u_1), \ldots, X_{m,n}(u_k)) \xra[n \to \infty]{d} \Normal_k(0, \Sigma)
    \quad \text{mit} \quad
    \Sigma_{ij} = u_i \wedge u_j - u_i \cdot u_j.
  \]

  Daraus folgt die schwache Konvergenz
  \[ X_{m,n}(\blank) \xra[n \to \infty]{d} \dot{B}(\blank) \]
  im Skorodoch-Raum $\mathcal{D} \cinterval{0}{1}$.
\end{bem}
\fi

\begin{lem}
  $T_{m,n} \xra[n \to \infty]{d} \sup_{0 \leq u \leq 1} \abs{\dot{B}(u)} \sim K$
\end{lem}

\begin{entscheidungsregel}[\emph{Kolmogorow(-Smirnow)-2SP-Test}] \mbox{}\\
  $H_0 : F = F^*$ wird genau dann abgelehnt, falls $T_{m,n} > K_{1-\alpha}$.
\end{entscheidungsregel}

% Vorlesung vom 16.11.2015

% §2.4. 2-Stichprobentest von Wilcoxon-Mann-Whitney (U-Test)

\begin{situation}
  Gegeben seien zwei unabhängige SPn $X_1, \ldots, X_n \sim F$ und $X_1^{*}, \ldots, X_m^{*} \sim F^{*}$, wobei $F$ und $F^*$ stetig sind.
  Ziel: Prüfen von $H_0 : F = F^{*}$ vs. $H_1 : F \neq F^{*}$.
  Konstruktion einer Rangstatistik für konkrete SPn $x_1, \ldots, x_n$ und $x_1^{*}, \ldots, x_m^{*}$.
  \begin{enumerate}
    \item Ordnen: $x_{1:n} < \ldots < x_{n:n}$ und $x_{1:m}^{*} < \ldots < x_{m:m}^{*}$
    \item $\nu_1, \ldots, \nu_m \in \{ 1, \ldots, m+n \}$ seien die Ränge der Werte $x_{i:m}^{*}$ innerhalb der Gesamt-SP:
    \begin{align*}
      x_{1:n} & < \ldots < x_{\nu_1 - 1:n} < x_1^{*} < x_{\nu_1:n} < \ldots < x_{\nu_2 - 2:n} < x_{2:m}^{*} < x_{\nu_2 - 1:n} \\
      & < \ldots < x_{\nu_m-m:n} < x_{m:m}^{*} < x_{\nu_m - m + 1 : n} < \ldots < x_{n:n}
    \end{align*}
  \end{enumerate}
  Heuristik: $H_0$ wird angenommen, falls sich die $x$- und $x^{*}$-Werte "`gut durchmischen"', \dh{} die Anzahl der $x$-Werte, die vor bzw. nach den $x^{*}$-Werten liegen, darf nicht zu groß werden.
  Testgröße:
  \begin{align*}
    W_{m,n} & \coloneqq \abs{\Set{(i, j) \in \{ 1, \ldots, n \} \times \{ 1, \ldots, m \}}{ X_i < X_j^{*}}} = \sum_{i=1}^n \sum_{j=1}^m \ind_{\{ X_i < X_j^{*} \}} \\
    & = \sum_{j=1}^m \abs{\Set{i}{X_i < X_{j:m}^{*}}} = \sum_{j=1}^m (\nu_j - j) = \nu_1 + \ldots + \nu_m - \tfrac{m}{2} (m+1)
  \end{align*}
  Unter $H_0$ gilt:
  \begin{align*}
    \E W_{m,n} & = \sum_{i=1}^n \sum_{j=1}^m \E \ind_{\{ X_i < X_j^{*} \}} = \tfrac{m \cdot n}{2} \\
    \var W_{m,n} & = \tfrac{m \cdot n}{12} (m + n + 1)
  \end{align*}
\end{situation}

\begin{lem}
  Unter $H_0 : F = F^*$ stetig gilt
  \begin{align*}
    g_{m,n}(z) & \coloneqq \sum_{k=0}^{n \cdot m} \P(W_{m,n} = k) z^k = \frac{z^{-m (m+1) / 2}}{{m + n \choose m}} \enspace\qquad \sum_{\mathclap{1 \leq \nu_1 < \ldots < \nu_m \leq m+m}} \enspace z^{\nu_1 + \ldots + \nu_m} \\
    & = \tfrac{1}{{m + n \choose m}} \prod_{k=1}^m \frac{1 - z^{n+k}}{1 - z^k}
  \end{align*}
\end{lem}

\begin{entscheidungsregel}
  Ablehnung von $H_0$, falls $w_{m,n} \leq c_{\nicefrac{\alpha}{2}}$ oder $w_{m,n} \geq m \cdot n - c_{\nicefrac{\alpha}{2}}$, wobei $c_{\nicefrac{\alpha}{2}} = \min \Set{k \geq 0}{\P(W_{m,n} \leq k) = \P(W_{m,n} \geq m \cdot n - k) \geq \nicefrac{\alpha}{2}}$
  ($c_{\nicefrac{\alpha}{2}} - \tfrac{m \cdot n}{2}$ ist Quanil der Ordnung $\tfrac{\alpha}{2}$ der ZG $W_{m,n} - \tfrac{m \cdot n}{2}$).
\end{entscheidungsregel}

Annahme von $H_0$ genau dann, wenn $\abs{w_{m,n} - \tfrac{m \cdot n}{2}} < \tfrac{m \cdot n}{2} - c_{\nicefrac{\alpha}{2}}$

\begin{satz}
  Unter $H_0 : F = F^{*}$ stetig gilt
  \[ T_{m,n} \coloneqq \frac{W_{m,n} - \tfrac{m \cdot n}{2}}{\sqrt{\tfrac{m \cdot n}{2} (m + n + 1)}} \xra[m, n \to \infty]{d} \Normal(0, 1) \]
\end{satz}

% §2.5 Kruskal-Wallis-Test

\begin{bem}[\emph{Kruskal-Wallis-Test}]
  Gegeben seien $k$ Messreihen $X_{i,1}, \ldots, X_{i,n_i} \sim F_i$, $i = 1, \ldots, k$ unabhängige SPn, $F_i$ stetig. \\
  Ziel: Testen von $H_0 : F_1 = \ldots = F_k$.
  Vorgehen:
  \begin{enumerate}
    \item Ordnen der Beobachtungen der Größe nach
    \item $\nu_{i,1} < \ldots < \nu_{i,n_i}$ Platznummern der $n_i$ Beobachtungen der $i$-ten Messreihe in der Gesamt-SP
    \item $\overline{\nu}_i \coloneqq \tfrac{1}{n_i} (\nu_{i,1} + \ldots + \nu_{i,n_i})$, $\overline{\nu} \coloneqq \tfrac{1}{n} \sum_{i=1}^k n_i \overline{\nu}_i$ mit $n \coloneqq n_1 + \ldots + n_k$.
  \end{enumerate}
  Heuristik: $H_0$ ist richtig, falls $\overline{\nu}_i \approx \overline{\nu}$ für alle $i$.
  Testgröße:
  \[ \tfrac{12}{n (n+1)} \sum_{i=1}^k n_i (\overline{\nu}_i - \tfrac{n+1}{2})^2 \xra[n_i \to \infty]{d} \chi^2_{k-1} \]
\end{bem}

% Faustregel: $\min_{1 \leq i \leq k} n_i \geq 5$, $k \geq 4$

% §3. U-Statistiken

% §3.1. Hoeffdings Projektionsmethode und ZGWS

\begin{situation}
  Sei $n \geq m$, $X_1, \ldots, X_n \sim F$ \iid{}, $h : \R^m \to \R^1$ Borel-messbar und symmetrisch. $\E \abs{h(X_1, \ldots, X_m)} < \infty$
\end{situation}

\begin{defn}
  Die \emph{U-Statistik der Ordnung $m$} mit Kernfunktion $h$ ist
  \[ U_i^{(m)} \coloneqq \tfrac{1}{{n \choose m}} \sum_{1 \leq i_1 < \ldots < i_m \leq n} h(X_{i_1}, \ldots, X_{i_m}). \]
\end{defn}

\begin{bem}
  Offenbar: $\E U_i^{(m)} = \E h(X_1, \ldots, X_m)$.
\end{bem}

\begin{bsp}
  Für $m=2$ gilt $\sigma^2 = \var(X_1) = \tfrac{1}{2} \E (X_1 - X_2)^2$.
  Davon inspiriert setzen wir $h(x_1, x_2) \coloneqq \tfrac{1}{2}(x_1 - x_2)^2$.
  Damit haben wir
  \[ U_n^{(2)} = \tfrac{2}{n (n-1)} \sum_{1 \leq i < j \leq n} \tfrac{1}{2} (X_i - X_j)^2 = \tfrac{1}{n (n-1)} ( (n-1) \sum_{i=1}^n X_i^2 - (X_1 + \ldots X_n)^2 + \sum_{i=1}^n X_i^2 ) - X_n = \tfrac{1}{n} (X_1 + \ldots + X_n) = \tfrac{1}{n-1} ( \sum_{i=1}^n X_i^2 - n (\overline{X}_n)^2 ) = \tfrac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X}_n)^2 = S_n^2 \]
\end{bsp}

\begin{lem}[\emph{Hoeffdings Projektionsmethode}]
  Sei $g(x) = \E[h(X_1, \ldots, X_n) | X_1 = x ] = \E h(x, X_2, \ldots, X_m) = \Int{}{}{\cdots \Int{}{}{h(x, x_2, \ldots, x_n)}{F(x_2)} \cdots}{F(x_n)}$
  \[ \tilde{U}_n^{(m)} = \theta + \sum_{i=1}^n (\E(U_n^{(m)} | X_i) - \theta) \]
  wobei $\theta \coloneqq \E U_n^{(m)}$.
  Falls $\E h^2(X_1, \ldots, X_m) < \infty$, so gilt
  \begin{itemize}
    \item $\var(U_n^{(m)} - \tilde{U}_n^{(m)}) = \var(U_n^{(m)})$
    \item $\E (U_n^{(m)} | X_i = x) = \theta + \tfrac{m}{n} (g(x) - \theta)$
  \end{itemize}
\end{lem}

\begin{lem}
  \begin{itemize}
    \item $\var(\tilde{U}^{(m)}_n) = \tfrac{m^2}{n} \cdot \var(g(X_1)) = \frac{m^2}{2} ( \E g^2(X_1) - \theta^2 )$
    \item $\var(U_n) = \tfrac{1}{{n \choose m}} \sum_{k=1}^m {m \choose k} {n - m \choose m - k} \left( \E[h(X_1, \ldots, X_k, X_{k+1}, \ldots, X_m) \cdot h(X_1, \ldots, X_k, X_{m+1}, \ldots, X_{2m-k})] - \theta^2 \right)$ sofern $\E h(X_1, \ldots, X_m) < \infty$.
  \end{itemize}
\end{lem}

\begin{kor}
  Für $m=2$ folgt
  \begin{align*}
    & \var(U_n - \tilde{U}_n) = \var(U_n) - \var(\tilde{U}_n) \\
    & = \tfrac{1}{n (n-1)} \left( \E (\overline{h}(X_1, X_2)^2) - 2 (n-2) \E \overline{h}(X_1, X_2) \overline{h}(X_2, X_3) \right) - \tfrac{4}{n} \var(g(X_1)) \\
    & = \tfrac{1}{n (n-1)} ( \var(h(X_1, X_2)) - 2 \var g(X_1) )
  \end{align*}
  Also $\var(U_n^{(m)} - \tilde{U}_n^{(m)}) \leq c_m \frac{\var(h(X_1, \ldots, X_m))}{n^2}$.
\end{kor}


\end{document}